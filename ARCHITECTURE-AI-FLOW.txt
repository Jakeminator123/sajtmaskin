================================================================================
SAJTMASKIN - AI ARCHITECTURE & DATA FLOW
================================================================================
Last updated: 2026-01-26
Author: AI Assistant (Claude)

This document describes how AI services are used in Sajtmaskin, what is
confirmed vs uncertain, and proposed improvements.

================================================================================
SECTION 1: THE TWO AI ENDPOINTS
================================================================================

There are TWO separate AI services used in this project:

┌─────────────────────────────────────────────────────────────────────────────┐
│ 1. VERCEL AI GATEWAY                                                        │
│    URL: https://ai-gateway.vercel.sh/v1                                     │
│    Auth: AI_GATEWAY_API_KEY (env variable)                                  │
│                                                                             │
│    PURPOSE: Access external AI models (OpenAI, Anthropic, Google, etc.)    │
│    USED FOR:                                                                │
│      - Prompt Assist (rewriting user prompts before sending to v0)         │
│      - Brief generation (deep analysis of user requirements)               │
│      - Text analysis (src/app/api/text/analyze/route.ts)                   │
│      - Audit functionality (src/app/api/audit/route.ts)                    │
│      - Project analysis (src/app/api/projects/[id]/analyze/route.ts)       │
│                                                                             │
│    MODEL FORMAT: "provider/model-name"                                      │
│    EXAMPLES:                                                                │
│      - openai/gpt-5.2                                                       │
│      - anthropic/claude-opus-4.5                                            │
│      - google/gemini-2.5-pro                                                │
│      - deepseek/deepseek-v3.2                                               │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ 2. V0 PLATFORM API                                                          │
│    URL: https://api.v0.dev/v1                                               │
│    Auth: V0_API_KEY (env variable)                                          │
│                                                                             │
│    PURPOSE: Generate websites, components, and code                         │
│    USED FOR:                                                                │
│      - Site generation (the core feature)                                   │
│      - Code generation                                                      │
│      - Preview rendering                                                    │
│      - Image generation (AI-generated images in the site)                  │
│                                                                             │
│    MODEL: You do NOT choose the model - v0 uses its own internal model     │
│    SDK: v0-sdk package (initialized in src/lib/v0.ts)                      │
└─────────────────────────────────────────────────────────────────────────────┘


================================================================================
SECTION 2: DATA FLOW - HOW A SITE GETS GENERATED
================================================================================

Step-by-step flow when user enters a prompt:

    USER INPUT
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 1: PROMPT ASSIST (Optional - if enabled)                             │
│                                                                           │
│ Location: src/lib/hooks/usePromptAssist.ts                                │
│           src/app/api/ai/chat/route.ts                                    │
│           src/app/api/ai/brief/route.ts                                   │
│                                                                           │
│ What happens:                                                             │
│   - User's simple prompt is sent to AI Gateway                            │
│   - A system prompt with technical instructions is included               │
│   - The AI rewrites/expands the prompt into a detailed specification     │
│                                                                           │
│ Example:                                                                  │
│   INPUT:  "Make a landing page for my coffee shop"                        │
│   OUTPUT: "Create a modern, responsive landing page for an artisan       │
│            coffee shop. Include: hero section with high-quality imagery, │
│            menu section with pricing, about section, contact form,       │
│            testimonials carousel. Use warm earth tones..."               │
└───────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 2: V0 GENERATION                                                     │
│                                                                           │
│ Location: src/lib/v0/v0-generator.ts                                      │
│           src/app/api/v0/chats/stream/route.ts                            │
│                                                                           │
│ What happens:                                                             │
│   - The (improved) prompt is sent to v0 Platform API                      │
│   - v0 generates React/Next.js code                                       │
│   - v0 may generate images if needed                                      │
│   - Response is streamed back (SSE - Server-Sent Events)                  │
│                                                                           │
│ Output:                                                                   │
│   - Generated code (React components, pages, etc.)                        │
│   - Preview URL                                                           │
│   - Any AI-generated images                                               │
└───────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 3: IMAGE STORAGE (If images were generated)                          │
│                                                                           │
│ Location: Vercel Blob Storage                                             │
│ Auth: BLOB_READ_WRITE_TOKEN (env variable)                                │
│                                                                           │
│ What happens:                                                             │
│   - AI-generated images from v0 are uploaded to Vercel Blob               │
│   - URLs are rewritten to point to blob storage                           │
│   - This makes images persistent and fast to load                         │
│                                                                           │
│ IMPORTANT: Without BLOB_READ_WRITE_TOKEN, images may not display!        │
└───────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 4: DEPLOYMENT (Optional - if user deploys)                           │
│                                                                           │
│ Location: src/lib/vercelDeploy.ts                                         │
│           src/lib/vercel/vercel-client.ts                                 │
│ Auth: VERCEL_TOKEN (env variable)                                         │
│                                                                           │
│ What happens:                                                             │
│   - Generated code is packaged                                            │
│   - Deployed to Vercel via their deployment API                           │
│   - User gets a live URL                                                  │
└───────────────────────────────────────────────────────────────────────────┘


================================================================================
SECTION 3: ENVIRONMENT VARIABLES REQUIRED
================================================================================

REQUIRED FOR CORE FUNCTIONALITY:
--------------------------------
V0_API_KEY              - For v0 Platform API (site generation)
AI_GATEWAY_API_KEY      - For Vercel AI Gateway (prompt assist, analysis)
BLOB_READ_WRITE_TOKEN   - For Vercel Blob (image storage)

REQUIRED FOR DEPLOYMENT:
------------------------
VERCEL_TOKEN            - For deploying generated sites to Vercel

OPTIONAL:
---------
DEBUG                   - Enable debug logging (e.g., "AI,v0,DB")
V0_STREAM_DEBUG         - Enable v0 streaming debug ("1" to enable)


================================================================================
SECTION 4: WHAT I AM CERTAIN OF
================================================================================

[CONFIRMED] The codebase uses two distinct API endpoints:
  - ai-gateway.vercel.sh/v1 for external AI models
  - api.v0.dev/v1 for site generation

[CONFIRMED] AI Gateway accepts model IDs in "provider/model" format
  - Evidence: src/app/api/ai/gateway/models/route.ts fetches model list
  - Evidence: The model list you provided matches this format

[CONFIRMED] v0-sdk uses createClient() factory function, not a class
  - Evidence: Build errors when using "new V0()" were fixed with createClient()

[CONFIRMED] Prompt Assist sends BOTH system instructions AND user prompt
  - Evidence: src/lib/builder/promptAssist.ts contains buildV0RewriteSystemPrompt()
  - The system prompt includes technical requirements (Next.js, Tailwind, etc.)

[CONFIRMED] Images require Vercel Blob storage to persist
  - Evidence: BLOB_READ_WRITE_TOKEN is checked in multiple places
  - Without it, images generated by v0 may not display in preview


================================================================================
SECTION 5: WHAT I AM LESS CERTAIN OF
================================================================================

[UNCERTAIN] Exact billing/cost structure for AI Gateway vs v0 API
  - I stated "per token" vs "per generation" but haven't verified pricing docs

[UNCERTAIN] Whether AI Gateway is a standalone Vercel product or part of v0
  - The URL suggests it's a Vercel service, but relationship to v0 is unclear
  - Need to verify: Is AI_GATEWAY_API_KEY the same as V0_API_KEY?

[UNCERTAIN] Internal model used by v0 Platform API
  - I said "v0 uses its own internal model" but haven't confirmed this
  - v0 might use a fine-tuned version of GPT-4 or Claude internally

[UNCERTAIN] Whether "Brief Mode" always improves results
  - It adds latency (extra API call)
  - Quality improvement depends on the AI model chosen for brief generation

[UNCERTAIN] Rate limits and quotas for each API
  - Haven't checked documentation for limits on either endpoint


================================================================================
SECTION 6: PROPOSED IMPROVEMENTS
================================================================================

1. CONSOLIDATE API KEY HANDLING
   Current: Multiple helper functions (getGatewayApiKey, etc.) scattered
   Proposed: Single src/lib/ai/config.ts with all AI configuration

2. ADD FALLBACK FOR AI GATEWAY
   Current: If AI Gateway fails, prompt assist just fails
   Proposed: Fall back to sending raw prompt to v0 (skip enhancement)

3. CACHE MODEL LIST
   Current: Model list fetched on every request to /api/ai/gateway/models
   Proposed: Cache for 1 hour to reduce API calls

4. VERIFY ENVIRONMENT VARIABLES ON STARTUP
   Current: Errors occur at runtime when keys are missing
   Proposed: Add startup check that warns about missing keys

5. ADD COST TRACKING
   Current: No visibility into API costs
   Proposed: Log token usage and estimated costs per generation

6. DOCUMENT API RESPONSE FORMATS
   Current: Response handling is spread across files
   Proposed: Create TypeScript types for all API responses


================================================================================
SECTION 7: FILES TO REVIEW FOR UNDERSTANDING
================================================================================

Core AI integration:
  src/lib/v0.ts                           - v0 SDK initialization
  src/lib/v0/v0-generator.ts              - Main generation logic
  src/lib/builder/promptAssist.ts         - Prompt enhancement logic
  src/lib/builder/defaults.ts             - Default models and settings

API routes:
  src/app/api/v0/chats/stream/route.ts    - Main v0 generation endpoint
  src/app/api/ai/chat/route.ts            - Prompt assist chat endpoint
  src/app/api/ai/brief/route.ts           - Deep brief generation
  src/app/api/ai/gateway/models/route.ts  - List available AI models

Storage and deployment:
  src/lib/vercelDeploy.ts                 - Deployment to Vercel
  src/lib/vercel/vercel-client.ts         - Vercel API client


================================================================================
SECTION 8: OPEN QUESTIONS TO INVESTIGATE
================================================================================

1. Can the same API key (V0_API_KEY) be used for both v0 and AI Gateway?
   - Or are they separate products with separate billing?

2. What is the token limit for prompts sent to v0?
   - Does Brief Mode risk exceeding this limit?

3. Are there rate limits on v0 generation?
   - What happens when limits are exceeded?

4. Can we get streaming output from AI Gateway?
   - Currently using streaming for v0, but unclear for gateway

5. Is there a way to specify model preferences for v0's internal generation?
   - Or is it always using a fixed model?


================================================================================
END OF DOCUMENT
================================================================================
