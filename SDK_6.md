---

title: RAG Agent

description: Learn how to build a RAG Agent with the AI SDK and Next.js

tags:

&nbsp; \[

&nbsp;   'rag',

&nbsp;   'chatbot',

&nbsp;   'next',

&nbsp;   'embeddings',

&nbsp;   'database',

&nbsp;   'retrieval',

&nbsp;   'memory',

&nbsp;   'agent',

&nbsp; ]

---

\# RAG Agent Guide

In this guide, you will learn how to build a retrieval-augmented generation (RAG) agent.

<video

&nbsp; src="/images/rag-guide-demo.mp4"

&nbsp; autoplay

&nbsp; height={540}

&nbsp; width={910}

&nbsp; controls

&nbsp; playsinline

/>

Before we dive in, let's look at what RAG is, and why we would want to use it.

\### What is RAG?

RAG stands for retrieval augmented generation. In simple terms, RAG is the process of providing a Large Language Model (LLM) with specific information relevant to the prompt.

\### Why is RAG important?

While LLMs are powerful, the information they can reason on is restricted to the data they were trained on. This problem becomes apparent when asking an LLM for information outside of their training data, like proprietary data or common knowledge that has occurred after the model’s training cutoff. RAG solves this problem by fetching information relevant to the prompt and then passing that to the model as context.

To illustrate with a basic example, imagine asking the model for your favorite food:

```txt

\*\*input\*\*

What is my favorite food?



\*\*generation\*\*

I don't have access to personal information about individuals, including their

favorite foods.

```

Not surprisingly, the model doesn’t know. But imagine, alongside your prompt, the model received some extra context:

```txt

\*\*input\*\*

Respond to the user's prompt using only the provided context.

user prompt: 'What is my favorite food?'

context: user loves chicken nuggets



\*\*generation\*\*

Your favorite food is chicken nuggets!

```

Just like that, you have augmented the model’s generation by providing relevant information to the query. Assuming the model has the appropriate information, it is now highly likely to return an accurate response to the users query. But how does it retrieve the relevant information? The answer relies on a concept called embedding.

<Note>

&nbsp; You could fetch any context for your RAG application (eg. Google search).

&nbsp; Embeddings and Vector Databases are just a specific retrieval approach to

&nbsp; achieve semantic search.

</Note>

\### Embedding

\[Embeddings](/docs/ai-sdk-core/embeddings) are a way to represent words, phrases, or images as vectors in a high-dimensional space. In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.

In practice, this means that if you embedded the words `cat` and `dog`, you would expect them to be plotted close to each other in vector space. The process of calculating the similarity between two vectors is called ‘cosine similarity’ where a value of 1 would indicate high similarity and a value of -1 would indicate high opposition.

<Note>

&nbsp; Don’t worry if this seems complicated. a high level understanding is all you

&nbsp; need to get started! For a more in-depth introduction to embeddings, check out

&nbsp; \[this guide](https://jalammar.github.io/illustrated-word2vec/).

</Note>

As mentioned above, embeddings are a way to represent the semantic meaning of \*\*words and phrases\*\*. The implication here is that the larger the input to your embedding, the lower quality the embedding will be. So how would you approach embedding content longer than a simple phrase?

\### Chunking

Chunking refers to the process of breaking down a particular source material into smaller pieces. There are many different approaches to chunking and it’s worth experimenting as the most effective approach can differ by use case. A simple and common approach to chunking (and what you will be using in this guide) is separating written content by sentences.

Once your source material is appropriately chunked, you can embed each one and then store the embedding and the chunk together in a database. Embeddings can be stored in any database that supports vectors. For this tutorial, you will be using \[Postgres](https://www.postgresql.org/) alongside the \[pgvector](https://github.com/pgvector/pgvector) plugin.

<MDXImage

&nbsp; srcLight="/images/rag-guide-1.png"

&nbsp; srcDark="/images/rag-guide-1-dark.png"

&nbsp; width={800}

&nbsp; height={800}

/>

\### All Together Now

Combining all of this together, RAG is the process of enabling the model to respond with information outside of it’s training data by embedding a users query, retrieving the relevant source material (chunks) with the highest semantic similarity, and then passing them alongside the initial query as context. Going back to the example where you ask the model for your favorite food, the prompt preparation process would look like this.

<MDXImage

&nbsp; srcLight="/images/rag-guide-2.png"

&nbsp; srcDark="/images/rag-guide-2-dark.png"

&nbsp; width={800}

&nbsp; height={800}

/>

By passing the appropriate context and refining the model’s objective, you are able to fully leverage its strengths as a reasoning machine.

Onto the project!

\## Project Setup

In this project, you will build a agent that will only respond with information that it has within its knowledge base. The agent will be able to both store and retrieve information. This project has many interesting use cases from customer support through to building your own second brain!

This project will use the following stack:

\- \[Next.js](https://nextjs.org) 14 (App Router)

\- \[ AI SDK ](/docs)

\- \[ Vercel AI Gateway ](/providers/ai-sdk-providers/ai-gateway)

\- \[ Drizzle ORM ](https://orm.drizzle.team)

\- \[ Postgres ](https://www.postgresql.org/) with \[ pgvector ](https://github.com/pgvector/pgvector)

\- \[ shadcn-ui ](https://ui.shadcn.com) and \[ TailwindCSS ](https://tailwindcss.com) for styling

\### Clone Repo

To reduce the scope of this guide, you will be starting with a \[repository](https://github.com/vercel/ai-sdk-rag-starter) that already has a few things set up for you:

\- Drizzle ORM (`lib/db`) including an initial migration and a script to migrate (`db:migrate`)

\- a basic schema for the `resources` table (this will be for source material)

\- a Server Action for creating a `resource`

To get started, clone the starter repository with the following command:

<Snippet

&nbsp; text={\[

&nbsp; 'git clone https://github.com/vercel/ai-sdk-rag-starter',

&nbsp; 'cd ai-sdk-rag-starter',

&nbsp; ]}

/>

First things first, run the following command to install the project’s dependencies:

<Snippet text="pnpm install" />

\### Create Database

You will need a Postgres database to complete this tutorial. If you don't have Postgres setup on your local machine you can:

\- Create a free Postgres database with Vercel (recommended - see instructions below); or

\- Follow \[this guide](https://www.prisma.io/dataguide/postgresql/setting-up-a-local-postgresql-database) to set it up locally

\#### Setting up Postgres with Vercel

To set up a Postgres instance on your Vercel account:

1\. Go to \[Vercel.com](https://vercel.com) and make sure you're logged in

1\. Navigate to your team homepage

1\. Click on the \*\*Integrations\*\* tab

1\. Click \*\*Browse Marketplace\*\*

1\. Look for the \*\*Storage\*\* option in the sidebar

1\. Select the \*\*Neon\*\* option (recommended, but any other PostgreSQL database provider should work)

1\. Click \*\*Install\*\*, then click \*\*Install\*\* again in the top right corner

1\. On the "Get Started with Neon" page, click \*\*Create Database\*\* on the right

1\. Select your region (e.g., Washington, D.C., U.S. East)

1\. Turn off \*\*Auth\*\*

1\. Click \*\*Continue\*\*

1\. Name your database (you can use the default name or rename it to something like "RagTutorial")

1\. Click \*\*Create\*\* in the bottom right corner

1\. After seeing "Database created successfully", click \*\*Done\*\*

1\. You'll be redirected to your database instance

1\. In the Quick Start section, click \*\*Show secrets\*\*

1\. Copy the full `DATABASE\_URL` environment variable

\### Migrate Database

Once you have a Postgres database, you need to add the connection string as an environment secret.

Make a copy of the `.env.example` file and rename it to `.env`.

<Snippet text="cp .env.example .env" />

Open the new `.env` file. You should see an item called `DATABASE\_URL`. Copy in your database connection string after the equals sign.

With that set up, you can now run your first database migration. Run the following command:

<Snippet text="pnpm db:migrate" />

This will first add the `pgvector` extension to your database. Then it will create a new table for your `resources` schema that is defined in `lib/db/schema/resources.ts`. This schema has four columns: `id`, `content`, `createdAt`, and `updatedAt`.

<Note>

&nbsp; If you experience an error with the migration, see the \[troubleshooting

&nbsp; section](#troubleshooting-migration-error) below.

</Note>

\### Vercel AI Gateway Key

For this guide, you will need a Vercel AI Gateway API key, which gives you access to hundreds of models from different providers with one API key. If you haven't obtained your Vercel AI Gateway API key, you can do so by \[signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai\&title=Go+to+AI+Gateway) on the Vercel website.

<Note>

&nbsp; The AI SDK's Vercel AI Gateway Provider is the default global provider, so you

&nbsp; can access models using a simple string in the model configuration. If you

&nbsp; prefer to use a specific provider like OpenAI directly, see the \[provider

&nbsp; management](/docs/ai-sdk-core/provider-management) documentation.

</Note>

Now, open your `.env` file and add your API Gateway key:

```env filename=".env"

AI\_GATEWAY\_API\_KEY=your-api-key

```

Replace `your-api-key` with your actual Vercel AI Gateway API key.

\## Build

Let’s build a quick task list of what needs to be done:

1\. Create a table in your database to store embeddings

2\. Add logic to chunk and create embeddings when creating resources

3\. Create an agent

4\. Give the agent tools to query / create resources for it’s knowledge base

\### Create Embeddings Table

Currently, your application has one table (`resources`) which has a column (`content`) for storing content. Remember, each `resource` (source material) will have to be chunked, embedded, and then stored. Let’s create a table called `embeddings` to store these chunks.

Create a new file (`lib/db/schema/embeddings.ts`) and add the following code:

```tsx filename="lib/db/schema/embeddings.ts"

import { nanoid } from '@/lib/utils';

import { index, pgTable, text, varchar, vector } from 'drizzle-orm/pg-core';

import { resources } from './resources';



export const embeddings = pgTable(

&nbsp; 'embeddings',

&nbsp; {

&nbsp;   id: varchar('id', { length: 191 })

&nbsp;     .primaryKey()

&nbsp;     .$defaultFn(() => nanoid()),

&nbsp;   resourceId: varchar('resource\_id', { length: 191 }).references(

&nbsp;     () => resources.id,

&nbsp;     { onDelete: 'cascade' },

&nbsp;   ),

&nbsp;   content: text('content').notNull(),

&nbsp;   embedding: vector('embedding', { dimensions: 1536 }).notNull(),

&nbsp; },

&nbsp; table => ({

&nbsp;   embeddingIndex: index('embeddingIndex').using(

&nbsp;     'hnsw',

&nbsp;     table.embedding.op('vector\_cosine\_ops'),

&nbsp;   ),

&nbsp; }),

);

```

This table has four columns:

\- `id` - unique identifier

\- `resourceId` - a foreign key relation to the full source material

\- `content` - the plain text chunk

\- `embedding` - the vector representation of the plain text chunk

To perform similarity search, you also need to include an index (\[HNSW](https://github.com/pgvector/pgvector?tab=readme-ov-file#hnsw) or \[IVFFlat](https://github.com/pgvector/pgvector?tab=readme-ov-file#ivfflat)) on this column for better performance.

To push this change to the database, run the following command:

<Snippet text="pnpm db:push" />

\### Add Embedding Logic

Now that you have a table to store embeddings, it’s time to write the logic to create the embeddings.

Create a file with the following command:

<Snippet text="mkdir lib/ai \&\& touch lib/ai/embedding.ts" />

\### Generate Chunks

Remember, to create an embedding, you will start with a piece of source material (unknown length), break it down into smaller chunks, embed each chunk, and then save the chunk to the database. Let’s start by creating a function to break the source material into small chunks.

```tsx filename="lib/ai/embedding.ts"

const generateChunks = (input: string): string\[] => {

&nbsp; return input

&nbsp;   .trim()

&nbsp;   .split('.')

&nbsp;   .filter(i => i !== '');

};

```

This function will take an input string and split it by periods, filtering out any empty items. This will return an array of strings. It is worth experimenting with different chunking techniques in your projects as the best technique will vary.

\### Install AI SDK

You will use the AI SDK to create embeddings. This will require two more dependencies, which you can install by running the following command:

<Snippet text="pnpm add ai @ai-sdk/react" />

This will install the \[AI SDK](/docs) and the AI SDK's React hooks.

<Note>

&nbsp; The AI SDK is designed to be a unified interface to interact with any large

&nbsp; language model. This means that you can change model and providers with just

&nbsp; one line of code! Learn more about \[available providers](/providers) and

&nbsp; \[building custom providers](/providers/community-providers/custom-providers)

&nbsp; in the \[providers](/providers) section.

</Note>

\### Generate Embeddings

Let’s add a function to generate embeddings. Copy the following code into your `lib/ai/embedding.ts` file.

```tsx filename="lib/ai/embedding.ts" highlight="1-2,4,13-22"

import { embedMany } from 'ai';



const embeddingModel = 'openai/text-embedding-ada-002';



const generateChunks = (input: string): string\[] => {

&nbsp; return input

&nbsp;   .trim()

&nbsp;   .split('.')

&nbsp;   .filter(i => i !== '');

};



export const generateEmbeddings = async (

&nbsp; value: string,

): Promise<Array<{ embedding: number\[]; content: string }>> => {

&nbsp; const chunks = generateChunks(value);

&nbsp; const { embeddings } = await embedMany({

&nbsp;   model: embeddingModel,

&nbsp;   values: chunks,

&nbsp; });

&nbsp; return embeddings.map((e, i) => ({ content: chunks\[i], embedding: e }));

};

```

In this code, you first define the model you want to use for the embeddings. In this example, you are using OpenAI’s `text-embedding-ada-002` embedding model.

Next, you create an asynchronous function called `generateEmbeddings`. This function will take in the source material (`value`) as an input and return a promise of an array of objects, each containing an embedding and content. Within the function, you first generate chunks for the input. Then, you pass those chunks to the \[`embedMany`](/docs/reference/ai-sdk-core/embed-many) function imported from the AI SDK which will return embeddings of the chunks you passed in. Finally, you map over and return the embeddings in a format that is ready to save in the database.

\### Update Server Action

Open the file at `lib/actions/resources.ts`. This file has one function, `createResource`, which, as the name implies, allows you to create a resource.

```tsx filename="lib/actions/resources.ts"

'use server';



import {

&nbsp; NewResourceParams,

&nbsp; insertResourceSchema,

&nbsp; resources,

} from '@/lib/db/schema/resources';

import { db } from '../db';



export const createResource = async (input: NewResourceParams) => {

&nbsp; try {

&nbsp;   const { content } = insertResourceSchema.parse(input);



&nbsp;   const \[resource] = await db

&nbsp;     .insert(resources)

&nbsp;     .values({ content })

&nbsp;     .returning();



&nbsp;   return 'Resource successfully created.';

&nbsp; } catch (e) {

&nbsp;   if (e instanceof Error)

&nbsp;     return e.message.length > 0 ? e.message : 'Error, please try again.';

&nbsp; }

};

```

This function is a \[Server Action](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components), as denoted by the `“use server”;` directive at the top of the file. This means that it can be called anywhere in your Next.js application. This function will take an input, run it through a \[Zod](https://zod.dev) schema to ensure it adheres to the correct schema, and then creates a new resource in the database. This is the ideal location to generate and store embeddings of the newly created resources.

Update the file with the following code:

```tsx filename="lib/actions/resources.ts" highlight="9-10,21-27,29"

'use server';



import {

&nbsp; NewResourceParams,

&nbsp; insertResourceSchema,

&nbsp; resources,

} from '@/lib/db/schema/resources';

import { db } from '../db';

import { generateEmbeddings } from '../ai/embedding';

import { embeddings as embeddingsTable } from '../db/schema/embeddings';



export const createResource = async (input: NewResourceParams) => {

&nbsp; try {

&nbsp;   const { content } = insertResourceSchema.parse(input);



&nbsp;   const \[resource] = await db

&nbsp;     .insert(resources)

&nbsp;     .values({ content })

&nbsp;     .returning();



&nbsp;   const embeddings = await generateEmbeddings(content);

&nbsp;   await db.insert(embeddingsTable).values(

&nbsp;     embeddings.map(embedding => ({

&nbsp;       resourceId: resource.id,

&nbsp;       ...embedding,

&nbsp;     })),

&nbsp;   );



&nbsp;   return 'Resource successfully created and embedded.';

&nbsp; } catch (error) {

&nbsp;   return error instanceof Error \&\& error.message.length > 0

&nbsp;     ? error.message

&nbsp;     : 'Error, please try again.';

&nbsp; }

};

```

First, you call the `generateEmbeddings` function created in the previous step, passing in the source material (`content`). Once you have your embeddings (`e`) of the source material, you can save them to the database, passing the `resourceId` alongside each embedding.

\### Create Root Page

Great! Let's build the frontend. The AI SDK’s \[`useChat`](/docs/reference/ai-sdk-ui/use-chat) hook allows you to easily create a conversational user interface for your agent.

Replace your root page (`app/page.tsx`) with the following code.

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();

&nbsp; return (

&nbsp;   <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">

&nbsp;     <div className="space-y-4">

&nbsp;       {messages.map(m => (

&nbsp;         <div key={m.id} className="whitespace-pre-wrap">

&nbsp;           <div>

&nbsp;             <div className="font-bold">{m.role}</div>

&nbsp;             {m.parts.map(part => {

&nbsp;               switch (part.type) {

&nbsp;                 case 'text':

&nbsp;                   return <p>{part.text}</p>;

&nbsp;               }

&nbsp;             })}

&nbsp;           </div>

&nbsp;         </div>

&nbsp;       ))}

&nbsp;     </div>



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         sendMessage({ text: input });

&nbsp;         setInput('');

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"

&nbsp;         value={input}

&nbsp;         placeholder="Say something..."

&nbsp;         onChange={e => setInput(e.currentTarget.value)}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

The `useChat` hook enables the streaming of chat messages from your AI provider (you will be using OpenAI via the Vercel AI Gateway), manages the state for chat input, and updates the UI automatically as new messages are received.

Run the following command to start the Next.js dev server:

<Snippet text="pnpm run dev" />

Head to \[http://localhost:3000](http://localhost:3000/). You should see an empty screen with an input bar floating at the bottom. Try to send a message. The message shows up in the UI for a fraction of a second and then disappears. This is because you haven’t set up the corresponding API route to call the model! By default, `useChat` will send a POST request to the `/api/chat` endpoint with the `messages` as the request body.

<Note>You can customize the endpoint in the useChat configuration object</Note>

\### Create API Route

In Next.js, you can create custom request handlers for a given route using \[Route Handlers](https://nextjs.org/docs/app/building-your-application/routing/route-handlers). Route Handlers are defined in a `route.ts` file and can export HTTP methods like `GET`, `POST`, `PUT`, `PATCH` etc.

Create a file at `app/api/chat/route.ts` by running the following command:

<Snippet text="mkdir -p app/api/chat \&\& touch app/api/chat/route.ts" />

Open the file and add the following code:

```tsx filename="app/api/chat/route.ts"

import { convertToModelMessages, streamText, UIMessage } from 'ai';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

In this code, you declare and export an asynchronous function called POST. You retrieve the `messages` from the request body and then pass them to the \[`streamText`](/docs/reference/ai-sdk-core/stream-text) function imported from the AI SDK, alongside the model you would like to use. Finally, you return the model’s response in `UIMessageStreamResponse` format.

Head back to the browser and try to send a message again. You should see a response from the model streamed directly in!

\### Refining your prompt

While you now have a working agent, it isn't doing anything special.

Let’s add system instructions to refine and restrict the model’s behavior. In this case, you want the model to only use information it has retrieved to generate responses. Update your route handler with the following code:

```tsx filename="app/api/chat/route.ts" highlight="12-14"

import { convertToModelMessages, streamText, UIMessage } from 'ai';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   system: `You are a helpful assistant. Check your knowledge base before answering any questions.

&nbsp;   Only respond to questions using information from tool calls.

&nbsp;   if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Head back to the browser and try to ask the model what your favorite food is. The model should now respond exactly as you instructed above (“Sorry, I don’t know”) given it doesn’t have any relevant information.

In its current form, your agent is now, well, useless. How do you give the model the ability to add and query information?

\### Using Tools

A \[tool](/docs/foundations/tools) is a function that can be called by the model to perform a specific task. You can think of a tool like a program you give to the model that it can run as and when it deems necessary.

Let’s see how you can create a tool to give the model the ability to create, embed and save a resource to your agents’ knowledge base.

\### Add Resource Tool

Update your route handler with the following code:

```tsx filename="app/api/chat/route.ts" highlight="18-29"

import { createResource } from '@/lib/actions/resources';

import { convertToModelMessages, streamText, tool, UIMessage } from 'ai';

import { z } from 'zod';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   system: `You are a helpful assistant. Check your knowledge base before answering any questions.

&nbsp;   Only respond to questions using information from tool calls.

&nbsp;   if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   tools: {

&nbsp;     addResource: tool({

&nbsp;       description: `add a resource to your knowledge base.

&nbsp;         If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,

&nbsp;       inputSchema: z.object({

&nbsp;         content: z

&nbsp;           .string()

&nbsp;           .describe('the content or resource to add to the knowledge base'),

&nbsp;       }),

&nbsp;       execute: async ({ content }) => createResource({ content }),

&nbsp;     }),

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

In this code, you define a tool called `addResource`. This tool has three elements:

\- \*\*description\*\*: description of the tool that will influence when the tool is picked.

\- \*\*inputSchema\*\*: \[Zod schema](/docs/foundations/tools#schema-specification-and-validation-with-zod) that defines the input necessary for the tool to run.

\- \*\*execute\*\*: An asynchronous function that is called with the arguments from the tool call.

In simple terms, on each generation, the model will decide whether it should call the tool. If it deems it should call the tool, it will extract the input and then append a new `message` to the `messages` array of type `tool-call`. The AI SDK will then run the `execute` function with the parameters provided by the `tool-call` message.

Head back to the browser and tell the model your favorite food. You should see an empty response in the UI. Did anything happen? Let’s see. Run the following command in a new terminal window.

<Snippet text="pnpm db:studio" />

This will start Drizzle Studio where we can view the rows in our database. You should see a new row in both the `embeddings` and `resources` table with your favorite food!

Let’s make a few changes in the UI to communicate to the user when a tool has been called. Head back to your root page (`app/page.tsx`) and add the following code:

```tsx filename="app/page.tsx" highlight="14-32"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();

&nbsp; return (

&nbsp;   <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">

&nbsp;     <div className="space-y-4">

&nbsp;       {messages.map(m => (

&nbsp;         <div key={m.id} className="whitespace-pre-wrap">

&nbsp;           <div>

&nbsp;             <div className="font-bold">{m.role}</div>

&nbsp;             {m.parts.map(part => {

&nbsp;               switch (part.type) {

&nbsp;                 case 'text':

&nbsp;                   return <p>{part.text}</p>;

&nbsp;                 case 'tool-addResource':

&nbsp;                 case 'tool-getInformation':

&nbsp;                   return (

&nbsp;                     <p>

&nbsp;                       call{part.state === 'output-available' ? 'ed' : 'ing'}{' '}

&nbsp;                       tool: {part.type}

&nbsp;                       <pre className="my-4 bg-zinc-100 p-2 rounded-sm">

&nbsp;                         {JSON.stringify(part.input, null, 2)}

&nbsp;                       </pre>

&nbsp;                     </p>

&nbsp;                   );

&nbsp;               }

&nbsp;             })}

&nbsp;           </div>

&nbsp;         </div>

&nbsp;       ))}

&nbsp;     </div>



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         sendMessage({ text: input });

&nbsp;         setInput('');

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"

&nbsp;         value={input}

&nbsp;         placeholder="Say something..."

&nbsp;         onChange={e => setInput(e.currentTarget.value)}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

With this change, you now conditionally render the tool that has been called directly in the UI. Save the file and head back to browser. Tell the model your favorite movie. You should see which tool is called in place of the model’s typical text response.

<Note>

&nbsp; Don't worry about the `tool-getInformation` tool case in the switch statement

&nbsp; - we'll add that tool in a later section.

</Note>

\### Improving UX with Multi-Step Calls

It would be nice if the model could summarize the action too. However, technically, once the model calls a tool, it has completed its generation as it ‘generated’ a tool call. How could you achieve this desired behavior?

The AI SDK has a feature called \[`stopWhen`](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) which allows stopping conditions when the model generates a tool call. If those stopping conditions haven't been hit, the AI SDK will automatically send tool call results back to the model!

Open your root page (`api/chat/route.ts`) and add the following key to the `streamText` configuration object:

```tsx filename="api/chat/route.ts" highlight="8,24"

import { createResource } from '@/lib/actions/resources';

import {

&nbsp; convertToModelMessages,

&nbsp; streamText,

&nbsp; tool,

&nbsp; UIMessage,

&nbsp; stepCountIs,

} from 'ai';

import { z } from 'zod';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   system: `You are a helpful assistant. Check your knowledge base before answering any questions.

&nbsp;   Only respond to questions using information from tool calls.

&nbsp;   if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   stopWhen: stepCountIs(5),

&nbsp;   tools: {

&nbsp;     addResource: tool({

&nbsp;       description: `add a resource to your knowledge base.

&nbsp;         If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,

&nbsp;       inputSchema: z.object({

&nbsp;         content: z

&nbsp;           .string()

&nbsp;           .describe('the content or resource to add to the knowledge base'),

&nbsp;       }),

&nbsp;       execute: async ({ content }) => createResource({ content }),

&nbsp;     }),

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Head back to the browser and tell the model your favorite pizza topping (note: pineapple is not an option). You should see a follow-up response from the model confirming the action.

\### Retrieve Resource Tool

The model can now add and embed arbitrary information to your knowledge base. However, it still isn’t able to query it. Let’s create a new tool to allow the model to answer questions by finding relevant information in your knowledge base.

To find similar content, you will need to embed the users query, search the database for semantic similarities, then pass those items to the model as context alongside the query. To achieve this, let’s update your embedding logic file (`lib/ai/embedding.ts`):

```tsx filename="lib/ai/embedding.ts" highlight="1,3-5,27-34,36-49"

import { embed, embedMany } from 'ai';

import { db } from '../db';

import { cosineDistance, desc, gt, sql } from 'drizzle-orm';

import { embeddings } from '../db/schema/embeddings';



const embeddingModel = 'openai/text-embedding-ada-002';



const generateChunks = (input: string): string\[] => {

&nbsp; return input

&nbsp;   .trim()

&nbsp;   .split('.')

&nbsp;   .filter(i => i !== '');

};



export const generateEmbeddings = async (

&nbsp; value: string,

): Promise<Array<{ embedding: number\[]; content: string }>> => {

&nbsp; const chunks = generateChunks(value);

&nbsp; const { embeddings } = await embedMany({

&nbsp;   model: embeddingModel,

&nbsp;   values: chunks,

&nbsp; });

&nbsp; return embeddings.map((e, i) => ({ content: chunks\[i], embedding: e }));

};



export const generateEmbedding = async (value: string): Promise<number\[]> => {

&nbsp; const input = value.replaceAll('\\\\n', ' ');

&nbsp; const { embedding } = await embed({

&nbsp;   model: embeddingModel,

&nbsp;   value: input,

&nbsp; });

&nbsp; return embedding;

};



export const findRelevantContent = async (userQuery: string) => {

&nbsp; const userQueryEmbedded = await generateEmbedding(userQuery);

&nbsp; const similarity = sql<number>`1 - (${cosineDistance(

&nbsp;   embeddings.embedding,

&nbsp;   userQueryEmbedded,

&nbsp; )})`;

&nbsp; const similarGuides = await db

&nbsp;   .select({ name: embeddings.content, similarity })

&nbsp;   .from(embeddings)

&nbsp;   .where(gt(similarity, 0.5))

&nbsp;   .orderBy(t => desc(t.similarity))

&nbsp;   .limit(4);

&nbsp; return similarGuides;

};

```

In this code, you add two functions:

\- `generateEmbedding`: generate a single embedding from an input string

\- `findRelevantContent`: embeds the user’s query, searches the database for similar items, then returns relevant items

With that done, it’s onto the final step: creating the tool.

Go back to your route handler (`api/chat/route.ts`) and add a new tool called `getInformation`:

```ts filename="api/chat/route.ts" highlight="11,37-43"

import { createResource } from '@/lib/actions/resources';

import {

&nbsp; convertToModelMessages,

&nbsp; streamText,

&nbsp; tool,

&nbsp; UIMessage,

&nbsp; stepCountIs,

} from 'ai';

import { z } from 'zod';

import { findRelevantContent } from '@/lib/ai/embedding';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   stopWhen: stepCountIs(5),

&nbsp;   system: `You are a helpful assistant. Check your knowledge base before answering any questions.

&nbsp;   Only respond to questions using information from tool calls.

&nbsp;   if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,

&nbsp;   tools: {

&nbsp;     addResource: tool({

&nbsp;       description: `add a resource to your knowledge base.

&nbsp;         If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,

&nbsp;       inputSchema: z.object({

&nbsp;         content: z

&nbsp;           .string()

&nbsp;           .describe('the content or resource to add to the knowledge base'),

&nbsp;       }),

&nbsp;       execute: async ({ content }) => createResource({ content }),

&nbsp;     }),

&nbsp;     getInformation: tool({

&nbsp;       description: `get information from your knowledge base to answer questions.`,

&nbsp;       inputSchema: z.object({

&nbsp;         question: z.string().describe('the users question'),

&nbsp;       }),

&nbsp;       execute: async ({ question }) => findRelevantContent(question),

&nbsp;     }),

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Head back to the browser, refresh the page, and ask for your favorite food. You should see the model call the `getInformation` tool, and then use the relevant information to formulate a response!

\## Conclusion

Congratulations, you have successfully built an AI agent that can dynamically add and retrieve information to and from a knowledge base. Throughout this guide, you learned how to create and store embeddings, set up server actions to manage resources, and use tools to extend the capabilities of your agent.

\## Troubleshooting Migration Error

If you experience an error with the migration, open your migration file (`lib/db/migrations/0000\_yielding\_bloodaxe.sql`), cut (copy and remove) the first line, and run it directly on your postgres instance. You should now be able to run the updated migration.

If you're using the Vercel setup above, you can run the command directly by either:

\- Going to the Neon console and entering the command there, or

\- Going back to the Vercel platform, navigating to the Quick Start section of your database, and finding the PSQL connection command (second tab). This will connect to your instance in the terminal where you can run the command directly.

\[More info](https://github.com/vercel/ai-sdk-rag-starter/issues/1).

---

title: Multi-Modal Agent

description: Learn how to build a multi-modal agent that can process images and PDFs with the AI SDK.

tags: \['multi-modal', 'agent', 'images', 'pdf', 'vision', 'next']

---

\# Multi-Modal Agent

In this guide, you will build a multi-modal agent capable of understanding both images and PDFs.

Multi-modal refers to the ability of the agent to understand and generate responses in multiple formats. In this guide, we'll focus on images and PDFs - two common document types that modern language models can process natively.

<Note>

&nbsp; For a complete list of providers and their multi-modal capabilities, visit the

&nbsp; \[providers documentation](/providers/ai-sdk-providers).

</Note>

We'll build this agent using OpenAI's GPT-4o, but the same code works seamlessly with other providers - you can switch between them by changing just one line of code.

\## Prerequisites

To follow this quickstart, you'll need:

\- Node.js 18+ and pnpm installed on your local development machine.

\- A Vercel AI Gateway API key.

If you haven't obtained your Vercel AI Gateway API key, you can do so by \[signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai\&title=Go+to+AI+Gateway) on the Vercel website.

\## Create Your Application

Start by creating a new Next.js application. This command will create a new directory named `multi-modal-agent` and set up a basic Next.js application inside it.

<div className="mb-4">

&nbsp; <Note>

&nbsp; Be sure to select yes when prompted to use the App Router. If you are

&nbsp; looking for the Next.js Pages Router quickstart guide, you can find it

&nbsp; \[here](/docs/getting-started/nextjs-pages-router).

&nbsp; </Note>

</div>

<Snippet text="pnpm create next-app@latest multi-modal-agent" />

Navigate to the newly created directory:

<Snippet text="cd multi-modal-agent" />

\### Install dependencies

Install `ai` and `@ai-sdk/react`, the AI SDK package and the AI SDK's React package respectively.

<Note>

&nbsp; The AI SDK is designed to be a unified interface to interact with any large

&nbsp; language model. This means that you can change model and providers with just

&nbsp; one line of code! Learn more about \[available providers](/providers) and

&nbsp; \[building custom providers](/providers/community-providers/custom-providers)

&nbsp; in the \[providers](/providers) section.

</Note>

<div className="my-4">

&nbsp; <Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add ai @ai-sdk/react" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install ai @ai-sdk/react" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add ai @ai-sdk/react" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add ai @ai-sdk/react" dark />

&nbsp; </Tab>

&nbsp; </Tabs>

</div>

\### Configure your Vercel AI Gateway API key

Create a `.env.local` file in your project root and add your Vercel AI Gateway API key. This key authenticates your application with Vercel AI Gateway.

<Snippet text="touch .env.local" />

Edit the `.env.local` file:

```env filename=".env.local"

AI\_GATEWAY\_API\_KEY=your\_api\_key\_here

```

Replace `your\_api\_key\_here` with your actual Vercel AI Gateway API key.

<Note className="mb-4">

&nbsp; The AI SDK's Vercel AI Gateway Provider is the default global provider, so you

&nbsp; can access models using a simple string in the model configuration. If you

&nbsp; prefer to use a specific provider like OpenAI directly, see the \[provider

&nbsp; management](/docs/ai-sdk-core/provider-management) documentation.

</Note>

\## Implementation Plan

To build a multi-modal agent, you will need to:

\- Create a Route Handler to handle incoming chat messages and generate responses.

\- Wire up the UI to display chat messages, provide a user input, and handle submitting new messages.

\- Add the ability to upload images and PDFs and attach them alongside the chat messages.

\## Create a Route Handler

Create a route handler, `app/api/chat/route.ts` and add the following code:

```tsx filename="app/api/chat/route.ts"

import { streamText, convertToModelMessages, type UIMessage } from 'ai';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Let's take a look at what is happening in this code:

1\. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the agent and provides the agent with the necessary context to make the next generation.

2\. Convert the UI messages to model messages using `convertToModelMessages`, which transforms the UI-focused message format to the format expected by the language model.

3\. Call \[`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (converted in step 2). You can pass additional \[settings](/docs/ai-sdk-core/settings) to further customize the model's behavior.

4\. The `streamText` function returns a \[`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the \[ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.

5\. Finally, return the result to the client to stream the response.

This Route Handler creates a POST request endpoint at `/api/chat`.

\## Wire up the UI

Now that you have a Route Handler that can query a large language model (LLM), it's time to setup your frontend. \[ AI SDK UI ](/docs/ai-sdk-ui) abstracts the complexity of a chat interface into one hook, \[`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`app/page.tsx`) with the following code to show a list of chat messages and provide a user message input:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');



&nbsp; const { messages, sendMessage } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });



&nbsp; return (

&nbsp;   <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id} className="whitespace-pre-wrap">

&nbsp;         {m.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {m.parts.map((part, index) => {

&nbsp;           if (part.type === 'text') {

&nbsp;             return <span key={`${m.id}-text-${index}`}>{part.text}</span>;

&nbsp;           }

&nbsp;           return null;

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form

&nbsp;       onSubmit={async event => {

&nbsp;         event.preventDefault();

&nbsp;         sendMessage({

&nbsp;           role: 'user',

&nbsp;           parts: \[{ type: 'text', text: input }],

&nbsp;         });

&nbsp;         setInput('');

&nbsp;       }}

&nbsp;       className="fixed bottom-0 w-full max-w-md mb-8 border border-gray-300 rounded shadow-xl"

&nbsp;     >

&nbsp;       <input

&nbsp;         className="w-full p-2"

&nbsp;         value={input}

&nbsp;         placeholder="Say something..."

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

<Note>

&nbsp; Make sure you add the `"use client"` directive to the top of your file. This

&nbsp; allows you to add interactivity with JavaScript.

</Note>

This page utilizes the `useChat` hook, configured with `DefaultChatTransport` to specify the API endpoint. The `useChat` hook provides multiple utility functions and state variables:

\- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).

\- `sendMessage` - function to send a new message to the AI.

\- Each message contains a `parts` array that can include text, images, PDFs, and other content types.

\- Files are converted to data URLs before being sent to maintain compatibility across different environments.

\## Add File Upload

To make your agent multi-modal, let's add the ability to upload and send both images and PDFs to the model. In v5, files are sent as part of the message's `parts` array. Files are converted to data URLs using the FileReader API before being sent to the server.

Update your root page (`app/page.tsx`) with the following code:

```tsx filename="app/page.tsx" highlight="4-5,10-12,15-39,46-81,87-97"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useRef, useState } from 'react';

import Image from 'next/image';



async function convertFilesToDataURLs(files: FileList) {

&nbsp; return Promise.all(

&nbsp;   Array.from(files).map(

&nbsp;     file =>

&nbsp;       new Promise<{

&nbsp;         type: 'file';

&nbsp;         mediaType: string;

&nbsp;         url: string;

&nbsp;       }>((resolve, reject) => {

&nbsp;         const reader = new FileReader();

&nbsp;         reader.onload = () => {

&nbsp;           resolve({

&nbsp;             type: 'file',

&nbsp;             mediaType: file.type,

&nbsp;             url: reader.result as string,

&nbsp;           });

&nbsp;         };

&nbsp;         reader.onerror = reject;

&nbsp;         reader.readAsDataURL(file);

&nbsp;       }),

&nbsp;   ),

&nbsp; );

}



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const \[files, setFiles] = useState<FileList | undefined>(undefined);

&nbsp; const fileInputRef = useRef<HTMLInputElement>(null);



&nbsp; const { messages, sendMessage } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });



&nbsp; return (

&nbsp;   <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id} className="whitespace-pre-wrap">

&nbsp;         {m.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {m.parts.map((part, index) => {

&nbsp;           if (part.type === 'text') {

&nbsp;             return <span key={`${m.id}-text-${index}`}>{part.text}</span>;

&nbsp;           }

&nbsp;           if (part.type === 'file' \&\& part.mediaType?.startsWith('image/')) {

&nbsp;             return (

&nbsp;               <Image

&nbsp;                 key={`${m.id}-image-${index}`}

&nbsp;                 src={part.url}

&nbsp;                 width={500}

&nbsp;                 height={500}

&nbsp;                 alt={`attachment-${index}`}

&nbsp;               />

&nbsp;             );

&nbsp;           }

&nbsp;           if (part.type === 'file' \&\& part.mediaType === 'application/pdf') {

&nbsp;             return (

&nbsp;               <iframe

&nbsp;                 key={`${m.id}-pdf-${index}`}

&nbsp;                 src={part.url}

&nbsp;                 width={500}

&nbsp;                 height={600}

&nbsp;                 title={`pdf-${index}`}

&nbsp;               />

&nbsp;             );

&nbsp;           }

&nbsp;           return null;

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form

&nbsp;       className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2"

&nbsp;       onSubmit={async event => {

&nbsp;         event.preventDefault();



&nbsp;         const fileParts =

&nbsp;           files \&\& files.length > 0

&nbsp;             ? await convertFilesToDataURLs(files)

&nbsp;             : \[];



&nbsp;         sendMessage({

&nbsp;           role: 'user',

&nbsp;           parts: \[{ type: 'text', text: input }, ...fileParts],

&nbsp;         });



&nbsp;         setInput('');

&nbsp;         setFiles(undefined);



&nbsp;         if (fileInputRef.current) {

&nbsp;           fileInputRef.current.value = '';

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         type="file"

&nbsp;         accept="image/\*,application/pdf"

&nbsp;         className=""

&nbsp;         onChange={event => {

&nbsp;           if (event.target.files) {

&nbsp;             setFiles(event.target.files);

&nbsp;           }

&nbsp;         }}

&nbsp;         multiple

&nbsp;         ref={fileInputRef}

&nbsp;       />

&nbsp;       <input

&nbsp;         className="w-full p-2"

&nbsp;         value={input}

&nbsp;         placeholder="Say something..."

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

In this code, you:

1\. Add a helper function `convertFilesToDataURLs` to convert file uploads to data URLs.

1\. Create state to hold the input text, files, and a ref to the file input field.

1\. Configure `useChat` with `DefaultChatTransport` to specify the API endpoint.

1\. Display messages using the `parts` array structure, rendering text, images, and PDFs appropriately.

1\. Update the `onSubmit` function to send messages with the `sendMessage` function, including both text and file parts.

1\. Add a file input field to the form, including an `onChange` handler to handle updating the files state.

\## Running Your Application

With that, you have built everything you need for your multi-modal agent! To start your application, use the command:

<Snippet text="pnpm run dev" />

Head to your browser and open http://localhost:3000. You should see an input field and a button to upload files.

Try uploading an image or PDF and asking the model questions about it. Watch as the model's response is streamed back to you!

\## Using Other Providers

With the AI SDK's unified provider interface you can easily switch to other providers that support multi-modal capabilities:

```tsx filename="app/api/chat/route.ts"

// Using Anthropic

const result = streamText({

&nbsp; model: 'anthropic/claude-sonnet-4-20250514',

&nbsp; messages: await convertToModelMessages(messages),

});



// Using Google

const result = streamText({

&nbsp; model: 'google/gemini-2.5-flash',

&nbsp; messages: await convertToModelMessages(messages),

});

```

Install the provider package (`@ai-sdk/anthropic` or `@ai-sdk/google`) and update your API keys in `.env.local`. The rest of your code remains the same.

<Note>

&nbsp; Different providers may have varying file size limits and performance

&nbsp; characteristics. Check the \[provider

&nbsp; documentation](/providers/ai-sdk-providers) for specific details.

</Note>

\## Where to Next?

You've built a multi-modal AI agent using the AI SDK! Experiment and extend the functionality of this application further by exploring \[tool calling](/docs/ai-sdk-core/tools-and-tool-calling).

---

title: Slackbot Agent Guide

description: Learn how to use the AI SDK to build an AI Agent in Slack.

tags: \['agents', 'chatbot']

---

\# Building an AI Agent in Slack with the AI SDK

In this guide, you will learn how to build a Slackbot powered by the AI SDK. The bot will be able to respond to direct messages and mentions in channels using the full context of the thread.

\## Slack App Setup

Before we start building, you'll need to create and configure a Slack app:

1\. Go to \[api.slack.com/apps](https://api.slack.com/apps)

2\. Click "Create New App" and choose "From scratch"

3\. Give your app a name and select your workspace

4\. Under "OAuth \& Permissions", add the following bot token scopes:

&nbsp; - `app\_mentions:read`

&nbsp; - `chat:write`

&nbsp; - `im:history`

&nbsp; - `im:write`

&nbsp; - `assistant:write`

5\. Install the app to your workspace (button under "OAuth Tokens" subsection)

6\. Copy the Bot User OAuth Token and Signing Secret for the next step

7\. Under App Home -> Show Tabs -> Chat Tab, check "Allow users to send Slash commands and messages from the chat tab"

\## Project Setup

This project uses the following stack:

\- \[AI SDK by Vercel](/docs)

\- \[Slack Web API](https://api.slack.com/web)

\- \[Vercel](https://vercel.com)

\- \[OpenAI](https://openai.com)

\## Getting Started

1\. Clone \[the repository](https://github.com/vercel-labs/ai-sdk-slackbot) and check out the `starter` branch

<Snippet

&nbsp; text={\[

&nbsp; 'git clone https://github.com/vercel-labs/ai-sdk-slackbot.git',

&nbsp; 'cd ai-sdk-slackbot',

&nbsp; 'git checkout starter',

&nbsp; ]}

/>

2\. Install dependencies

<Snippet text={\['pnpm install']} />

\## Project Structure

The starter repository already includes:

\- Slack utilities (`lib/slack-utils.ts`) including functions for validating incoming requests, converting Slack threads to AI SDK compatible message formats, and getting the Slackbot's user ID

\- General utility functions (`lib/utils.ts`) including initial Exa setup

\- Files to handle the different types of Slack events (`lib/handle-messages.ts` and `lib/handle-app-mention.ts`)

\- An API endpoint (`POST`) for Slack events (`api/events.ts`)

\## Event Handler

First, let's take a look at our API route (`api/events.ts`):

```typescript

import type { SlackEvent } from '@slack/web-api';

import {

&nbsp; assistantThreadMessage,

&nbsp; handleNewAssistantMessage,

} from '../lib/handle-messages';

import { waitUntil } from '@vercel/functions';

import { handleNewAppMention } from '../lib/handle-app-mention';

import { verifyRequest, getBotId } from '../lib/slack-utils';



export async function POST(request: Request) {

&nbsp; const rawBody = await request.text();

&nbsp; const payload = JSON.parse(rawBody);

&nbsp; const requestType = payload.type as 'url\_verification' | 'event\_callback';



&nbsp; // See https://api.slack.com/events/url\_verification

&nbsp; if (requestType === 'url\_verification') {

&nbsp;   return new Response(payload.challenge, { status: 200 });

&nbsp; }



&nbsp; await verifyRequest({ requestType, request, rawBody });



&nbsp; try {

&nbsp;   const botUserId = await getBotId();



&nbsp;   const event = payload.event as SlackEvent;



&nbsp;   if (event.type === 'app\_mention') {

&nbsp;     waitUntil(handleNewAppMention(event, botUserId));

&nbsp;   }



&nbsp;   if (event.type === 'assistant\_thread\_started') {

&nbsp;     waitUntil(assistantThreadMessage(event));

&nbsp;   }



&nbsp;   if (

&nbsp;     event.type === 'message' \&\&

&nbsp;     !event.subtype \&\&

&nbsp;     event.channel\_type === 'im' \&\&

&nbsp;     !event.bot\_id \&\&

&nbsp;     !event.bot\_profile \&\&

&nbsp;     event.bot\_id !== botUserId

&nbsp;   ) {

&nbsp;     waitUntil(handleNewAssistantMessage(event, botUserId));

&nbsp;   }



&nbsp;   return new Response('Success!', { status: 200 });

&nbsp; } catch (error) {

&nbsp;   console.error('Error generating response', error);

&nbsp;   return new Response('Error generating response', { status: 500 });

&nbsp; }

}

```

This file defines a `POST` function that handles incoming requests from Slack. First, you check the request type to see if it's a URL verification request. If it is, you respond with the challenge string provided by Slack. If it's an event callback, you verify the request and then have access to the event data. This is where you can implement your event handling logic.

You then handle three types of events: `app\_mention`, `assistant\_thread\_started`, and `message`:

\- For `app\_mention`, you call `handleNewAppMention` with the event and the bot user ID.

\- For `assistant\_thread\_started`, you call `assistantThreadMessage` with the event.

\- For `message`, you call `handleNewAssistantMessage` with the event and the bot user ID.

Finally, you respond with a success message to Slack. Note, each handler function is wrapped in a `waitUntil` function. Let's take a look at what this means and why it's important.

\### The waitUntil Function

Slack expects a response within 3 seconds to confirm the request is being handled. However, generating AI responses can take longer. If you don't respond to the Slack request within 3 seconds, Slack will send another request, leading to another invocation of your API route, another call to the LLM, and ultimately another response to the user. To solve this, you can use the `waitUntil` function, which allows you to run your AI logic after the response is sent, without blocking the response itself.

This means, your API endpoint will:

1\. Immediately respond to Slack (within 3 seconds)

2\. Continue processing the message asynchronously

3\. Send the AI response when it's ready

\## Event Handlers

Let's look at how each event type is currently handled.

\### App Mentions

When a user mentions your bot in a channel, the `app\_mention` event is triggered. The `handleNewAppMention` function in `handle-app-mention.ts` processes these mentions:

1\. Checks if the message is from a bot to avoid infinite response loops

2\. Creates a status updater to show the bot is "thinking"

3\. If the mention is in a thread, it retrieves the thread history

4\. Calls the LLM with the message content (using the `generateResponse` function which you will implement in the next section)

5\. Updates the initial "thinking" message with the AI response

Here's the code for the `handleNewAppMention` function:

```typescript filename="lib/handle-app-mention.ts"

import { AppMentionEvent } from '@slack/web-api';

import { client, getThread } from './slack-utils';

import { generateResponse } from './ai';



const updateStatusUtil = async (

&nbsp; initialStatus: string,

&nbsp; event: AppMentionEvent,

) => {

&nbsp; const initialMessage = await client.chat.postMessage({

&nbsp;   channel: event.channel,

&nbsp;   thread\_ts: event.thread\_ts ?? event.ts,

&nbsp;   text: initialStatus,

&nbsp; });



&nbsp; if (!initialMessage || !initialMessage.ts)

&nbsp;   throw new Error('Failed to post initial message');



&nbsp; const updateMessage = async (status: string) => {

&nbsp;   await client.chat.update({

&nbsp;     channel: event.channel,

&nbsp;     ts: initialMessage.ts as string,

&nbsp;     text: status,

&nbsp;   });

&nbsp; };

&nbsp; return updateMessage;

};



export async function handleNewAppMention(

&nbsp; event: AppMentionEvent,

&nbsp; botUserId: string,

) {

&nbsp; console.log('Handling app mention');

&nbsp; if (event.bot\_id || event.bot\_id === botUserId || event.bot\_profile) {

&nbsp;   console.log('Skipping app mention');

&nbsp;   return;

&nbsp; }



&nbsp; const { thread\_ts, channel } = event;

&nbsp; const updateMessage = await updateStatusUtil('is thinking...', event);



&nbsp; if (thread\_ts) {

&nbsp;   const messages = await getThread(channel, thread\_ts, botUserId);

&nbsp;   const result = await generateResponse(messages, updateMessage);

&nbsp;   updateMessage(result);

&nbsp; } else {

&nbsp;   const result = await generateResponse(

&nbsp;     \[{ role: 'user', content: event.text }],

&nbsp;     updateMessage,

&nbsp;   );

&nbsp;   updateMessage(result);

&nbsp; }

}

```

Now let's see how new assistant threads and messages are handled.

\### Assistant Thread Messages

When a user starts a thread with your assistant, the `assistant\_thread\_started` event is triggered. The `assistantThreadMessage` function in `handle-messages.ts` handles this:

1\. Posts a welcome message to the thread

2\. Sets up suggested prompts to help users get started

Here's the code for the `assistantThreadMessage` function:

```typescript filename="lib/handle-messages.ts"

import type { AssistantThreadStartedEvent } from '@slack/web-api';

import { client } from './slack-utils';



export async function assistantThreadMessage(

&nbsp; event: AssistantThreadStartedEvent,

) {

&nbsp; const { channel\_id, thread\_ts } = event.assistant\_thread;

&nbsp; console.log(`Thread started: ${channel\_id} ${thread\_ts}`);

&nbsp; console.log(JSON.stringify(event));



&nbsp; await client.chat.postMessage({

&nbsp;   channel: channel\_id,

&nbsp;   thread\_ts: thread\_ts,

&nbsp;   text: "Hello, I'm an AI assistant built with the AI SDK by Vercel!",

&nbsp; });



&nbsp; await client.assistant.threads.setSuggestedPrompts({

&nbsp;   channel\_id: channel\_id,

&nbsp;   thread\_ts: thread\_ts,

&nbsp;   prompts: \[

&nbsp;     {

&nbsp;       title: 'Get the weather',

&nbsp;       message: 'What is the current weather in London?',

&nbsp;     },

&nbsp;     {

&nbsp;       title: 'Get the news',

&nbsp;       message: 'What is the latest Premier League news from the BBC?',

&nbsp;     },

&nbsp;   ],

&nbsp; });

}

```

\### Direct Messages

For direct messages to your bot, the `message` event is triggered and the event is handled by the `handleNewAssistantMessage` function in `handle-messages.ts`:

1\. Verifies the message isn't from a bot

2\. Updates the status to show the response is being generated

3\. Retrieves the conversation history

4\. Calls the LLM with the conversation context

5\. Posts the LLM's response to the thread

Here's the code for the `handleNewAssistantMessage` function:

```typescript filename="lib/handle-messages.ts"

import type { GenericMessageEvent } from '@slack/web-api';

import { client, getThread } from './slack-utils';

import { generateResponse } from './ai';



export async function handleNewAssistantMessage(

&nbsp; event: GenericMessageEvent,

&nbsp; botUserId: string,

) {

&nbsp; if (

&nbsp;   event.bot\_id ||

&nbsp;   event.bot\_id === botUserId ||

&nbsp;   event.bot\_profile ||

&nbsp;   !event.thread\_ts

&nbsp; )

&nbsp;   return;



&nbsp; const { thread\_ts, channel } = event;

&nbsp; const updateStatus = updateStatusUtil(channel, thread\_ts);

&nbsp; updateStatus('is thinking...');



&nbsp; const messages = await getThread(channel, thread\_ts, botUserId);

&nbsp; const result = await generateResponse(messages, updateStatus);



&nbsp; await client.chat.postMessage({

&nbsp;   channel: channel,

&nbsp;   thread\_ts: thread\_ts,

&nbsp;   text: result,

&nbsp;   unfurl\_links: false,

&nbsp;   blocks: \[

&nbsp;     {

&nbsp;       type: 'section',

&nbsp;       text: {

&nbsp;         type: 'mrkdwn',

&nbsp;         text: result,

&nbsp;       },

&nbsp;     },

&nbsp;   ],

&nbsp; });



&nbsp; updateStatus('');

}

```

With the event handlers in place, let's now implement the AI logic.

\## Implementing AI Logic

The core of our application is the `generateResponse` function in `lib/generate-response.ts`, which processes messages and generates responses using the AI SDK.

Here's how to implement it:

```typescript filename="lib/generate-response.ts"

import { generateText, ModelMessage } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



export const generateResponse = async (

&nbsp; messages: ModelMessage\[],

&nbsp; updateStatus?: (status: string) => void,

) => {

&nbsp; const { text } = await generateText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   system: `You are a Slack bot assistant. Keep your responses concise and to the point.

&nbsp;   - Do not tag users.

&nbsp;   - Current date is: ${new Date().toISOString().split('T')\[0]}`,

&nbsp;   messages,

&nbsp; });



&nbsp; // Convert markdown to Slack mrkdwn format

&nbsp; return text.replace(/\\\[(.\*?)\\]\\((.\*?)\\)/g, '<$2|$1>').replace(/\\\*\\\*/g, '\*');

};

```

This basic implementation:

1\. Uses the AI SDK's `generateText` function to call Anthropic's `claude-sonnet-4.5` model

2\. Provides a system prompt to guide the model's behavior

3\. Formats the response for Slack's markdown format

\## Enhancing with Tools

The real power of the AI SDK comes from tools that enable your bot to perform actions. Let's add two useful tools:

```typescript filename="lib/generate-response.ts"

import { generateText, tool, ModelMessage, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';

import { exa } from './utils';



export const generateResponse = async (

&nbsp; messages: ModelMessage\[],

&nbsp; updateStatus?: (status: string) => void,

) => {

&nbsp; const { text } = await generateText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   system: `You are a Slack bot assistant. Keep your responses concise and to the point.

&nbsp;   - Do not tag users.

&nbsp;   - Current date is: ${new Date().toISOString().split('T')\[0]}

&nbsp;   - Always include sources in your final response if you use web search.`,

&nbsp;   messages,

&nbsp;   stopWhen: stepCountIs(10),

&nbsp;   tools: {

&nbsp;     getWeather: tool({

&nbsp;       description: 'Get the current weather at a location',

&nbsp;       inputSchema: z.object({

&nbsp;         latitude: z.number(),

&nbsp;         longitude: z.number(),

&nbsp;         city: z.string(),

&nbsp;       }),

&nbsp;       execute: async ({ latitude, longitude, city }) => {

&nbsp;         updateStatus?.(`is getting weather for ${city}...`);



&nbsp;         const response = await fetch(

&nbsp;           `https://api.open-meteo.com/v1/forecast?latitude=${latitude}\&longitude=${longitude}\&current=temperature\_2m,weathercode,relativehumidity\_2m\&timezone=auto`,

&nbsp;         );



&nbsp;         const weatherData = await response.json();

&nbsp;         return {

&nbsp;           temperature: weatherData.current.temperature\_2m,

&nbsp;           weatherCode: weatherData.current.weathercode,

&nbsp;           humidity: weatherData.current.relativehumidity\_2m,

&nbsp;           city,

&nbsp;         };

&nbsp;       },

&nbsp;     }),

&nbsp;     searchWeb: tool({

&nbsp;       description: 'Use this to search the web for information',

&nbsp;       inputSchema: z.object({

&nbsp;         query: z.string(),

&nbsp;         specificDomain: z

&nbsp;           .string()

&nbsp;           .nullable()

&nbsp;           .describe(

&nbsp;             'a domain to search if the user specifies e.g. bbc.com. Should be only the domain name without the protocol',

&nbsp;           ),

&nbsp;       }),

&nbsp;       execute: async ({ query, specificDomain }) => {

&nbsp;         updateStatus?.(`is searching the web for ${query}...`);

&nbsp;         const { results } = await exa.searchAndContents(query, {

&nbsp;           livecrawl: 'always',

&nbsp;           numResults: 3,

&nbsp;           includeDomains: specificDomain ? \[specificDomain] : undefined,

&nbsp;         });



&nbsp;         return {

&nbsp;           results: results.map(result => ({

&nbsp;             title: result.title,

&nbsp;             url: result.url,

&nbsp;             snippet: result.text.slice(0, 1000),

&nbsp;           })),

&nbsp;         };

&nbsp;       },

&nbsp;     }),

&nbsp;   },

&nbsp; });



&nbsp; // Convert markdown to Slack mrkdwn format

&nbsp; return text.replace(/\\\[(.\*?)\\]\\((.\*?)\\)/g, '<$2|$1>').replace(/\\\*\\\*/g, '\*');

};

```

In this updated implementation:

1\. You added two tools:

&nbsp; - `getWeather`: Fetches weather data for a specified location

&nbsp; - `searchWeb`: Searches the web for information using the Exa API

2\. You set `stopWhen: stepCountIs(10)` to enable multi-step conversations. This defines the stopping conditions of your agent, when the model generates a tool call. This will automatically send any tool results back to the LLM to trigger additional tool calls or responses as the LLM deems necessary. This turns your LLM call from a one-off operation into a multi-step agentic flow.

\## How It Works

When a user interacts with your bot:

1\. The Slack event is received and processed by your API endpoint

2\. The user's message and the thread history is passed to the `generateResponse` function

3\. The AI SDK processes the message and may invoke tools as needed

4\. The response is formatted for Slack and sent back to the user

The tools are automatically invoked based on the user's intent. For example, if a user asks "What's the weather in London?", the AI will:

1\. Recognize this as a weather query

2\. Call the `getWeather` tool with London's coordinates (inferred by the LLM)

3\. Process the weather data

4\. Generate a final response, answering the user's question

\## Deploying the App

1\. Install the Vercel CLI

<Snippet text={\['pnpm install -g vercel']} />

2\. Deploy the app

<Snippet text={\['vercel deploy']} />

3\. Copy the deployment URL and update the Slack app's Event Subscriptions to point to your Vercel URL

4\. Go to your project's deployment settings (Your project -> Settings -> Environment Variables) and add your environment variables

```bash

SLACK\_BOT\_TOKEN=your\_slack\_bot\_token

SLACK\_SIGNING\_SECRET=your\_slack\_signing\_secret

OPENAI\_API\_KEY=your\_openai\_api\_key

EXA\_API\_KEY=your\_exa\_api\_key

```

<Note>

&nbsp; Make sure to redeploy your app after updating environment variables.

</Note>

5\. Head back to the \[https://api.slack.com/](https://api.slack.com/) and navigate to the "Event Subscriptions" page. Enable events and add your deployment URL.

```bash

https://your-vercel-url.vercel.app/api/events

```

6\. On the Events Subscription page, subscribe to the following events.

&nbsp; - `app\_mention`

&nbsp; - `assistant\_thread\_started`

&nbsp; - `message:im`

Finally, head to Slack and test the app by sending a message to the bot.

\## Next Steps

You've built a Slack chatbot powered by the AI SDK! Here are some ways you could extend it:

1\. Add memory for specific users to give the LLM context of previous interactions

2\. Implement more tools like database queries or knowledge base searches

3\. Add support for rich message formatting with blocks

4\. Add analytics to track usage patterns

<Note>

&nbsp; In a production environment, it is recommended to implement a robust queueing

&nbsp; system to ensure messages are properly handled.

</Note>

---

title: Natural Language Postgres

description: Learn how to build a Next.js app that lets you talk to a PostgreSQL database in natural language.

tags: \['agents', 'next', 'tools']

---

\# Natural Language Postgres Guide

In this guide, you will learn how to build an app that uses AI to interact with a PostgreSQL database using natural language.

The application will:

\- Generate SQL queries from a natural language input

\- Explain query components in plain English

\- Create a chart to visualise query results

You can find a completed version of this project at \[natural-language-postgres.vercel.app](https://natural-language-postgres.vercel.app).

\## Project setup

This project uses the following stack:

\- \[Next.js](https://nextjs.org) (App Router)

\- \[AI SDK](/docs)

\- \[OpenAI](https://openai.com)

\- \[Zod](https://zod.dev)

\- \[Postgres](https://www.postgresql.org/) with \[ Vercel Postgres ](https://vercel.com/postgres)

\- \[shadcn-ui](https://ui.shadcn.com) and \[TailwindCSS](https://tailwindcss.com) for styling

\- \[Recharts](https://recharts.org) for data visualization

\### Clone repo

To focus on the AI-powered functionality rather than project setup and configuration we've prepared a starter repository which includes a database schema and a few components.

Clone the starter repository and check out the `starter` branch:

<Snippet

&nbsp; text={\[

&nbsp; 'git clone https://github.com/vercel-labs/natural-language-postgres',

&nbsp; 'cd natural-language-postgres',

&nbsp; 'git checkout starter',

&nbsp; ]}

/>

\### Project setup and data

Let's set up the project and seed the database with the dataset:

1\. Install dependencies:

<Snippet text={\['pnpm install']} />

2\. Copy the example environment variables file:

<Snippet text={\['cp .env.example .env']} />

3\. Add your environment variables to `.env`:

```bash filename=".env"

OPENAI\_API\_KEY="your\_api\_key\_here"

POSTGRES\_URL="..."

POSTGRES\_PRISMA\_URL="..."

POSTGRES\_URL\_NO\_SSL="..."

POSTGRES\_URL\_NON\_POOLING="..."

POSTGRES\_USER="..."

POSTGRES\_HOST="..."

POSTGRES\_PASSWORD="..."

POSTGRES\_DATABASE="..."

```

4\. This project uses CB Insights' Unicorn Companies dataset. You can download the dataset by following these instructions:

&nbsp; - Navigate to \[CB Insights Unicorn Companies](https://www.cbinsights.com/research-unicorn-companies)

&nbsp; - Enter in your email. You will receive a link to download the dataset.

&nbsp; - Save it as `unicorns.csv` in your project root

<Note>

&nbsp; You will need a Postgres database to complete this tutorial. If you don't have

&nbsp; Postgres setup on your local machine you can: - Create a free Postgres

&nbsp; database with Vercel (recommended - see instructions below); or - Follow \[this

&nbsp; guide](https://www.prisma.io/dataguide/postgresql/setting-up-a-local-postgresql-database)

&nbsp; to set it up locally

</Note>

\#### Setting up Postgres with Vercel

To set up a Postgres instance on your Vercel account:

1\. Go to \[Vercel.com](https://vercel.com) and make sure you're logged in

1\. Navigate to your team homepage

1\. Click on the \*\*Integrations\*\* tab

1\. Click \*\*Browse Marketplace\*\*

1\. Look for the \*\*Storage\*\* option in the sidebar

1\. Select the \*\*Neon\*\* option (recommended, but any other PostgreSQL database provider should work)

1\. Click \*\*Install\*\*, then click \*\*Install\*\* again in the top right corner

1\. On the "Get Started with Neon" page, click \*\*Create Database\*\* on the right

1\. Select your region (e.g., Washington, D.C., U.S. East)

1\. Turn off \*\*Auth\*\*

1\. Click \*\*Continue\*\*

1\. Name your database (you can use the default name or rename it to something like "NaturalLanguagePostgres")

1\. Click \*\*Create\*\* in the bottom right corner

1\. After seeing "Database created successfully", click \*\*Done\*\*

1\. You'll be redirected to your database instance

1\. In the Quick Start section, click \*\*Show secrets\*\*

1\. Copy the full `DATABASE\_URL` environment variable and use it to populate the Postgres environment variables in your `.env` file

\### About the dataset

The Unicorn List dataset contains the following information about unicorn startups (companies with a valuation above $1bn):

\- Company name

\- Valuation

\- Date joined (unicorn status)

\- Country

\- City

\- Industry

\- Select investors

This dataset contains over 1000 rows of data over 7 columns, giving us plenty of structured data to analyze. This makes it perfect for exploring various SQL queries that can reveal interesting insights about the unicorn startup ecosystem.

5\. Now that you have the dataset downloaded and added to your project, you can initialize the database with the following command:

<Snippet text={\['pnpm run seed']} />

Note: this step can take a little while. You should see a message indicating the Unicorns table has been created and then that the database has been seeded successfully.

<Note>

&nbsp; Remember, the dataset should be named `unicorns.csv` and located in root of

&nbsp; your project.

</Note>

6\. Start the development server:

<Snippet text={\['pnpm run dev']} />

Your application should now be running at \[http://localhost:3000](http://localhost:3000).

\## Project structure

The starter repository already includes everything that you will need, including:

\- Database seed script (`lib/seed.ts`)

\- Basic components built with shadcn/ui (`components/`)

\- Function to run SQL queries (`app/actions.ts`)

\- Type definitions for the database schema (`lib/types.ts`)

\### Existing components

The application contains a single page in `app/page.tsx` that serves as the main interface.

At the top, you'll find a header (`header.tsx`) displaying the application title and description. Below that is an input field and search button (`search.tsx`) where you can enter natural language queries.

Initially, the page shows a collection of suggested example queries (`suggested-queries.tsx`) that you can click to quickly try out the functionality.

When you submit a query:

\- The suggested queries section disappears and a loading state appears

\- Once complete, a card appears with "TODO - IMPLEMENT ABOVE" (`query-viewer.tsx`) which will eventually show your generated SQL

\- Below that is an empty results area with "No results found" (`results.tsx`)

After you implement the core functionality:

\- The results section will display data in a table format

\- A toggle button will allow switching between table and chart views

\- The chart view will visualize your query results

Let's implement the AI-powered functionality to bring it all together.

\## Building the application

As a reminder, this application will have three main features:

1\. Generate SQL queries from natural language

2\. Create a chart from the query results

3\. Explain SQL queries in plain English

For each of these features, you'll use the AI SDK via \[ Server Actions ](https://react.dev/reference/rsc/server-actions) to interact with OpenAI's GPT-4o and GPT-4o-mini models. Server Actions are a powerful React Server Component feature that allows you to call server-side functions directly from your frontend code.

Let's start with generating a SQL query from natural language.

\## Generate SQL queries

\### Providing context

For the model to generate accurate SQL queries, it needs context about your database schema, tables, and relationships. You will communicate this information through a prompt that should include:

1\. Schema information

2\. Example data formats

3\. Available SQL operations

4\. Best practices for query structure

5\. Nuanced advice for specific fields

Let's write a prompt that includes all of this information:

```txt

You are a SQL (postgres) and data visualization expert. Your job is to help the user write a SQL query to retrieve the data they need. The table schema is as follows:



unicorns (

&nbsp; id SERIAL PRIMARY KEY,

&nbsp; company VARCHAR(255) NOT NULL UNIQUE,

&nbsp; valuation DECIMAL(10, 2) NOT NULL,

&nbsp; date\_joined DATE,

&nbsp; country VARCHAR(255) NOT NULL,

&nbsp; city VARCHAR(255) NOT NULL,

&nbsp; industry VARCHAR(255) NOT NULL,

&nbsp; select\_investors TEXT NOT NULL

);



Only retrieval queries are allowed.



For things like industry, company names and other string fields, use the ILIKE operator and convert both the search term and the field to lowercase using LOWER() function. For example: LOWER(industry) ILIKE LOWER('%search\_term%').



Note: select\_investors is a comma-separated list of investors. Trim whitespace to ensure you're grouping properly. Note, some fields may be null or have only one value.

When answering questions about a specific field, ensure you are selecting the identifying column (ie. what is Vercel's valuation would select company and valuation').



The industries available are:

\- healthcare \& life sciences

\- consumer \& retail

\- financial services

\- enterprise tech

\- insurance

\- media \& entertainment

\- industrials

\- health



If the user asks for a category that is not in the list, infer based on the list above.



Note: valuation is in billions of dollars so 10b would be 10.0.

Note: if the user asks for a rate, return it as a decimal. For example, 0.1 would be 10%.



If the user asks for 'over time' data, return by year.



When searching for UK or USA, write out United Kingdom or United States respectively.



EVERY QUERY SHOULD RETURN QUANTITATIVE DATA THAT CAN BE PLOTTED ON A CHART! There should always be at least two columns. If the user asks for a single column, return the column and the count of the column. If the user asks for a rate, return the rate as a decimal. For example, 0.1 would be 10%.

```

There are several important elements of this prompt:

\- Schema description helps the model understand exactly what data fields to work with

\- Includes rules for handling queries based on common SQL patterns - for example, always using ILIKE for case-insensitive string matching

\- Explains how to handle edge cases in the dataset, like dealing with the comma-separated investors field and ensuring whitespace is properly handled

\- Instead of having the model guess at industry categories, it provides the exact list that exists in the data, helping avoid mismatches

\- The prompt helps standardize data transformations - like knowing to interpret "10b" as "10.0" billion dollars, or that rates should be decimal values

\- Clear rules ensure the query output will be chart-friendly by always including at least two columns of data that can be plotted

This prompt structure provides a strong foundation for query generation, but you should experiment and iterate based on your specific needs and the model you're using.

\### Create a Server Action

With the prompt done, let's create a Server Action.

Open `app/actions.ts`. You should see one action already defined (`runGeneratedSQLQuery`).

Add a new action. This action should be asynchronous and take in one parameter - the natural language query.

```ts filename="app/actions.ts"
/\* ...rest of the file... \*/;

export const generateQuery = async (input: string) => {};
```

In this action, you'll use the `generateObject` function from the AI SDK which allows you to constrain the model's output to a pre-defined schema. This process, sometimes called structured output, ensures the model returns only the SQL query without any additional prefixes, explanations, or formatting that would require manual parsing.

```ts filename="app/actions.ts"

/\* ...other imports... \*/

import { generateObject } from 'ai';

import { z } from 'zod';



/\* ...rest of the file... \*/



export const generateQuery = async (input: string) => {

&nbsp; 'use server';

&nbsp; try {

&nbsp;   const result = await generateObject({

&nbsp;     model: 'openai/gpt-4o',

&nbsp;     system: `You are a SQL (postgres) ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY

&nbsp;     prompt: `Generate the query necessary to retrieve the data the user wants: ${input}`,

&nbsp;     schema: z.object({

&nbsp;       query: z.string(),

&nbsp;     }),

&nbsp;   });

&nbsp;   return result.object.query;

&nbsp; } catch (e) {

&nbsp;   console.error(e);

&nbsp;   throw new Error('Failed to generate query');

&nbsp; }

};

```

Note, you are constraining the output to a single string field called `query` using `zod`, a TypeScript schema validation library. This will ensure the model only returns the SQL query itself. The resulting generated query will then be returned.

\### Update the frontend

With the Server Action in place, you can now update the frontend to call this action when the user submits a natural language query. In the root page (`app/page.tsx`), you should see a `handleSubmit` function that is called when the user submits a query.

Import the `generateQuery` function and call it with the user's input.

```typescript filename="app/page.tsx" highlight="21"

/\* ...other imports... \*/

import { runGeneratedSQLQuery, generateQuery } from './actions';



/\* ...rest of the file... \*/



const handleSubmit = async (suggestion?: string) => {

&nbsp; clearExistingData();



&nbsp; const question = suggestion ?? inputValue;

&nbsp; if (inputValue.length === 0 \&\& !suggestion) return;



&nbsp; if (question.trim()) {

&nbsp;   setSubmitted(true);

&nbsp; }



&nbsp; setLoading(true);

&nbsp; setLoadingStep(1);

&nbsp; setActiveQuery('');



&nbsp; try {

&nbsp;   const query = await generateQuery(question);



&nbsp;   if (query === undefined) {

&nbsp;     toast.error('An error occurred. Please try again.');

&nbsp;     setLoading(false);

&nbsp;     return;

&nbsp;   }



&nbsp;   setActiveQuery(query);

&nbsp;   setLoadingStep(2);



&nbsp;   const companies = await runGeneratedSQLQuery(query);

&nbsp;   const columns = companies.length > 0 ? Object.keys(companies\[0]) : \[];

&nbsp;   setResults(companies);

&nbsp;   setColumns(columns);



&nbsp;   setLoading(false);

&nbsp; } catch (e) {

&nbsp;   toast.error('An error occurred. Please try again.');

&nbsp;   setLoading(false);

&nbsp; }

};



/\* ...rest of the file... \*/

```

Now, when the user submits a natural language query (ie. "how many unicorns are from San Francisco?"), that question will be sent to your newly created Server Action. The Server Action will call the model, passing in your system prompt and the users query, and return the generated SQL query in a structured format. This query is then passed to the `runGeneratedSQLQuery` action to run the query against your database. The results are then saved in local state and displayed to the user.

Save the file, make sure the dev server is running, and then head to `localhost:3000` in your browser. Try submitting a natural language query and see the generated SQL query and results. You should see a SQL query generated and displayed under the input field. You should also see the results of the query displayed in a table below the input field.

Try clicking the SQL query to see the full query if it's too long to display in the input field. You should see a button on the right side of the input field with a question mark icon. Clicking this button currently does nothing, but you'll add the "explain query" functionality to it in the next step.

\## Explain SQL Queries

Next, let's add the ability to explain SQL queries in plain English. This feature helps users understand how the generated SQL query works by breaking it down into logical sections.

As with the SQL query generation, you'll need a prompt to guide the model when explaining queries.

Let's craft a prompt for the explain query functionality:

```txt

You are a SQL (postgres) expert. Your job is to explain to the user write a SQL query you wrote to retrieve the data they asked for. The table schema is as follows:

unicorns (

&nbsp; id SERIAL PRIMARY KEY,

&nbsp; company VARCHAR(255) NOT NULL UNIQUE,

&nbsp; valuation DECIMAL(10, 2) NOT NULL,

&nbsp; date\_joined DATE,

&nbsp; country VARCHAR(255) NOT NULL,

&nbsp; city VARCHAR(255) NOT NULL,

&nbsp; industry VARCHAR(255) NOT NULL,

&nbsp; select\_investors TEXT NOT NULL

);



When you explain you must take a section of the query, and then explain it. Each "section" should be unique. So in a query like: "SELECT \* FROM unicorns limit 20", the sections could be "SELECT \*", "FROM UNICORNS", "LIMIT 20".

If a section doesn't have any explanation, include it, but leave the explanation empty.

```

Like the prompt for generating SQL queries, you provide the model with the schema of the database. Additionally, you provide an example of what each section of the query might look like. This helps the model understand the structure of the query and how to break it down into logical sections.

\### Create a Server Action

Add a new Server Action to generate explanations for SQL queries.

This action takes two parameters - the original natural language input and the generated SQL query.

```ts filename="app/actions.ts"

/\* ...rest of the file... \*/



export const explainQuery = async (input: string, sqlQuery: string) => {

&nbsp; 'use server';

&nbsp; try {

&nbsp;   const result = await generateObject({

&nbsp;     model: 'openai/gpt-4o',

&nbsp;     system: `You are a SQL (postgres) expert. ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY

&nbsp;     prompt: `Explain the SQL query you generated to retrieve the data the user wanted. Assume the user is not an expert in SQL. Break down the query into steps. Be concise.



&nbsp;     User Query:

&nbsp;     ${input}



&nbsp;     Generated SQL Query:

&nbsp;     ${sqlQuery}`,

&nbsp;   });

&nbsp;   return result.object;

&nbsp; } catch (e) {

&nbsp;   console.error(e);

&nbsp;   throw new Error('Failed to generate query');

&nbsp; }

};

```

This action uses the `generateObject` function again. However, you haven't defined the schema yet. Let's define it in another file so it can also be used as a type in your components.

Update your `lib/types.ts` file to include the schema for the explanations:

```ts filename="lib/types.ts"

import { z } from 'zod';



/\* ...rest of the file... \*/



export const explanationSchema = z.object({

&nbsp; section: z.string(),

&nbsp; explanation: z.string(),

});



export type QueryExplanation = z.infer<typeof explanationSchema>;

```

This schema defines the structure of the explanation that the model will generate. Each explanation will have a `section` and an `explanation`. The `section` is the part of the query being explained, and the `explanation` is the plain English explanation of that section. Go back to your `actions.ts` file and import and use the `explanationSchema`:

```ts filename="app/actions.ts" highlight="2,19,20"

// other imports

import { explanationSchema } from '@/lib/types';



/\* ...rest of the file... \*/



export const explainQuery = async (input: string, sqlQuery: string) => {

&nbsp; 'use server';

&nbsp; try {

&nbsp;   const result = await generateObject({

&nbsp;     model: 'openai/gpt-4o',

&nbsp;     system: `You are a SQL (postgres) expert. ...`, // SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY

&nbsp;     prompt: `Explain the SQL query you generated to retrieve the data the user wanted. Assume the user is not an expert in SQL. Break down the query into steps. Be concise.



&nbsp;     User Query:

&nbsp;     ${input}



&nbsp;     Generated SQL Query:

&nbsp;     ${sqlQuery}`,

&nbsp;     schema: explanationSchema,

&nbsp;     output: 'array',

&nbsp;   });

&nbsp;   return result.object;

&nbsp; } catch (e) {

&nbsp;   console.error(e);

&nbsp;   throw new Error('Failed to generate query');

&nbsp; }

};

```

<Note>

&nbsp; You can use `output: "array"` to indicate to the model that you expect an

&nbsp; array of objects matching the schema to be returned.

</Note>

\### Update query viewer

Next, update the `query-viewer.tsx` component to display these explanations. The `handleExplainQuery` function is called every time the user clicks the question icon button on the right side of the query. Let's update this function to use the new `explainQuery` action:

```ts filename="components/query-viewer.tsx" highlight="2,10,11"

/\* ...other imports... \*/

import { explainQuery } from '@/app/actions';



/\* ...rest of the component... \*/



const handleExplainQuery = async () => {

&nbsp; setQueryExpanded(true);

&nbsp; setLoadingExplanation(true);



&nbsp; const explanations = await explainQuery(inputValue, activeQuery);

&nbsp; setQueryExplanations(explanations);



&nbsp; setLoadingExplanation(false);

};



/\* ...rest of the component... \*/

```

Now when users click the explanation button (the question mark icon), the component will:

1\. Show a loading state

2\. Send the active SQL query and the users natural language query to your Server Action

3\. The model will generate an array of explanations

4\. The explanations will be set in the component state and rendered in the UI

Submit a new query and then click the explanation button. Hover over different elements of the query. You should see the explanations for each section!

\## Visualizing query results

Finally, let's render the query results visually in a chart. There are two approaches you could take:

1\. Send both the query and data to the model and ask it to return the data in a visualization-ready format. While this provides complete control over the visualization, it requires the model to send back all of the data, which significantly increases latency and costs.

2\. Send the query and data to the model and ask it to generate a chart configuration (fixed-size and not many tokens) that maps your data appropriately. This configuration specifies how to visualize the information while delivering the insights from your natural language query. Importantly, this is done without requiring the model return the full dataset.

Since you don't know the SQL query or data shape beforehand, let's use the second approach to dynamically generate chart configurations based on the query results and user intent.

\### Generate the chart configuration

For this feature, you'll create a Server Action that takes the query results and the user's original natural language query to determine the best visualization approach. Your application is already set up to use `shadcn` charts (which uses \[`Recharts`](https://recharts.org/en-US/) under the hood) so the model will need to generate:

\- Chart type (bar, line, area, or pie)

\- Axis mappings

\- Visual styling

Let's start by defining the schema for the chart configuration in `lib/types.ts`:

```ts filename="lib/types.ts"

/\* ...rest of the file... \*/



export const configSchema = z

&nbsp; .object({

&nbsp;   description: z

&nbsp;     .string()

&nbsp;     .describe(

&nbsp;       'Describe the chart. What is it showing? What is interesting about the way the data is displayed?',

&nbsp;     ),

&nbsp;   takeaway: z.string().describe('What is the main takeaway from the chart?'),

&nbsp;   type: z.enum(\['bar', 'line', 'area', 'pie']).describe('Type of chart'),

&nbsp;   title: z.string(),

&nbsp;   xKey: z.string().describe('Key for x-axis or category'),

&nbsp;   yKeys: z

&nbsp;     .array(z.string())

&nbsp;     .describe(

&nbsp;       'Key(s) for y-axis values this is typically the quantitative column',

&nbsp;     ),

&nbsp;   multipleLines: z

&nbsp;     .boolean()

&nbsp;     .describe(

&nbsp;       'For line charts only: whether the chart is comparing groups of data.',

&nbsp;     )

&nbsp;     .optional(),

&nbsp;   measurementColumn: z

&nbsp;     .string()

&nbsp;     .describe(

&nbsp;       'For line charts only: key for quantitative y-axis column to measure against (eg. values, counts etc.)',

&nbsp;     )

&nbsp;     .optional(),

&nbsp;   lineCategories: z

&nbsp;     .array(z.string())

&nbsp;     .describe(

&nbsp;       'For line charts only: Categories used to compare different lines or data series. Each category represents a distinct line in the chart.',

&nbsp;     )

&nbsp;     .optional(),

&nbsp;   colors: z

&nbsp;     .record(

&nbsp;       z.string().describe('Any of the yKeys'),

&nbsp;       z.string().describe('Color value in CSS format (e.g., hex, rgb, hsl)'),

&nbsp;     )

&nbsp;     .describe('Mapping of data keys to color values for chart elements')

&nbsp;     .optional(),

&nbsp;   legend: z.boolean().describe('Whether to show legend'),

&nbsp; })

&nbsp; .describe('Chart configuration object');



export type Config = z.infer<typeof configSchema>;

```

<Note>

&nbsp; Replace the existing `export type Config = any;` type with the new one.

</Note>

This schema makes extensive use of Zod's `.describe()` function to give the model extra context about each of the key's you are expecting in the chart configuration. This will help the model understand the purpose of each key and generate more accurate results.

Another important technique to note here is that you are defining `description` and `takeaway` fields. Not only are these useful for the user to quickly understand what the chart means and what they should take away from it, but they also force the model to generate a description of the data first, before it attempts to generate configuration attributes like axis and columns. This will help the model generate more accurate and relevant chart configurations.

\### Create the Server Action

Create a new action in `app/actions.ts`:

```ts

/\* ...other imports... \*/

import { Config, configSchema, explanationsSchema, Result } from '@/lib/types';



/\* ...rest of the file... \*/



export const generateChartConfig = async (

&nbsp; results: Result\[],

&nbsp; userQuery: string,

) => {

&nbsp; 'use server';



&nbsp; try {

&nbsp;   const { object: config } = await generateObject({

&nbsp;     model: 'openai/gpt-4o',

&nbsp;     system: 'You are a data visualization expert.',

&nbsp;     prompt: `Given the following data from a SQL query result, generate the chart config that best visualises the data and answers the users query.

&nbsp;     For multiple groups use multi-lines.



&nbsp;     Here is an example complete config:

&nbsp;     export const chartConfig = {

&nbsp;       type: "pie",

&nbsp;       xKey: "month",

&nbsp;       yKeys: \["sales", "profit", "expenses"],

&nbsp;       colors: {

&nbsp;         sales: "#4CAF50",    // Green for sales

&nbsp;         profit: "#2196F3",   // Blue for profit

&nbsp;         expenses: "#F44336"  // Red for expenses

&nbsp;       },

&nbsp;       legend: true

&nbsp;     }



&nbsp;     User Query:

&nbsp;     ${userQuery}



&nbsp;     Data:

&nbsp;     ${JSON.stringify(results, null, 2)}`,

&nbsp;     schema: configSchema,

&nbsp;   });



&nbsp;   // Override with shadcn theme colors

&nbsp;   const colors: Record<string, string> = {};

&nbsp;   config.yKeys.forEach((key, index) => {

&nbsp;     colors\[key] = `hsl(var(--chart-${index + 1}))`;

&nbsp;   });



&nbsp;   const updatedConfig = { ...config, colors };

&nbsp;   return { config: updatedConfig };

&nbsp; } catch (e) {

&nbsp;   console.error(e);

&nbsp;   throw new Error('Failed to generate chart suggestion');

&nbsp; }

};

```

\### Update the chart component

With the action in place, you'll want to trigger it automatically after receiving query results. This ensures the visualization appears almost immediately after data loads.

Update the `handleSubmit` function in your root page (`app/page.tsx`) to generate and set the chart configuration after running the query:

```typescript filename="app/page.tsx" highlight="38,39"

/\* ...other imports... \*/

import { getCompanies, generateQuery, generateChartConfig } from './actions';



/\* ...rest of the file... \*/

const handleSubmit = async (suggestion?: string) => {

&nbsp; clearExistingData();



&nbsp; const question = suggestion ?? inputValue;

&nbsp; if (inputValue.length === 0 \&\& !suggestion) return;



&nbsp; if (question.trim()) {

&nbsp;   setSubmitted(true);

&nbsp; }



&nbsp; setLoading(true);

&nbsp; setLoadingStep(1);

&nbsp; setActiveQuery('');



&nbsp; try {

&nbsp;   const query = await generateQuery(question);



&nbsp;   if (query === undefined) {

&nbsp;     toast.error('An error occurred. Please try again.');

&nbsp;     setLoading(false);

&nbsp;     return;

&nbsp;   }



&nbsp;   setActiveQuery(query);

&nbsp;   setLoadingStep(2);



&nbsp;   const companies = await runGeneratedSQLQuery(query);

&nbsp;   const columns = companies.length > 0 ? Object.keys(companies\[0]) : \[];

&nbsp;   setResults(companies);

&nbsp;   setColumns(columns);



&nbsp;   setLoading(false);



&nbsp;   const { config } = await generateChartConfig(companies, question);

&nbsp;   setChartConfig(config);

&nbsp; } catch (e) {

&nbsp;   toast.error('An error occurred. Please try again.');

&nbsp;   setLoading(false);

&nbsp; }

};



/\* ...rest of the file... \*/

```

Now when users submit queries, the application will:

1\. Generate and run the SQL query

2\. Display the table results

3\. Generate a chart configuration for the results

4\. Allow toggling between table and chart views

Head back to the browser and test the application with a few queries. You should see the chart visualization appear after the table results.

\## Next steps

You've built an AI-powered SQL analysis tool that can convert natural language to SQL queries, visualize query results, and explain SQL queries in plain English.

You could, for example, extend the application to use your own data sources or add more advanced features like customizing the chart configuration schema to support more chart types and options. You could also add more complex SQL query generation capabilities.

---

title: Get started with Computer Use

description: Get started with Claude's Computer Use capabilities with the AI SDK

tags: \['computer-use', 'tools']

---

\# Get started with Computer Use

With the \[release of Computer Use in Claude 3.5 Sonnet](https://www.anthropic.com/news/3-5-models-and-computer-use), you can now direct AI models to interact with computers like humans do - moving cursors, clicking buttons, and typing text. This capability enables automation of complex tasks while leveraging Claude's advanced reasoning abilities.

The AI SDK is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Anthropic's Claude alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more. In this guide, you will learn how to integrate Computer Use into your AI SDK applications.

<Note>

&nbsp; Computer Use is currently in beta with some \[ limitations

&nbsp; ](https://docs.anthropic.com/en/docs/build-with-claude/computer-use#understand-computer-use-limitations).

&nbsp; The feature may be error-prone at times. Anthropic recommends starting with

&nbsp; low-risk tasks and implementing appropriate safety measures.

</Note>

\## Computer Use

Anthropic recently released a new version of the Claude 3.5 Sonnet model which is capable of 'Computer Use'. This allows the model to interact with computer interfaces through basic actions like:

\- Moving the cursor

\- Clicking buttons

\- Typing text

\- Taking screenshots

\- Reading screen content

\## How It Works

Computer Use enables the model to read and interact with on-screen content through a series of coordinated steps. Here's how the process works:

1\. \*\*Start with a prompt and tools\*\*

&nbsp; Add Anthropic-defined Computer Use tools to your request and provide a task (prompt) for the model. For example: "save an image to your downloads folder."

2\. \*\*Select the right tool\*\*

&nbsp; The model evaluates which computer tools can help accomplish the task. It then sends a formatted `tool\_call` to use the appropriate tool.

3\. \*\*Execute the action and return results\*\*

&nbsp; The AI SDK processes Claude's request by running the selected tool. The results can then be sent back to Claude through a `tool\_result` message.

4\. \*\*Complete the task through iterations\*\*

&nbsp; Claude analyzes each result to determine if more actions are needed. It continues requesting tool use and processing results until it completes your task or requires additional input.

\### Available Tools

There are three main tools available in the Computer Use API:

1\. \*\*Computer Tool\*\*: Enables basic computer control like mouse movement, clicking, and keyboard input

2\. \*\*Text Editor Tool\*\*: Provides functionality for viewing and editing text files

3\. \*\*Bash Tool\*\*: Allows execution of bash commands

\### Implementation Considerations

Computer Use tools in the AI SDK are predefined interfaces that require your own implementation of the execution layer. While the SDK provides the type definitions and structure for these tools, you need to:

1\. Set up a controlled environment for Computer Use execution

2\. Implement core functionality like mouse control and keyboard input

3\. Handle screenshot capture and processing

4\. Set up rules and limits for how Claude can interact with your system

The recommended approach is to start with \[ Anthropic's reference implementation ](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo), which provides:

\- A containerized environment configured for safe Computer Use

\- Ready-to-use (Python) implementations of Computer Use tools

\- An agent loop for API interaction and tool execution

\- A web interface for monitoring and control

This reference implementation serves as a foundation to understand the requirements before building your own custom solution.

\## Getting Started with the AI SDK

<Note>

&nbsp; If you have never used the AI SDK before, start by following the \[Getting

&nbsp; Started guide](/docs/getting-started).

</Note>

<Note>

&nbsp; For a working example of Computer Use implementation with Next.js and the AI

&nbsp; SDK, check out our \[AI SDK Computer Use

&nbsp; Template](https://github.com/vercel-labs/ai-sdk-computer-use).

</Note>

First, ensure you have the AI SDK and \[Anthropic AI SDK provider](/providers/ai-sdk-providers/anthropic) installed:

<Snippet text="pnpm add ai @ai-sdk/anthropic" />

You can add Computer Use to your AI SDK applications using provider-defined-client tools. These tools accept various input parameters (like display height and width in the case of the computer tool) and then require that you define an execute function.

Here's how you could set up the Computer Tool with the AI SDK:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { getScreenshot, executeComputerAction } from '@/utils/computer-use';



const computerTool = anthropic.tools.computer\_20250124({

&nbsp; displayWidthPx: 1920,

&nbsp; displayHeightPx: 1080,

&nbsp; execute: async ({ action, coordinate, text }) => {

&nbsp;   switch (action) {

&nbsp;     case 'screenshot': {

&nbsp;       return {

&nbsp;         type: 'image',

&nbsp;         data: getScreenshot(),

&nbsp;       };

&nbsp;     }

&nbsp;     default: {

&nbsp;       return executeComputerAction(action, coordinate, text);

&nbsp;     }

&nbsp;   }

&nbsp; },

&nbsp; toModelOutput({ output }) {

&nbsp;   return typeof output === 'string'

&nbsp;     ? \[{ type: 'text', text: output }]

&nbsp;     : \[{ type: 'image', data: output.data, mediaType: 'image/png' }];

&nbsp; },

});

```

The `computerTool` handles two main actions: taking screenshots via `getScreenshot()` and executing computer actions like mouse movements and clicks through `executeComputerAction()`. Remember, you have to implement this execution logic (eg. the `getScreenshot` and `executeComputerAction` functions) to handle the actual computer interactions. The `execute` function should handle all low-level interactions with the operating system.

Finally, to send tool results back to the model, use the \[`toModelOutput()`](/docs/foundations/prompts#multi-modal-tool-results) function to convert text and image responses into a format the model can process. The AI SDK includes experimental support for these multi-modal tool results when using Anthropic's models.

<Note>

&nbsp; Computer Use requires appropriate safety measures like using virtual machines,

&nbsp; limiting access to sensitive data, and implementing human oversight for

&nbsp; critical actions.

</Note>

\### Using Computer Tools with Text Generation

Once your tool is defined, you can use it with both the \[`generateText`](/docs/reference/ai-sdk-core/generate-text) and \[`streamText`](/docs/reference/ai-sdk-core/stream-text) functions.

For one-shot text generation, use `generateText`:

```ts

const result = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4-20250514',

&nbsp; prompt: 'Move the cursor to the center of the screen and take a screenshot',

&nbsp; tools: { computer: computerTool },

});



console.log(result.text);

```

For streaming responses, use `streamText` to receive updates in real-time:

```ts

const result = streamText({

&nbsp; model: 'anthropic/claude-sonnet-4-20250514',

&nbsp; prompt: 'Open the browser and navigate to vercel.com',

&nbsp; tools: { computer: computerTool },

});



for await (const chunk of result.textStream) {

&nbsp; console.log(chunk);

}

```

\### Configure Multi-Step (Agentic) Generations

To allow the model to perform multiple steps without user intervention, use the `stopWhen` parameter. This will automatically send any tool results back to the model to trigger a subsequent generation:

```ts highlight="1,7"

import { stepCountIs } from 'ai';



const stream = streamText({

&nbsp; model: 'anthropic/claude-sonnet-4-20250514',

&nbsp; prompt: 'Open the browser and navigate to vercel.com',

&nbsp; tools: { computer: computerTool },

&nbsp; stopWhen: stepCountIs(10), // experiment with this value based on your use case

});

```

\### Combine Multiple Tools

You can combine multiple tools in a single request to enable more complex workflows. The AI SDK supports all three of Claude's Computer Use tools:

```ts

const computerTool = anthropic.tools.computer\_20250124({

&nbsp; ...

});



const bashTool = anthropic.tools.bash\_20250124({

&nbsp; execute: async ({ command, restart }) => execSync(command).toString()

});



const textEditorTool = anthropic.tools.textEditor\_20250124({

&nbsp; execute: async ({

&nbsp;   command,

&nbsp;   path,

&nbsp;   file\_text,

&nbsp;   insert\_line,

&nbsp;   new\_str,

&nbsp;   old\_str,

&nbsp;   view\_range

&nbsp; }) => {

&nbsp;   // Handle file operations based on command

&nbsp;   switch(command) {

&nbsp;     return executeTextEditorFunction({

&nbsp;       command,

&nbsp;       path,

&nbsp;       fileText: file\_text,

&nbsp;       insertLine: insert\_line,

&nbsp;       newStr: new\_str,

&nbsp;       oldStr: old\_str,

&nbsp;       viewRange: view\_range

&nbsp;     });

&nbsp;   }

&nbsp; }

});





const response = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4-20250514',

&nbsp; prompt: "Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal",

&nbsp; tools: {

&nbsp;   computer: computerTool,

&nbsp;   bash: bashTool,

&nbsp;   str\_replace\_editor: textEditorTool,

&nbsp; },

});

```

<Note>

&nbsp; Always implement appropriate \[security measures](#security-measures) and

&nbsp; obtain user consent before enabling Computer Use in production applications.

</Note>

\### Best Practices for Computer Use

To get the best results when using Computer Use:

1\. Specify simple, well-defined tasks with explicit instructions for each step

2\. Prompt Claude to verify outcomes through screenshots

3\. Use keyboard shortcuts when UI elements are difficult to manipulate

4\. Include example screenshots for repeatable tasks

5\. Provide explicit tips in system prompts for known tasks

\## Security Measures

Remember, Computer Use is a beta feature. Please be aware that it poses unique risks that are distinct from standard API features or chat interfaces. These risks are heightened when using Computer Use to interact with the internet. To minimize risks, consider taking precautions such as:

1\. Use a dedicated virtual machine or container with minimal privileges to prevent direct system attacks or accidents.

2\. Avoid giving the model access to sensitive data, such as account login information, to prevent information theft.

3\. Limit internet access to an allowlist of domains to reduce exposure to malicious content.

4\. Ask a human to confirm decisions that may result in meaningful real-world consequences as well as any tasks requiring affirmative consent, such as accepting cookies, executing financial transactions, or agreeing to terms of service.

---

title: Get started with Gemini 3

description: Get started with Gemini 3 using the AI SDK.

tags: \['getting-started']

---

\# Get started with Gemini 3

With the release of Gemini 3, Google's most intelligent model to date, there has never been a better time to start building AI applications that combine state-of-the-art reasoning with multimodal understanding.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Gemini 3 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## Gemini 3

Gemini 3 represents a significant leap forward in AI capabilities, combining all of Gemini's strengths together to help you bring any idea to life. It delivers:

\- State-of-the-art reasoning with unprecedented depth and nuance

\- PhD-level performance on complex benchmarks like Humanity's Last Exam (37.5%) and GPQA Diamond (91.9%)

\- Leading multimodal understanding with 81% on MMMU-Pro and 87.6% on Video-MMMU

\- Best-in-class vibe coding and agentic capabilities

\- Superior long-horizon planning for multi-step workflows

Gemini 3 Pro is currently available in preview, offering great performance across all benchmarks.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Gemini 3 with the AI SDK:

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: google('gemini-3-pro-preview'),

&nbsp; prompt: 'Explain the concept of the Hilbert space.',

});

console.log(text);

```

\### Enhanced Reasoning with Thinking Mode

Gemini 3 models can use enhanced reasoning through thinking mode, which improves their ability to solve complex problems. You can control the thinking level using the `thinkingLevel` provider option:

```ts

import { google, GoogleGenerativeAIProviderOptions } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: google('gemini-3-pro-preview'),

&nbsp; prompt: 'What is the sum of the first 10 prime numbers?',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     thinkingConfig: {

&nbsp;       includeThoughts: true,

&nbsp;       thinkingLevel: 'low',

&nbsp;     },

&nbsp;   } satisfies GoogleGenerativeAIProviderOptions,

&nbsp; },

});



console.log(text);

```

The `thinkingLevel` parameter accepts different values to control the depth of reasoning applied to your prompt:

\- Gemini 3 Pro supports: `'low'` and `'high'`

\- Gemini 3 Flash supports: `'minimal'`, `'low'`, `'medium'`, and `'high'`

\### Using Tools with the AI SDK

Gemini 3 excels at tool calling with improved reliability and consistency for multi-step workflows. Here's an example of using tool calling with the AI SDK:

```ts

import { z } from 'zod';

import { generateText, tool, stepCountIs } from 'ai';

import { google } from '@ai-sdk/google';



const result = await generateText({

&nbsp; model: google('gemini-3-pro-preview'),

&nbsp; prompt: 'What is the weather in San Francisco?',

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; stopWhen: stepCountIs(5), // enables multi-step calling

});



console.log(result.text);



console.log(result.steps);

```

\### Using Google Search with Gemini

With \[search grounding](https://ai.google.dev/gemini-api/docs/google-search), Gemini can access the latest information using Google search. Here's an example of using Google Search with the AI SDK:

```ts

import { google } from '@ai-sdk/google';

import { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text, sources, providerMetadata } = await generateText({

&nbsp; model: google('gemini-3-pro-preview'),

&nbsp; tools: {

&nbsp;   google\_search: google.tools.googleSearch({}),

&nbsp; },

&nbsp; prompt:

&nbsp;   'List the top 5 San Francisco news from the past week.' +

&nbsp;   'You must include the date of each article.',

});



// access the grounding metadata. Casting to the provider metadata type

// is optional but provides autocomplete and type safety.

const metadata = providerMetadata?.google as

&nbsp; | GoogleGenerativeAIProviderMetadata

&nbsp; | undefined;

const groundingMetadata = metadata?.groundingMetadata;

const safetyRatings = metadata?.safetyRatings;



console.log({ text, sources, groundingMetadata, safetyRatings });

```

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), \[`useObject`](/docs/reference/ai-sdk-ui/use-object), and \[`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and Gemini 3 Pro:

In a new Next.js application, first install the AI SDK and the Google Generative AI provider:

<Snippet text="pnpm install ai @ai-sdk/google" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { google } from '@ai-sdk/google';

import { streamText, UIMessage, convertToModelMessages } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: google('gemini-3-pro-preview'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();

&nbsp; return (

&nbsp;   <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id} className="whitespace-pre-wrap">

&nbsp;         {message.role === 'user' ? 'User: ' : 'Gemini: '}

&nbsp;         {message.parts.map((part, i) => {

&nbsp;           switch (part.type) {

&nbsp;             case 'text':

&nbsp;               return <div key={`${message.id}-${i}`}>{part.text}</div>;

&nbsp;           }

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         sendMessage({ text: input });

&nbsp;         setInput('');

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         className="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"

&nbsp;         value={input}

&nbsp;         placeholder="Say something..."

&nbsp;         onChange={e => setInput(e.currentTarget.value)}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Get Started

Ready to dive in? Here's how you can begin:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Use ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

5\. Read more about the \[Google Generative AI provider](/providers/ai-sdk-providers/google-generative-ai).

---

title: Get started with Claude 4

description: Get started with Claude 4 using the AI SDK.

tags: \['getting-started']

---

\# Get started with Claude 4

With the release of Claude 4, there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities and advanced intelligence.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Claude 4 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## Claude 4

Claude 4 is Anthropic's most advanced model family to date, offering exceptional capabilities across reasoning, instruction following, coding, and knowledge tasks. Available in two variants—Sonnet and Opus—Claude 4 delivers state-of-the-art performance with enhanced reliability and control. Claude 4 builds on the extended thinking capabilities introduced in Claude 3.7, allowing for even more sophisticated problem-solving through careful, step-by-step reasoning.

Claude 4 excels at complex reasoning, code generation and analysis, detailed content creation, and agentic capabilities, making it ideal for powering sophisticated AI workflows, customer-facing agents, and applications requiring nuanced understanding and responses. Claude Opus 4 is an excellent coding model, leading on SWE-bench (72.5%) and Terminal-bench (43.2%), with the ability to sustain performance on long-running tasks that require focused effort and thousands of steps. Claude Sonnet 4 significantly improves on Sonnet 3.7, excelling in coding with 72.7% on SWE-bench while balancing performance and efficiency.

\### Prompt Engineering for Claude 4 Models

Claude 4 models respond well to clear, explicit instructions. The following best practices can help achieve optimal performance:

1\. \*\*Provide explicit instructions\*\*: Clearly state what you want the model to do, including specific steps or formats for the response.

2\. \*\*Include context and motivation\*\*: Explain why a task is being performed to help the model better understand the underlying goals.

3\. \*\*Avoid negative examples\*\*: When providing examples, only demonstrate the behavior you want to see, not what you want to avoid.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Claude 4 Sonnet with the AI SDK:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-20250514'),

&nbsp; prompt: 'How will quantum computing impact cryptography by 2050?',

});

console.log(text);

```

\### Reasoning Ability

Claude 4 enhances the extended thinking capabilities first introduced in Claude 3.7 Sonnet—the ability to solve complex problems with careful, step-by-step reasoning. Additionally, both Opus 4 and Sonnet 4 can now use tools during extended thinking, allowing Claude to alternate between reasoning and tool use to improve responses. You can enable extended thinking using the `thinking` provider option and specifying a thinking budget in tokens. For interleaved thinking (where Claude can think in between tool calls) you'll need to enable a beta feature using the `anthropic-beta` header:

```ts

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-20250514'),

&nbsp; prompt: 'How will quantum computing impact cryptography by 2050?',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 15000 },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

&nbsp; headers: {

&nbsp;   'anthropic-beta': 'interleaved-thinking-2025-05-14',

&nbsp; },

});



console.log(text); // text response

console.log(reasoningText); // reasoning text

console.log(reasoning); // reasoning details including redacted reasoning

```

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), \[`useObject`](/docs/reference/ai-sdk-ui/use-object), and \[`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and Claude Sonnet 4:

In a new Next.js application, first install the AI SDK and the Anthropic provider:

<Snippet text="pnpm install ai @ai-sdk/anthropic" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { streamText, convertToModelMessages, type UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: anthropic('claude-sonnet-4-20250514'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   headers: {

&nbsp;     'anthropic-beta': 'interleaved-thinking-2025-05-14',

&nbsp;   },

&nbsp;   providerOptions: {

&nbsp;     anthropic: {

&nbsp;       thinking: { type: 'enabled', budgetTokens: 15000 },

&nbsp;     } satisfies AnthropicProviderOptions,

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   sendReasoning: true,

&nbsp; });

}

```

<Note>

&nbsp; You can forward the model's reasoning tokens to the client with

&nbsp; `sendReasoning: true` in the `toUIMessageStreamResponse` method.

</Note>

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat({

&nbsp;   transport: new DefaultChatTransport({ api: '/api/chat' }),

&nbsp; });



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   if (input.trim()) {

&nbsp;     sendMessage({ text: input });

&nbsp;     setInput('');

&nbsp;   }

&nbsp; };



&nbsp; return (

&nbsp;   <div className="flex flex-col h-screen max-w-2xl mx-auto p-4">

&nbsp;     <div className="flex-1 overflow-y-auto space-y-4 mb-4">

&nbsp;       {messages.map(message => (

&nbsp;         <div

&nbsp;           key={message.id}

&nbsp;           className={`p-3 rounded-lg ${

&nbsp;             message.role === 'user' ? 'bg-blue-50 ml-auto' : 'bg-gray-50'

&nbsp;           }`}

&nbsp;         >

&nbsp;           <p className="font-semibold">

&nbsp;             {message.role === 'user' ? 'You' : 'Claude 4'}

&nbsp;           </p>

&nbsp;           {message.parts.map((part, index) => {

&nbsp;             if (part.type === 'text') {

&nbsp;               return (

&nbsp;                 <div key={index} className="mt-1">

&nbsp;                   {part.text}

&nbsp;                 </div>

&nbsp;               );

&nbsp;             }

&nbsp;             if (part.type === 'reasoning') {

&nbsp;               return (

&nbsp;                 <pre

&nbsp;                   key={index}

&nbsp;                   className="bg-gray-100 p-2 rounded mt-2 text-xs overflow-x-auto"

&nbsp;                 >

&nbsp;                   <details>

&nbsp;                     <summary className="cursor-pointer">

&nbsp;                       View reasoning

&nbsp;                     </summary>

&nbsp;                     {part.text}

&nbsp;                   </details>

&nbsp;                 </pre>

&nbsp;               );

&nbsp;             }

&nbsp;           })}

&nbsp;         </div>

&nbsp;       ))}

&nbsp;     </div>

&nbsp;     <form onSubmit={handleSubmit} className="flex gap-2">

&nbsp;       <input

&nbsp;         name="prompt"

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         className="flex-1 p-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500"

&nbsp;         placeholder="Ask Claude 4 something..."

&nbsp;       />

&nbsp;       <button

&nbsp;         type="submit"

&nbsp;         className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600"

&nbsp;       >

&nbsp;         Send

&nbsp;       </button>

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

<Note>

&nbsp; You can access the model's reasoning tokens with the `reasoning` part on the

&nbsp; message `parts`. The reasoning text is available in the `text` property of the

&nbsp; reasoning part.

</Note>

The useChat hook on your root page (`app/page.tsx`) will make a request to your LLM provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\### Claude 4 Model Variants

Claude 4 is available in two variants, each optimized for different use cases:

\- \*\*Claude Sonnet 4\*\*: Balanced performance suitable for most enterprise applications, with significant improvements over Sonnet 3.7.

\- \*\*Claude Opus 4\*\*: Anthropic's most powerful model and the best coding model available. Excels at sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours.

\## Get Started

Ready to dive in? Here's how you can begin:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Use ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: OpenAI Responses API

description: Get started with the OpenAI Responses API using the AI SDK.

tags: \['getting-started', 'agents']

---

\# Get started with OpenAI Responses API

With the \[release of OpenAI's responses API](https://openai.com/index/new-tools-for-building-agents/), there has never been a better time to start building AI applications, particularly those that require a deeper understanding of the world.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## OpenAI Responses API

OpenAI recently released the Responses API, a brand new way to build applications on OpenAI's platform. The new API offers a way to persist chat history, a web search tool for grounding LLM responses, file search tool for finding relevant files, and a computer use tool for building agents that can interact with and operate computers. Let's explore how to use the Responses API with the AI SDK.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call GPT-4o with the new Responses API using the AI SDK:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const { text } = await generateText({

&nbsp; model: openai.responses('gpt-4o'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});

```

\### Generating Structured Data

While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions (\[`generateObject`](/docs/reference/ai-sdk-core/generate-object) and \[`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```ts

import { generateObject } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: openai.responses('gpt-4o'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.

\### Using Tools with the AI SDK

The Responses API supports tool calling out of the box, allowing it to interact with external systems and perform discrete tasks. Here's an example of using tool calling with the AI SDK:

```ts

import { generateText, tool } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { text } = await generateText({

&nbsp; model: openai.responses('gpt-4o'),

&nbsp; prompt: 'What is the weather like today in San Francisco?',

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; stopWhen: stepCountIs(5), // enable multi-step 'agentic' LLM calls

});

```

This example demonstrates how `stopWhen` transforms a single LLM call into an agent. The `stopWhen: stepCountIs(5)` parameter allows the model to autonomously call tools, analyze results, and make additional tool calls as needed - turning what would be a simple one-shot completion into an intelligent agent that can chain multiple actions together to complete complex tasks.

\### Web Search Tool

The Responses API introduces a built-in tool for grounding responses called `webSearch`. With this tool, the model can access the internet to find relevant information for its responses.

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.responses('gpt-4o-mini'),

&nbsp; prompt: 'What happened in San Francisco last week?',

&nbsp; tools: {

&nbsp;   web\_search\_preview: openai.tools.webSearchPreview(),

&nbsp; },

});



console.log(result.text);

console.log(result.sources);

```

The `webSearch` tool also allows you to specify query-specific metadata that can be used to improve the quality of the search results.

```ts

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.responses('gpt-4o-mini'),

&nbsp; prompt: 'What happened in San Francisco last week?',

&nbsp; tools: {

&nbsp;   web\_search\_preview: openai.tools.webSearchPreview({

&nbsp;     searchContextSize: 'high',

&nbsp;     userLocation: {

&nbsp;       type: 'approximate',

&nbsp;       city: 'San Francisco',

&nbsp;       region: 'California',

&nbsp;     },

&nbsp;   }),

&nbsp; },

});



console.log(result.text);

console.log(result.sources);

```

\### MCP Tool

The Responses API also supports connecting to \[Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers. This allows models to call tools exposed by remote MCP servers or service connectors.

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.responses('gpt-5-mini'),

&nbsp; prompt: 'Search the web for the latest NYC mayoral election results',

&nbsp; tools: {

&nbsp;   mcp: openai.tools.mcp({

&nbsp;     serverLabel: 'web-search',

&nbsp;     serverUrl: 'https://mcp.exa.ai/mcp',

&nbsp;     serverDescription: 'A web-search API for AI agents',

&nbsp;   }),

&nbsp; },

});



console.log(result.text);

```

For more details on configuring the MCP tool, including authentication, tool filtering, and connector support, see the \[OpenAI provider documentation](/providers/ai-sdk-providers/openai#mcp-tool).

\## Using Persistence

With the Responses API, you can persist chat history with OpenAI across requests. This allows you to send just the user's last message and OpenAI can access the entire chat history.

There are two options available to use persistence:

\### With previousResponseId

```tsx filename="app/api/chat/route.ts"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result1 = await generateText({

&nbsp; model: openai.responses('gpt-4o-mini'),

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

});



const result2 = await generateText({

&nbsp; model: openai.responses('gpt-4o-mini'),

&nbsp; prompt: 'Summarize in 2 sentences',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     previousResponseId: result1.providerMetadata?.openai.responseId as string,

&nbsp;   },

&nbsp; },

});

```

\### With Conversations

You can use the \[Conversation API](https://platform.openai.com/docs/api-reference/conversations/create) to create a conversation.

Once you have created a conversation, you can continue it:

```tsx filename="app/api/chat/route.ts"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.responses('gpt-4o-mini'),

&nbsp; prompt: 'Summarize in 2 sentences',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     // The Conversation ID created via the OpenAI API to continue

&nbsp;     conversation: 'conv\_123',

&nbsp;   },

&nbsp; },

});

```

\## Migrating from Completions API

Migrating from the OpenAI Completions API (via the AI SDK) to the new Responses API is simple. To migrate, simply change your provider instance from `openai(modelId)` to `openai.responses(modelId)`:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



// Completions API

const { text } = await generateText({

&nbsp; model: openai('gpt-4o'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});



// Responses API

const { text } = await generateText({

&nbsp; model: openai.responses('gpt-4o'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});

```

When using the Responses API, provider specific options that were previously specified on the model provider instance have now moved to the `providerOptions` object:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



// Completions API

const { text } = await generateText({

&nbsp; model: openai('gpt-4o'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     parallelToolCalls: false,

&nbsp;   },

&nbsp; },

});



// Responses API

const { text } = await generateText({

&nbsp; model: openai.responses('gpt-4o'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     parallelToolCalls: false,

&nbsp;   },

&nbsp; },

});

```

\## Get Started

Ready to get started? Here's how you can dive in:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Check out ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: Google Gemini Image Generation

description: Generate and edit images with Google Gemini 2.5 Flash Image using the AI SDK.

tags: \['image-generation', 'google', 'gemini']

---

\# Generate and Edit Images with Google Gemini 2.5 Flash

This guide will show you how to generate and edit images with the AI SDK and Google's latest multimodal language model Gemini 2.5 Flash Image.

\## Generating Images

As Gemini 2.5 Flash Image is a language model with multimodal capabilities, you can use the `generateText` or `streamText` functions (not `generateImage`) to create images. The model determines which modality to respond in based on your prompt and configuration. Here's how to create your first image:

```ts

import { generateText } from 'ai';

import fs from 'node:fs';

import 'dotenv/config';



async function generateImage() {

&nbsp; const result = await generateText({

&nbsp;   model: 'google/gemini-2.5-flash-image',

&nbsp;   prompt:

&nbsp;     'Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme',

&nbsp; });



&nbsp; // Save generated images

&nbsp; for (const file of result.files) {

&nbsp;   if (file.mediaType.startsWith('image/')) {

&nbsp;     const timestamp = Date.now();

&nbsp;     const fileName = `generated-${timestamp}.png`;



&nbsp;     fs.mkdirSync('output', { recursive: true });

&nbsp;     await fs.promises.writeFile(`output/${fileName}`, file.uint8Array);



&nbsp;     console.log(`Generated and saved image: output/${fileName}`);

&nbsp;   }

&nbsp; }

}



generateImage().catch(console.error);

```

Here are some key points to remember:

\- Generated images are returned in the `result.files` array

\- Images are returned as `Uint8Array` data

\- The model leverages Gemini's world knowledge, so detailed prompts yield better results

\## Editing Images

Gemini 2.5 Flash Image excels at editing existing images with natural language instructions. You can add elements, modify styles, or transform images while maintaining their core characteristics:

```ts

import { generateText } from 'ai';

import fs from 'node:fs';

import 'dotenv/config';



async function editImage() {

&nbsp; const editResult = await generateText({

&nbsp;   model: 'google/gemini-2.5-flash-image',

&nbsp;   prompt: \[

&nbsp;     {

&nbsp;       role: 'user',

&nbsp;       content: \[

&nbsp;         {

&nbsp;           type: 'text',

&nbsp;           text: 'Add a small wizard hat to this cat. Keep everything else the same.',

&nbsp;         },

&nbsp;         {

&nbsp;           type: 'image',

&nbsp;           // image: DataContent (string | Uint8Array | ArrayBuffer | Buffer) or URL

&nbsp;           image: new URL(

&nbsp;             'https://raw.githubusercontent.com/vercel/ai/refs/heads/main/examples/ai-functions/data/comic-cat.png',

&nbsp;           ),

&nbsp;           mediaType: 'image/jpeg',

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   ],

&nbsp; });



&nbsp; // Save the edited image

&nbsp; const timestamp = Date.now();

&nbsp; fs.mkdirSync('output', { recursive: true });



&nbsp; for (const file of editResult.files) {

&nbsp;   if (file.mediaType.startsWith('image/')) {

&nbsp;     await fs.promises.writeFile(

&nbsp;       `output/edited-${timestamp}.png`,

&nbsp;       file.uint8Array,

&nbsp;     );

&nbsp;     console.log(`Saved edited image: output/edited-${timestamp}.png`);

&nbsp;   }

&nbsp; }

}



editImage().catch(console.error);

```

\## What's Next?

You've learned how to generate new images from text prompts and edit existing images using natural language instructions with Google's Gemini 2.5 Flash Image model.

For more advanced techniques, integration patterns, and practical examples, check out our \[Cookbook](/cookbook) where you'll find comprehensive guides for building sophisticated AI-powered applications.

---

title: Get started with Claude 3.7 Sonnet

description: Get started with Claude 3.7 Sonnet using the AI SDK.

tags: \['getting-started']

---

\# Get started with Claude 3.7 Sonnet

With the \[release of Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet), there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Claude 3.7 Sonnet alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## Claude 3.7 Sonnet

Claude 3.7 Sonnet is Anthropic's most intelligent model to date and the first Claude model to offer extended thinking—the ability to solve complex problems with careful, step-by-step reasoning. With Claude 3.7 Sonnet, you can balance speed and quality by choosing between standard thinking for near-instant responses or extended thinking or advanced reasoning. Claude 3.7 Sonnet is state-of-the-art for coding, and delivers advancements in computer use, agentic capabilities, complex reasoning, and content generation. With frontier performance and more control over speed, Claude 3.7 Sonnet is a great choice for powering AI agents, especially customer-facing agents, and complex AI workflows.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Claude 3.7 Sonnet with the AI SDK:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: anthropic('claude-3-7-sonnet-20250219'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

});

console.log(text); // text response

```

The unified interface also means that you can easily switch between providers by changing just two lines of code. For example, to use Claude 3.7 Sonnet via Amazon Bedrock:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



const { reasoning, text } = await generateText({

&nbsp; model: bedrock('anthropic.claude-3-7-sonnet-20250219-v1:0'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

});

```

\### Reasoning Ability

Claude 3.7 Sonnet introduces a new extended thinking—the ability to solve complex problems with careful, step-by-step reasoning. You can enable it using the `thinking` provider option and specifying a thinking budget in tokens:

```ts

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: anthropic('claude-3-7-sonnet-20250219'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});



console.log(reasoningText); // reasoning text

console.log(reasoning); // reasoning details including redacted reasoning

console.log(text); // text response

```

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and Claude 3.7 Sonnet:

In a new Next.js application, first install the AI SDK and the Anthropic provider:

<Snippet text="pnpm install ai @ai-sdk/anthropic" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { streamText, convertToModelMessages, type UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: anthropic('claude-3-7-sonnet-20250219'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   providerOptions: {

&nbsp;     anthropic: {

&nbsp;       thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;     } satisfies AnthropicProviderOptions,

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   sendReasoning: true,

&nbsp; });

}

```

<Note>

&nbsp; You can forward the model's reasoning tokens to the client with

&nbsp; `sendReasoning: true` in the `toUIMessageStreamResponse` method.

</Note>

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat({

&nbsp;   transport: new DefaultChatTransport({ api: '/api/chat' }),

&nbsp; });



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   if (input.trim()) {

&nbsp;     sendMessage({ text: input });

&nbsp;     setInput('');

&nbsp;   }

&nbsp; };



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) => {

&nbsp;           // text parts:

&nbsp;           if (part.type === 'text') {

&nbsp;             return <div key={index}>{part.text}</div>;

&nbsp;           }

&nbsp;           // reasoning parts:

&nbsp;           if (part.type === 'reasoning') {

&nbsp;             return <pre key={index}>{part.text}</pre>;

&nbsp;           }

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         name="prompt"

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;       <button type="submit">Send</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

<Note>

&nbsp; You can access the model's reasoning tokens with the `reasoning` part on the

&nbsp; message `parts`.

</Note>

The useChat hook on your root page (`app/page.tsx`) will make a request to your LLM provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Get Started

Ready to dive in? Here's how you can begin:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Use ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

Claude 3.7 Sonnet opens new opportunities for reasoning-intensive AI applications. Start building today and leverage the power of advanced reasoning in your AI projects.

---

title: Get started with Llama 3.1

description: Get started with Llama 3.1 using the AI SDK.

tags: \['getting-started']

---

\# Get started with Llama 3.1

<Note>

&nbsp; The current generation of Llama models is 3.3. Please note that while this

&nbsp; guide focuses on Llama 3.1, the newer Llama 3.3 models are now available and

&nbsp; may offer improved capabilities. The concepts and integration techniques

&nbsp; described here remain applicable, though you may want to use the latest

&nbsp; generation models for optimal performance.

</Note>

With the \[release of Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/), there has never been a better time to start building AI applications.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI application with large language models (LLMs) like Llama 3.1 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more

\## Llama 3.1

The release of Meta's Llama 3.1 is an important moment in AI development. As the first state-of-the-art open weight AI model, Llama 3.1 is helping accelerate developers building AI apps. Available in 8B, 70B, and 405B sizes, these instruction-tuned models work well for tasks like dialogue generation, translation, reasoning, and code generation.

\## Benchmarks

Llama 3.1 surpasses most available open-source chat models on common industry benchmarks and even outperforms some closed-source models, offering superior performance in language nuances, contextual understanding, and complex multi-step tasks. The models' refined post-training processes significantly improve response alignment, reduce false refusal rates, and enhance answer diversity, making Llama 3.1 a powerful and accessible tool for building generative AI applications.

!\[Llama 3.1 Benchmarks](/images/llama-3_1-benchmarks.png)

Source: \[Meta AI - Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3\_1/MODEL\_CARD.md)

\## Choosing Model Size

Llama 3.1 includes a new 405B parameter model, becoming the largest open-source model available today. This model is designed to handle the most complex and demanding tasks.

When choosing between the different sizes of Llama 3.1 models (405B, 70B, 8B), consider the trade-off between performance and computational requirements. The 405B model offers the highest accuracy and capability for complex tasks but requires significant computational resources. The 70B model provides a good balance of performance and efficiency for most applications, while the 8B model is suitable for simpler tasks or resource-constrained environments where speed and lower computational overhead are priorities.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Llama 3.1 (using \[DeepInfra](https://deepinfra.com)) with the AI SDK:

```ts

import { deepinfra } from '@ai-sdk/deepinfra';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),

&nbsp; prompt: 'What is love?',

});

```

<Note>

&nbsp; Llama 3.1 is available to use with many AI SDK providers including

&nbsp; \[DeepInfra](/providers/ai-sdk-providers/deepinfra), \[Amazon

&nbsp; Bedrock](/providers/ai-sdk-providers/amazon-bedrock),

&nbsp; \[Baseten](/providers/ai-sdk-providers/baseten)

&nbsp; \[Fireworks](/providers/ai-sdk-providers/fireworks), and more.

</Note>

AI SDK Core abstracts away the differences between model providers, allowing you to focus on building great applications. Prefer to use \[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock)? The unified interface also means that you can easily switch between models by changing just two lines of code.

```tsx highlight="2,5"

import { generateText } from 'ai';

import { bedrock } from '@ai-sdk/amazon-bedrock';



const { text } = await generateText({

&nbsp; model: bedrock('meta.llama3-1-405b-instruct-v1'),

&nbsp; prompt: 'What is love?',

});

```

\### Streaming the Response

To stream the model's response as it's being generated, update your code snippet to use the \[`streamText`](/docs/reference/ai-sdk-core/stream-text) function.

```tsx

import { streamText } from 'ai';

import { deepinfra } from '@ai-sdk/deepinfra';



const { textStream } = streamText({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),

&nbsp; prompt: 'What is love?',

});

```

\### Generating Structured Data

While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions (\[`generateObject`](/docs/reference/ai-sdk-core/generate-object) and \[`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```ts

import { generateObject } from 'ai';

import { deepinfra } from '@ai-sdk/deepinfra';

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.

\### Tools

While LLMs have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). The solution: tools, which are like programs that you provide to the model, which it can choose to call as necessary.

\### Using Tools with the AI SDK

The AI SDK supports tool usage across several of its functions, including \[`generateText`](/docs/reference/ai-sdk-core/generate-text) and \[`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui). By passing one or more tools to the `tools` parameter, you can extend the capabilities of LLMs, allowing them to perform discrete tasks and interact with external systems.

Here's an example of how you can use a tool with the AI SDK and Llama 3.1:

```ts

import { generateText, tool } from 'ai';

import { deepinfra } from '@ai-sdk/deepinfra';

import { z } from 'zod';



const { text } = await generateText({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp; prompt: 'What is the weather like today?',

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

});

```

In this example, the `getWeather` tool allows the model to fetch real-time weather data, enhancing its ability to provide accurate and up-to-date information.

\### Agents

Agents take your AI applications a step further by allowing models to execute multiple steps (i.e. tools) in a non-deterministic way, making decisions based on context and user input.

Agents use LLMs to choose the next step in a problem-solving process. They can reason at each step and make decisions based on the evolving context.

\### Implementing Agents with the AI SDK

The AI SDK supports agent implementation through the `maxSteps` parameter. This allows the model to make multiple decisions and tool calls in a single interaction.

Here's an example of an agent that solves math problems:

```tsx

import { generateText, tool } from 'ai';

import { deepinfra } from '@ai-sdk/deepinfra';

import \* as mathjs from 'mathjs';

import { z } from 'zod';



const problem =

&nbsp; 'Calculate the profit for a day if revenue is $5000 and expenses are $3500.';



const { text: answer } = await generateText({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp; system:

&nbsp;   'You are solving math problems. Reason step by step. Use the calculator when necessary.',

&nbsp; prompt: problem,

&nbsp; tools: {

&nbsp;   calculate: tool({

&nbsp;     description: 'A tool for evaluating mathematical expressions.',

&nbsp;     inputSchema: z.object({ expression: z.string() }),

&nbsp;     execute: async ({ expression }) => mathjs.evaluate(expression),

&nbsp;   }),

&nbsp; },

&nbsp; maxSteps: 5,

});

```

In this example, the agent can use the calculator tool multiple times if needed, reasoning through the problem step by step.

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and Llama 3.1 (via \[DeepInfra](https://deepinfra.com)):

```tsx filename="app/api/chat/route.ts"

import { deepinfra } from '@ai-sdk/deepinfra';

import { convertToModelMessages, streamText } from 'ai';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { messages, input, handleInputChange, handleSubmit } = useChat();



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.content}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input name="prompt" value={input} onChange={handleInputChange} />

&nbsp;       <button type="submit">Submit</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then streamed back in real-time and displayed in the chat UI.

This enables a seamless chat experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.

\### Going Beyond Text

The AI SDK's React Server Components (RSC) API enables you to create rich, interactive interfaces that go beyond simple text generation. With the \[`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function, you can dynamically stream React components from the server to the client.

Let's dive into how you can leverage tools with \[AI SDK RSC](/docs/ai-sdk-rsc/overview) to build a generative user interface with Next.js (App Router).

First, create a Server Action.

```tsx filename="app/actions.tsx"

'use server';



import { streamUI } from '@ai-sdk/rsc';

import { deepinfra } from '@ai-sdk/deepinfra';

import { z } from 'zod';



export async function streamComponent() {

&nbsp; const result = await streamUI({

&nbsp;   model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp;   prompt: 'Get the weather for San Francisco',

&nbsp;   text: ({ content }) => <div>{content}</div>,

&nbsp;   tools: {

&nbsp;     getWeather: {

&nbsp;       description: 'Get the weather for a location',

&nbsp;       inputSchema: z.object({ location: z.string() }),

&nbsp;       generate: async function\* ({ location }) {

&nbsp;         yield <div>loading...</div>;

&nbsp;         const weather = '25c'; // await getWeather(location);

&nbsp;         return (

&nbsp;           <div>

&nbsp;             the weather in {location} is {weather}.

&nbsp;           </div>

&nbsp;         );

&nbsp;       },

&nbsp;     },

&nbsp;   },

&nbsp; });

&nbsp; return result.value;

}

```

In this example, if the model decides to use the `getWeather` tool, it will first yield a `div` while fetching the weather data, then return a weather component with the fetched data (note: static data in this example). This allows for a more dynamic and responsive UI that can adapt based on the AI's decisions and external data.

On the frontend, you can call this Server Action like any other asynchronous function in your application. In this case, the function returns a regular React component.

```tsx filename="app/page.tsx"

'use client';



import { useState } from 'react';

import { streamComponent } from './actions';



export default function Page() {

&nbsp; const \[component, setComponent] = useState<React.ReactNode>();



&nbsp; return (

&nbsp;   <div>

&nbsp;     <form

&nbsp;       onSubmit={async e => {

&nbsp;         e.preventDefault();

&nbsp;         setComponent(await streamComponent());

&nbsp;       }}

&nbsp;     >

&nbsp;       <button>Stream Component</button>

&nbsp;     </form>

&nbsp;     <div>{component}</div>

&nbsp;   </div>

&nbsp; );

}

```

To see AI SDK RSC in action, check out our open-source \[Next.js Gemini Chatbot](https://gemini.vercel.ai/).

\## Migrate from OpenAI

One of the key advantages of the AI SDK is its unified API, which makes it incredibly easy to switch between different AI models and providers. This flexibility is particularly useful when you want to migrate from one model to another, such as moving from OpenAI's GPT models to Meta's Llama models hosted on DeepInfra.

Here's how simple the migration process can be:

\*\*OpenAI Example:\*\*

```tsx

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const { text } = await generateText({

&nbsp; model: openai('gpt-4.1'),

&nbsp; prompt: 'What is love?',

});

```

\*\*Llama on DeepInfra Example:\*\*

```tsx

import { generateText } from 'ai';

import { deepinfra } from '@ai-sdk/deepinfra';



const { text } = await generateText({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp; prompt: 'What is love?',

});

```

Thanks to the unified API, the core structure of the code remains the same. The main differences are:

1\. Creating a DeepInfra client

2\. Changing the model name from `openai("gpt-4.1")` to `deepinfra("meta-llama/Meta-Llama-3.1-70B-Instruct")`.

With just these few changes, you've migrated from using OpenAI's GPT-4-Turbo to Meta's Llama 3.1 hosted on DeepInfra. The `generateText` function and its usage remain identical, showcasing the power of the AI SDK's unified API.

This feature allows you to easily experiment with different models, compare their performance, and choose the best one for your specific use case without having to rewrite large portions of your codebase.

\## Prompt Engineering and Fine-tuning

While the Llama 3.1 family of models are powerful out-of-the-box, their performance can be enhanced through effective prompt engineering and fine-tuning techniques.

\### Prompt Engineering

Prompt engineering is the practice of crafting input prompts to elicit desired outputs from language models. It involves structuring and phrasing prompts in ways that guide the model towards producing more accurate, relevant, and coherent responses.

For more information on prompt engineering techniques (specific to Llama models), check out these resources:

\- \[Official Llama 3.1 Prompt Guide](https://llama.meta.com/docs/how-to-guides/prompting)

\- \[Prompt Engineering with Llama 3](https://github.com/amitsangani/Llama/blob/main/Llama\_3\_Prompt\_Engineering.ipynb)

\- \[How to prompt Llama 3](https://huggingface.co/blog/llama3#how-to-prompt-llama-3)

\### Fine-tuning

Fine-tuning involves further training a pre-trained model on a specific dataset or task to customize its performance for particular use cases. This process allows you to adapt Llama 3.1 to your specific domain or application, potentially improving its accuracy and relevance for your needs.

To learn more about fine-tuning Llama models, check out these resources:

\- \[Official Fine-tuning Llama Guide](https://llama.meta.com/docs/how-to-guides/fine-tuning)

\- \[Fine-tuning and Inference with Llama 3](https://docs.inferless.com/how-to-guides/how-to-finetune--and-inference-llama3)

\- \[Fine-tuning Models with Fireworks AI](https://docs.fireworks.ai/fine-tuning/fine-tuning-models)

\- \[Fine-tuning Llama with Modal](https://modal.com/docs/examples/llm-finetuning)

\## Conclusion

The AI SDK offers a powerful and flexible way to integrate cutting-edge AI models like Llama 3.1 into your applications. With AI SDK Core, you can seamlessly switch between different AI models and providers by changing just two lines of code. This flexibility allows for quick experimentation and adaptation, reducing the time required to change models from days to minutes.

The AI SDK ensures that your application remains clean and modular, accelerating development and future-proofing against the rapidly evolving landscape.

Ready to get started? Here's how you can dive in:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Check out ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: Get started with GPT-5

description: Get started with GPT-5 using the AI SDK.

tags: \['getting-started']

---

\# Get started with OpenAI GPT-5

With the \[release of OpenAI's GPT-5 model](https://openai.com/index/introducing-gpt-5), there has never been a better time to start building AI applications with advanced capabilities like verbosity control, web search, and native multi-modal understanding.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like OpenAI GPT-5 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## OpenAI GPT-5

OpenAI's GPT-5 represents their latest advancement in language models, offering powerful new features including verbosity control for tailored response lengths, integrated web search capabilities, reasoning summaries for transparency, and native support for text, images, audio, and PDFs. The model is available in three variants: `gpt-5`, `gpt-5-mini` for faster, more cost-effective processing, and `gpt-5-nano` for ultra-efficient operations.

\### Prompt Engineering for GPT-5

Here are the key strategies for effective prompting:

\#### Core Principles

1\. \*\*Be precise and unambiguous\*\*: Avoid contradictory or ambiguous instructions. GPT-5 performs best with clear, explicit guidance.

2\. \*\*Use structured prompts\*\*: Leverage XML-like tags to organize different sections of your instructions for better clarity.

3\. \*\*Natural language works best\*\*: While being precise, write prompts as you would explain to a skilled colleague.

\#### Prompting Techniques

\*\*1. Agentic Workflow Control\*\*

\- Adjust the `reasoningEffort` parameter to calibrate model autonomy

\- Set clear stop conditions and define explicit tool call budgets

\- Provide guidance on exploration depth and persistence

```ts

// Example with reasoning effort control

const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Analyze this complex dataset and provide insights.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     reasoningEffort: 'high', // Increases autonomous exploration

&nbsp;   },

&nbsp; },

});

```

\*\*2. Structured Prompt Format\*\*

Use XML-like tags to organize your prompts:

```

<context\_gathering>

Goal: Extract key performance metrics from the report

Method: Focus on quantitative data and year-over-year comparisons

Early stop criteria: Stop after finding 5 key metrics

</context\_gathering>



<task>

Analyze the attached financial report and identify the most important metrics.

</task>

```

\*\*3. Tool Calling Best Practices\*\*

\- Use tool preambles to provide clear upfront plans

\- Define safe vs. unsafe actions for different tools

\- Create structured updates about tool call progress

\*\*4. Verbosity Control\*\*

\- Use the `textVerbosity` parameter to control response length programmatically

\- Override with natural language when needed for specific contexts

\- Balance between conciseness and completeness

\*\*5. Optimization Workflow\*\*

\- Start with a clear, simple prompt

\- Test and identify areas of ambiguity or confusion

\- Iteratively refine by removing contradictions

\- Consider using OpenAI's Prompt Optimizer tool for complex prompts

\- Document successful patterns for reuse

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call OpenAI GPT-5 with the AI SDK:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const { text } = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});

```

\### Generating Structured Data

While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions (\[`generateObject`](/docs/reference/ai-sdk-core/generate-object) and \[`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```ts

import { generateObject } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: openai('gpt-5'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.

\### Verbosity Control

One of GPT-5's new features is verbosity control, allowing you to adjust response length without modifying your prompt:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



// Concise response

const { text: conciseText } = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Explain quantum computing.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     textVerbosity: 'low', // Produces terse, minimal responses

&nbsp;   },

&nbsp; },

});



// Detailed response

const { text: detailedText } = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Explain quantum computing.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     textVerbosity: 'high', // Produces comprehensive, detailed responses

&nbsp;   },

&nbsp; },

});

```

\### Web Search

GPT-5 can access real-time information through the integrated web search tool:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'What are the latest developments in AI this week?',

&nbsp; tools: {

&nbsp;   web\_search: openai.tools.webSearch({

&nbsp;     searchContextSize: 'high',

&nbsp;   }),

&nbsp; },

});



// Access URL sources

const sources = result.sources;

```

\### Reasoning Summaries

For transparency into GPT-5's thought process, enable reasoning summaries:

```ts

import { openai } from '@ai-sdk/openai';

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: openai.responses('gpt-5'),

&nbsp; prompt:

&nbsp;   'Solve this logic puzzle: If all roses are flowers and some flowers fade quickly, do all roses fade quickly?',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     reasoningSummary: 'detailed', // 'auto' for condensed or 'detailed' for comprehensive

&nbsp;   },

&nbsp; },

});



// Stream reasoning and text separately

for await (const part of result.fullStream) {

&nbsp; if (part.type === 'reasoning') {

&nbsp;   console.log(part.textDelta);

&nbsp; } else if (part.type === 'text-delta') {

&nbsp;   process.stdout.write(part.textDelta);

&nbsp; }

}

```

\### Using Tools with the AI SDK

GPT-5 supports tool calling out of the box, allowing it to interact with external systems and perform discrete tasks. Here's an example of using tool calling with the AI SDK:

```ts

import { generateText, tool } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { toolResults } = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'What is the weather like today in San Francisco?',

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

});

```

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and OpenAI GPT-5:

In a new Next.js application, first install the AI SDK and the OpenAI provider:

<Snippet text="pnpm install ai @ai-sdk/openai @ai-sdk/react" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { openai } from '@ai-sdk/openai';

import { convertToModelMessages, streamText, UIMessage } from 'ai';



// Allow responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: openai('gpt-5'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat({});



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) => {

&nbsp;           if (part.type === 'text') {

&nbsp;             return <span key={index}>{part.text}</span>;

&nbsp;           }

&nbsp;           return null;

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({ text: input });

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         name="prompt"

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;       <button type="submit">Submit</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Get Started

Ready to get started? Here's how you can dive in:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/cookbook](/cookbook) to see the SDK in action and get inspired for your own projects.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at \[ai-sdk.dev/cookbook/guides](/cookbook/guides).

4\. Check out ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: Get started with OpenAI o1

description: Get started with OpenAI o1 using the AI SDK.

tags: \['getting-started', 'reasoning']

---

\# Get started with OpenAI o1

With the \[release of OpenAI's o1 series models](https://openai.com/index/learning-to-reason-with-llms/), there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like OpenAI o1 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## OpenAI o1

OpenAI released a series of AI models designed to spend more time thinking before responding. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math. These models, named the o1 series, are trained with reinforcement learning and can "think before they answer". As a result, they are able to produce a long internal chain of thought before responding to a prompt.

The main reasoning model available in the API is:

1\. \[\*\*o1\*\*](https://platform.openai.com/docs/models#o1): Designed to reason about hard problems using broad general knowledge about the world.

| Model | Streaming | Tools | Object Generation | Reasoning Effort |

| ----- | ------------------- | ------------------- | ------------------- | ------------------- |

| o1 | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

\### Benchmarks

OpenAI o1 models excel in scientific reasoning, with impressive performance across various domains:

\- Ranking in the 89th percentile on competitive programming questions (Codeforces)

\- Placing among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)

\- Exceeding human PhD-level accuracy on a benchmark of physics, biology, and chemistry problems (GPQA)

\[Source](https://openai.com/index/learning-to-reason-with-llms/)

\### Prompt Engineering for o1 Models

The o1 models perform best with straightforward prompts. Some prompt engineering techniques, like few-shot prompting or instructing the model to "think step by step," may not enhance performance and can sometimes hinder it. Here are some best practices:

1\. Keep prompts simple and direct: The models excel at understanding and responding to brief, clear instructions without the need for extensive guidance.

2\. Avoid chain-of-thought prompts: Since these models perform reasoning internally, prompting them to "think step by step" or "explain your reasoning" is unnecessary.

3\. Use delimiters for clarity: Use delimiters like triple quotation marks, XML tags, or section titles to clearly indicate distinct parts of the input, helping the model interpret different sections appropriately.

4\. Limit additional context in retrieval-augmented generation (RAG): When providing additional context or documents, include only the most relevant information to prevent the model from overcomplicating its response.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call OpenAI o1 with the AI SDK:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const { text } = await generateText({

&nbsp; model: openai('o1'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});

```

<Note>

&nbsp; To use the o1 model, you must either be using @ai-sdk/openai version 0.0.59 or

&nbsp; greater, or set `temperature: 1`.

</Note>

AI SDK Core abstracts away the differences between model providers, allowing you to focus on building great applications. The unified interface also means that you can easily switch between models by changing just one line of code.

```ts highlight="5"

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const { text } = await generateText({

&nbsp; model: openai('o1'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});

```

<Note>

&nbsp; System messages are automatically converted to OpenAI developer messages.

</Note>

\### Refining Reasoning Effort

You can control the amount of reasoning effort expended by o1 through the `reasoningEffort` parameter.

This parameter can be set to `'low'`, `'medium'`, or `'high'` to adjust how much time and computation the model spends on internal reasoning before producing a response.

```ts highlight="9"

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



// Reduce reasoning effort for faster responses

const { text } = await generateText({

&nbsp; model: openai('o1'),

&nbsp; prompt: 'Explain quantum entanglement briefly.',

&nbsp; providerOptions: {

&nbsp;   openai: { reasoningEffort: 'low' },

&nbsp; },

});

```

<Note>

&nbsp; The `reasoningEffort` parameter is only supported by o1 and has no effect on

&nbsp; other models.

</Note>

\### Generating Structured Data

While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions (\[`generateObject`](/docs/reference/ai-sdk-core/generate-object) and \[`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```ts

import { generateObject } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: openai('o1'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.

<Note>Structured object generation is supported with o1.</Note>

\### Tools

While LLMs have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). The solution: \[tools](/docs/foundations/tools), which are like programs that you provide to the model, which it can choose to call as necessary.

\### Using Tools with the AI SDK

The AI SDK supports tool usage across several of its functions, like \[`generateText`](/docs/reference/ai-sdk-core/generate-text) and \[`streamText`](/docs/reference/ai-sdk-core/stream-text). By passing one or more tools to the `tools` parameter, you can extend the capabilities of LLMs, allowing them to perform discrete tasks and interact with external systems.

Here's an example of how you can use a tool with the AI SDK and o1:

```ts

import { generateText, tool } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { text } = await generateText({

&nbsp; model: openai('o1'),

&nbsp; prompt: 'What is the weather like today?',

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

});

```

In this example, the `getWeather` tool allows the model to fetch real-time weather data (simulated for simplicity), enhancing its ability to provide accurate and up-to-date information.

<Note>Tools are compatible with o1.</Note>

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and OpenAI o1:

```tsx filename="app/api/chat/route.ts"

import { openai } from '@ai-sdk/openai';

import { convertToModelMessages, streamText, UIMessage } from 'ai';



// Allow responses up to 5 minutes

export const maxDuration = 300;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: openai('o1'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { messages, input, handleInputChange, handleSubmit, error } = useChat();



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.content}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input name="prompt" value={input} onChange={handleInputChange} />

&nbsp;       <button type="submit">Submit</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Get Started

Ready to get started? Here's how you can dive in:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.

1\. Check out our support for the o1 series of reasoning models in the \[OpenAI Provider](/providers/ai-sdk-providers/openai#reasoning-models).

1\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.

1\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at \[ai-sdk.dev/docs/guides](/docs/guides).

1\. Check out ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: Get started with OpenAI o3-mini

description: Get started with OpenAI o3-mini using the AI SDK.

tags: \['getting-started', 'reasoning']

---

\# Get started with OpenAI o3-mini

With the \[release of OpenAI's o3-mini model](https://openai.com/index/openai-o3-mini/), there has never been a better time to start building AI applications, particularly those that require complex STEM reasoning capabilities.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like OpenAI o3-mini alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## OpenAI o3-mini

OpenAI recently released a new AI model optimized for STEM reasoning that excels in science, math, and coding tasks. o3-mini matches o1's performance in these domains while delivering faster responses and lower costs. The model supports tool calling, structured outputs, and system messages, making it a great option for a wide range of applications.

o3-mini offers three reasoning effort levels:

1\. \[\*\*Low\*\*]: Optimized for speed while maintaining solid reasoning capabilities

2\. \[\*\*Medium\*\*]: Balanced approach matching o1's performance levels

3\. \[\*\*High\*\*]: Enhanced reasoning power exceeding o1 in many STEM domains

| Model | Streaming | Tool Calling | Structured Output | Reasoning Effort | Image Input |

| ------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| o3-mini | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

\### Benchmarks

OpenAI o3-mini demonstrates impressive performance across technical domains:

\- 87.3% accuracy on AIME competition math questions

\- 79.7% accuracy on PhD-level science questions (GPQA Diamond)

\- 2130 Elo rating on competitive programming (Codeforces)

\- 49.3% accuracy on verified software engineering tasks (SWE-bench)

<Note>These benchmark results are using high reasoning effort setting.</Note>

\[Source](https://openai.com/index/openai-o3-mini/)

\### Prompt Engineering for o3-mini

The o3-mini model performs best with straightforward prompts. Some prompt engineering techniques, like few-shot prompting or instructing the model to "think step by step," may not enhance performance and can sometimes hinder it. Here are some best practices:

1\. Keep prompts simple and direct: The model excels at understanding and responding to brief, clear instructions without the need for extensive guidance.

2\. Avoid chain-of-thought prompts: Since the model performs reasoning internally, prompting it to "think step by step" or "explain your reasoning" is unnecessary.

3\. Use delimiters for clarity: Use delimiters like triple quotation marks, XML tags, or section titles to clearly indicate distinct parts of the input.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call OpenAI o3-mini with the AI SDK:

```ts

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



const { text } = await generateText({

&nbsp; model: openai('o3-mini'),

&nbsp; prompt: 'Explain the concept of quantum entanglement.',

});

```

<Note>

&nbsp; To use o3-mini, you must be using @ai-sdk/openai version 1.1.9 or greater.

</Note>

<Note>

&nbsp; System messages are automatically converted to OpenAI developer messages.

</Note>

\### Refining Reasoning Effort

You can control the amount of reasoning effort expended by o3-mini through the `reasoningEffort` parameter.

This parameter can be set to `low`, `medium`, or `high` to adjust how much time and computation the model spends on internal reasoning before producing a response.

```ts highlight="9"

import { generateText } from 'ai';

import { openai } from '@ai-sdk/openai';



// Reduce reasoning effort for faster responses

const { text } = await generateText({

&nbsp; model: openai('o3-mini'),

&nbsp; prompt: 'Explain quantum entanglement briefly.',

&nbsp; providerOptions: {

&nbsp;   openai: { reasoningEffort: 'low' },

&nbsp; },

});

```

\### Generating Structured Data

While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions (\[`generateObject`](/docs/reference/ai-sdk-core/generate-object) and \[`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```ts

import { generateObject } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: openai('o3-mini'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.

\### Using Tools with the AI SDK

o3-mini supports tool calling out of the box, allowing it to interact with external systems and perform discrete tasks. Here's an example of using tool calling with the AI SDK:

```ts

import { generateText, tool } from 'ai';

import { openai } from '@ai-sdk/openai';

import { z } from 'zod';



const { text } = await generateText({

&nbsp; model: openai('o3-mini'),

&nbsp; prompt: 'What is the weather like today in San Francisco?',

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

});

```

In this example, the `getWeather` tool allows the model to fetch real-time weather data (simulated for simplicity), enhancing its ability to provide accurate and up-to-date information.

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and OpenAI o3-mini:

In a new Next.js application, first install the AI SDK and the DeepSeek provider:

<Snippet text="pnpm install ai @ai-sdk/openai @ai-sdk/react" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { openai } from '@ai-sdk/openai';

import { convertToModelMessages, streamText, UIMessage } from 'ai';



// Allow responses up to 5 minutes

export const maxDuration = 300;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: openai('o3-mini'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { messages, input, handleInputChange, handleSubmit, error } = useChat();



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.content}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input name="prompt" value={input} onChange={handleInputChange} />

&nbsp;       <button type="submit">Submit</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Get Started

Ready to get started? Here's how you can dive in:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.

2\. Check out our support for o3-mini in the \[OpenAI Provider](/providers/ai-sdk-providers/openai#reasoning-models).

3\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.

4\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at \[ai-sdk.dev/docs/guides](/docs/guides).

5\. Check out ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: Get started with DeepSeek R1

description: Get started with DeepSeek R1 using the AI SDK.

tags: \['getting-started', 'reasoning']

---

\# Get started with DeepSeek R1

With the \[release of DeepSeek R1](https://api-docs.deepseek.com/news/news250528), there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like DeepSeek R1 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## DeepSeek R1

DeepSeek R1 is a series of advanced AI models designed to tackle complex reasoning tasks in science, coding, and mathematics. These models are optimized to "think before they answer," producing detailed internal chains of thought that aid in solving challenging problems.

The series includes two primary variants:

\- \*\*DeepSeek R1-Zero\*\*: Trained exclusively with reinforcement learning (RL) without any supervised fine-tuning. It exhibits advanced reasoning capabilities but may struggle with readability and formatting.

\- \*\*DeepSeek R1\*\*: Combines reinforcement learning with cold-start data and supervised fine-tuning to improve both reasoning performance and the readability of outputs.

\### Benchmarks

DeepSeek R1 models excel in reasoning tasks, delivering competitive performance across key benchmarks:

\- \*\*AIME 2024 (Pass\\@1)\*\*: 79.8%

\- \*\*MATH-500 (Pass\\@1)\*\*: 97.3%

\- \*\*Codeforces (Percentile)\*\*: Top 4% (96.3%)

\- \*\*GPQA Diamond (Pass\\@1)\*\*: 71.5%

\[Source](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#4-evaluation-results)

\### Prompt Engineering for DeepSeek R1 Models

DeepSeek R1 models excel with structured and straightforward prompts. The following best practices can help achieve optimal performance:

1\. \*\*Use a structured format\*\*: Leverage the model’s preferred output structure with `<think>` tags for reasoning and `<answer>` tags for the final result.

2\. \*\*Prefer zero-shot prompts\*\*: Avoid few-shot prompting as it can degrade performance; instead, directly state the problem clearly.

3\. \*\*Specify output expectations\*\*: Guide the model by defining desired formats, such as markdown for readability or XML-like tags for clarity.

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call DeepSeek R1 with the AI SDK:

```ts

import { deepseek } from '@ai-sdk/deepseek';

import { generateText } from 'ai';



const { reasoningText, text } = await generateText({

&nbsp; model: deepseek('deepseek-reasoner'),

&nbsp; prompt: 'Explain quantum entanglement.',

});

```

The unified interface also means that you can easily switch between providers by changing just two lines of code. For example, to use DeepSeek R1 via Fireworks:

```ts

import { fireworks } from '@ai-sdk/fireworks';

import {

&nbsp; generateText,

&nbsp; wrapLanguageModel,

&nbsp; extractReasoningMiddleware,

} from 'ai';



// middleware to extract reasoning tokens

const enhancedModel = wrapLanguageModel({

&nbsp; model: fireworks('accounts/fireworks/models/deepseek-r1'),

&nbsp; middleware: extractReasoningMiddleware({ tagName: 'think' }),

});



const { reasoningText, text } = await generateText({

&nbsp; model: enhancedModel,

&nbsp; prompt: 'Explain quantum entanglement.',

});

```

Or to use Groq's `deepseek-r1-distill-llama-70b` model:

```ts

import { groq } from '@ai-sdk/groq';

import {

&nbsp; generateText,

&nbsp; wrapLanguageModel,

&nbsp; extractReasoningMiddleware,

} from 'ai';



// middleware to extract reasoning tokens

const enhancedModel = wrapLanguageModel({

&nbsp; model: groq('deepseek-r1-distill-llama-70b'),

&nbsp; middleware: extractReasoningMiddleware({ tagName: 'think' }),

});



const { reasoningText, text } = await generateText({

&nbsp; model: enhancedModel,

&nbsp; prompt: 'Explain quantum entanglement.',

});

```

<Note id="deepseek-r1-middleware">

The AI SDK provides a \[middleware](/docs/ai-sdk-core/middleware)

(`extractReasoningMiddleware`) that can be used to extract the reasoning

tokens from the model's output.

When using DeepSeek-R1 series models with third-party providers like Together AI, we recommend using the `startWithReasoning`

option in the `extractReasoningMiddleware` function, as they tend to bypass thinking patterns.

</Note>

\### Model Provider Comparison

You can use DeepSeek R1 with the AI SDK through various providers. Here's a comparison of the providers that support DeepSeek R1:

| Provider | Model ID | Reasoning Tokens |

| ------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ------------------- |

| \[DeepSeek](/providers/ai-sdk-providers/deepseek) | \[`deepseek-reasoner`](https://api-docs.deepseek.com/guides/reasoning\_model) | <Check size={18} /> |

| \[Fireworks](/providers/ai-sdk-providers/fireworks) | \[`accounts/fireworks/models/deepseek-r1`](https://fireworks.ai/models/fireworks/deepseek-r1) | Requires Middleware |

| \[Groq](/providers/ai-sdk-providers/groq) | \[`deepseek-r1-distill-llama-70b`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) | Requires Middleware |

| \[Azure](/providers/ai-sdk-providers/azure) | \[`DeepSeek-R1`](https://ai.azure.com/explore/models/DeepSeek-R1/version/1/registry/azureml-deepseek#code-samples) | Requires Middleware |

| \[Together AI](/providers/ai-sdk-providers/togetherai) | \[`deepseek-ai/DeepSeek-R1`](https://www.together.ai/models/deepseek-r1) | Requires Middleware |

| \[FriendliAI](/providers/community-providers/friendliai) | \[`deepseek-r1`](https://huggingface.co/deepseek-ai/DeepSeek-R1) | Requires Middleware |

| \[LangDB](/providers/community-providers/langdb) | \[`deepseek/deepseek-reasoner`](https://docs.langdb.ai/guides/deepseek) | Requires Middleware |

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with \[Next.js](https://nextjs.org), the AI SDK, and DeepSeek R1:

In a new Next.js application, first install the AI SDK and the DeepSeek provider:

<Snippet text="pnpm install ai @ai-sdk/deepseek @ai-sdk/react" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { deepseek } from '@ai-sdk/deepseek';

import { convertToModelMessages, streamText, UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: deepseek('deepseek-reasoner'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   sendReasoning: true,

&nbsp; });

}

```

<Note>

&nbsp; You can forward the model's reasoning tokens to the client with

&nbsp; `sendReasoning: true` in the `toDataStreamResponse` method.

</Note>

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();



&nbsp; const handleSubmit = (e: React.FormEvent<HTMLFormElement>) => {

&nbsp;   e.preventDefault();

&nbsp;   if (input.trim()) {

&nbsp;     sendMessage({ text: input });

&nbsp;     setInput('');

&nbsp;   }

&nbsp; };



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) => {

&nbsp;           if (part.type === 'reasoning') {

&nbsp;             return <pre key={index}>{part.text}</pre>;

&nbsp;           }

&nbsp;           if (part.type === 'text') {

&nbsp;             return <span key={index}>{part.text}</span>;

&nbsp;           }

&nbsp;           return null;

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         name="prompt"

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;       <button type="submit">Submit</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

<Note>

&nbsp; You can access the model's reasoning tokens through the `parts` array on the

&nbsp; `message` object, where reasoning parts have `type: 'reasoning'`.

</Note>

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Limitations

While DeepSeek R1 models are powerful, they have certain limitations:

\- No tool-calling support: DeepSeek R1 cannot directly interact with APIs or external tools.

\- No object generation support: DeepSeek R1 does not support structured object generation. However, you can combine it with models that support structured object generation (like gpt-4o-mini) to generate objects. See the \[structured object generation with a reasoning model recipe](/cookbook/node/generate-object-reasoning) for more information.

\## Get Started

Ready to dive in? Here's how you can begin:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Use ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

DeepSeek R1 opens new opportunities for reasoning-intensive AI applications. Start building today and leverage the power of advanced reasoning in your AI projects.

---

title: Get started with DeepSeek V3.2

description: Get started with DeepSeek V3.2 using the AI SDK.

tags: \['getting-started', 'agents']

---

\# Get started with DeepSeek V3.2

With the \[release of DeepSeek V3.2](https://api-docs.deepseek.com/news/news251201), there has never been a better time to start building AI applications that require advanced reasoning and agentic capabilities.

The \[AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like DeepSeek V3.2 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.

\## DeepSeek V3.2

DeepSeek V3.2 is a frontier model that harmonizes high computational efficiency with superior reasoning and agent performance. It introduces several key technical breakthroughs that enable it to perform comparably to GPT-5 while remaining open-source.

The series includes two primary variants:

\- \*\*DeepSeek V3.2\*\*: The official successor to V3.2-Exp. A balanced model optimized for both reasoning and inference efficiency, delivering GPT-5 level performance.

\- \*\*DeepSeek V3.2-Speciale\*\*: A high-compute variant with maxed-out reasoning capabilities that rivals Gemini-3.0-Pro. Achieves gold-medal performance in IMO 2025, CMO 2025, ICPC World Finals 2025, and IOI 2025. As of release, it does not support tool-use.

\### Benchmarks

DeepSeek V3.2 models excel in both reasoning and agentic tasks, delivering competitive performance across key benchmarks:

\*\*Reasoning Capabilities\*\*

\- \*\*AIME 2025 (Pass@1)\*\*: 96.0% (Speciale)

\- \*\*HMMT 2025 (Pass@1)\*\*: 99.2% (Speciale)

\- \*\*HLE (Pass@1)\*\*: 30.6%

\- \*\*Codeforces (Rating)\*\*: 2701 (Speciale)

\*\*Agentic Capabilities\*\*

\- \*\*SWE Verified (Resolved)\*\*: 73.1%

\- \*\*Terminal Bench 2.0 (Acc)\*\*: 46.4%

\- \*\*τ2 Bench (Pass@1)\*\*: 80.3%

\- \*\*Tool Decathlon (Pass@1)\*\*: 35.2%

\[Source](https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf)

\### Model Options

When using DeepSeek V3.2 with the AI SDK, you have two model options:

| Model Alias | Model Version | Description |

| ------------------- | --------------------------------- | ---------------------------------------------- |

| `deepseek-chat` | DeepSeek-V3.2 (Non-thinking Mode) | Standard chat model |

| `deepseek-reasoner` | DeepSeek-V3.2 (Thinking Mode) | Enhanced reasoning for complex problem-solving |

\## Getting Started with the AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building agents, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is \[AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call DeepSeek V3.2 with the AI SDK:

```ts

import { deepseek } from '@ai-sdk/deepseek';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: deepseek('deepseek-chat'),

&nbsp; prompt: 'Explain the concept of sparse attention in transformers.',

});

```

\### Building Interactive Interfaces

AI SDK Core can be paired with \[AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, and SvelteKit.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With three main hooks — \[`useChat`](/docs/reference/ai-sdk-ui/use-chat), \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and \[`useObject`](/docs/reference/ai-sdk-ui/use-object) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building an agent with \[Next.js](https://nextjs.org), the AI SDK, and DeepSeek V3.2:

In a new Next.js application, first install the AI SDK and the DeepSeek provider:

<Snippet text="pnpm install ai @ai-sdk/deepseek @ai-sdk/react" />

Then, create a route handler for the chat endpoint:

```tsx filename="app/api/chat/route.ts"

import { deepseek } from '@ai-sdk/deepseek';

import { convertToModelMessages, streamText, UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: deepseek('deepseek-reasoner'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({ sendReasoning: true });

}

```

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();



&nbsp; const handleSubmit = (e: React.FormEvent<HTMLFormElement>) => {

&nbsp;   e.preventDefault();

&nbsp;   if (input.trim()) {

&nbsp;     sendMessage({ text: input });

&nbsp;     setInput('');

&nbsp;   }

&nbsp; };



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) => {

&nbsp;           if (part.type === 'text' || part.type === 'reasoning') {

&nbsp;             return <div key={index}>{part.text}</div>;

&nbsp;           }

&nbsp;           return null;

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         name="prompt"

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;       <button type="submit">Submit</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.

\## Enhance Your Agent with Tools

One of the key strengths of DeepSeek V3.2 is its agentic capabilities. You can extend your agent's functionality by adding tools that allow the model to perform specific actions or retrieve information.

\### Update Your Route Handler

Let's add a weather tool to your agent. Update your route handler at `app/api/chat/route.ts`:

```tsx filename="app/api/chat/route.ts"

import { deepseek } from '@ai-sdk/deepseek';

import {

&nbsp; convertToModelMessages,

&nbsp; stepCountIs,

&nbsp; streamText,

&nbsp; tool,

&nbsp; UIMessage,

} from 'ai';

import { z } from 'zod';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: deepseek('deepseek-reasoner'),

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   tools: {

&nbsp;     weather: tool({

&nbsp;       description: 'Get the weather in a location',

&nbsp;       inputSchema: z.object({

&nbsp;         location: z.string().describe('The location to get the weather for'),

&nbsp;       }),

&nbsp;       execute: async ({ location }) => ({

&nbsp;         location,

&nbsp;         temperature: 72,

&nbsp;         unit: 'fahrenheit',

&nbsp;       }),

&nbsp;     }),

&nbsp;   },

&nbsp;   stopWhen: stepCountIs(5),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({ sendReasoning: true });

}

```

This adds a weather tool that the model can call when needed. The `stopWhen: stepCountIs(5)` parameter allows the agent to continue executing for multiple steps (up to 5), enabling it to use tools and reason iteratively before stopping. Learn more about \[loop control](/docs/agents/loop-control) to customize when and how your agent stops execution.

\## Get Started

Ready to dive in? Here's how you can begin:

1\. Explore the documentation at \[ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.

2\. Check out practical examples at \[ai-sdk.dev/examples](/examples) to see the SDK in action.

3\. Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at \[ai-sdk.dev/docs/guides](/docs/guides).

4\. Use ready-to-deploy AI templates at \[vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

---

title: Guides

description: Learn how to build AI applications with the AI SDK

---

\# Guides

These use-case specific guides are intended to help you build real applications with the AI SDK.

<IndexCards

&nbsp; cards={\[

&nbsp; {

&nbsp; title: 'RAG Agent',

&nbsp; description:

&nbsp; 'Learn how to build a RAG Agent with the AI SDK and Next.js.',

&nbsp; href: '/cookbook/guides/rag-chatbot',

&nbsp; },

&nbsp; {

&nbsp; title: 'Multi-Modal Agent',

&nbsp; description:

&nbsp; 'Learn how to build a multi-modal agent that can process images and PDFs with the AI SDK.',

&nbsp; href: '/cookbook/guides/multi-modal-chatbot',

&nbsp; },

&nbsp; {

&nbsp; title: 'Slackbot Agent',

&nbsp; description: 'Learn how to use the AI SDK to build an AI Agent in Slack.',

&nbsp; href: '/cookbook/guides/slackbot',

&nbsp; },

&nbsp; {

&nbsp; title: 'Natural Language Postgres (SQL Agent)',

&nbsp; description:

&nbsp; 'Learn how to build a Next.js app that lets you talk to a PostgreSQL database in natural language.',

&nbsp; href: '/cookbook/guides/natural-language-postgres',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with Computer Use',

&nbsp; description:

&nbsp; "Get started with Claude's Computer Use capabilities with the AI SDK.",

&nbsp; href: '/cookbook/guides/computer-use',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with Gemini 2.5',

&nbsp; description: 'Get started with Gemini 2.5 using the AI SDK.',

&nbsp; href: '/cookbook/guides/gemini-2-5',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with Claude 4',

&nbsp; description: 'Get started with Claude 4 using the AI SDK.',

&nbsp; href: '/cookbook/guides/claude-4',

&nbsp; },

&nbsp; {

&nbsp; title: 'OpenAI Responses API',

&nbsp; description:

&nbsp; 'Get started with the OpenAI Responses API using the AI SDK.',

&nbsp; href: '/cookbook/guides/openai-responses',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with Claude 3.7 Sonnet',

&nbsp; description: 'Get started with Claude 3.7 Sonnet using the AI SDK.',

&nbsp; href: '/cookbook/guides/sonnet-3-7',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with Llama 3.1',

&nbsp; description: 'Get started with Llama 3.1 using the AI SDK.',

&nbsp; href: '/cookbook/guides/llama-3_1',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with GPT-5',

&nbsp; description: 'Get started with GPT-5 using the AI SDK.',

&nbsp; href: '/cookbook/guides/gpt-5',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with OpenAI o1',

&nbsp; description: 'Get started with OpenAI o1 using the AI SDK.',

&nbsp; href: '/cookbook/guides/o1',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with OpenAI o3-mini',

&nbsp; description: 'Get started with OpenAI o3-mini using the AI SDK.',

&nbsp; href: '/cookbook/guides/o3',

&nbsp; },

&nbsp; {

&nbsp; title: 'Get started with DeepSeek R1',

&nbsp; description: 'Get started with DeepSeek R1 using the AI SDK.',

&nbsp; href: '/cookbook/guides/r1',

&nbsp; },

&nbsp; ]}

/>

---

title: Node.js HTTP Server

description: Learn how to use the AI SDK in a Node.js HTTP server

tags: \['api servers', 'streaming']

---

\# Node.js HTTP Server

You can use the AI SDK in a Node.js HTTP server to generate text and stream it to the client.

\## Examples

The examples start a simple HTTP server that listens on port 8080. You can e.g. test it using `curl`:

```bash

curl -X POST http://localhost:8080

```

<Note>

&nbsp; The examples use the Vercel AI Gateway. Ensure that your AI Gateway API key is

&nbsp; set in the `AI\_GATEWAY\_API\_KEY` environment variable.

</Note>

\*\*Full example\*\*: \[github.com/vercel/ai/examples/node-http-server](https://github.com/vercel/ai/tree/main/examples/node-http-server)

\### UI Message Stream

You can use the `pipeUIMessageStreamToResponse` method to pipe the stream data to the server response.

```ts filename='index.ts'

import { streamText } from 'ai';

import { createServer } from 'http';



createServer(async (req, res) => {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });



&nbsp; result.pipeUIMessageStreamToResponse(res);

}).listen(8080);

```

\### Sending Custom Data

`createUIMessageStream` and `pipeUIMessageStreamToResponse` can be used to send custom data to the client.

```ts filename='index.ts'

import {

&nbsp; createUIMessageStream,

&nbsp; pipeUIMessageStreamToResponse,

&nbsp; streamText,

} from 'ai';

import { createServer } from 'http';



createServer(async (req, res) => {

&nbsp; switch (req.url) {

&nbsp;   case '/stream-data': {

&nbsp;     const stream = createUIMessageStream({

&nbsp;       execute: ({ writer }) => {

&nbsp;         // write some custom data

&nbsp;         writer.write({ type: 'start' });



&nbsp;         writer.write({

&nbsp;           type: 'data-custom',

&nbsp;           data: {

&nbsp;             custom: 'Hello, world!',

&nbsp;           },

&nbsp;         });



&nbsp;         const result = streamText({

&nbsp;           model: 'openai/gpt-4o',

&nbsp;           prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;         });



&nbsp;         writer.merge(

&nbsp;           result.toUIMessageStream({

&nbsp;             sendStart: false,

&nbsp;             onError: error => {

&nbsp;               // Error messages are masked by default for security reasons.

&nbsp;               // If you want to expose the error message to the client, you can do so here:

&nbsp;               return error instanceof Error ? error.message : String(error);

&nbsp;             },

&nbsp;           }),

&nbsp;         );

&nbsp;       },

&nbsp;     });



&nbsp;     pipeUIMessageStreamToResponse({ stream, response: res });



&nbsp;     break;

&nbsp;   }

&nbsp; }

}).listen(8080);

```

\### Text Stream

You can send a text stream to the client using `pipeTextStreamToResponse`.

```ts filename='index.ts'

import { streamText } from 'ai';

import { createServer } from 'http';



createServer(async (req, res) => {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });



&nbsp; result.pipeTextStreamToResponse(res);

}).listen(8080);

```

\## Troubleshooting

\- Streaming not working when \[proxied](/docs/troubleshooting/streaming-not-working-when-proxied)

---

title: Express

description: Learn how to use the AI SDK in an Express server

tags: \['api servers', 'streaming']

---

\# Express

You can use the AI SDK in an \[Express](https://expressjs.com/) server to generate and stream text and objects to the client.

\## Examples

The examples start a simple HTTP server that listens on port 8080. You can e.g. test it using `curl`:

```bash

curl -X POST http://localhost:8080

```

<Note>

&nbsp; The examples use the Vercel AI Gateway. Ensure that your AI Gateway API key is

&nbsp; set in the `AI\_GATEWAY\_API\_KEY` environment variable.

</Note>

\*\*Full example\*\*: \[github.com/vercel/ai/examples/express](https://github.com/vercel/ai/tree/main/examples/express)

\### UI Message Stream

You can use the `pipeUIMessageStreamToResponse` method to pipe the stream data to the server response.

```ts filename='index.ts'

import { streamText } from 'ai';

import express, { Request, Response } from 'express';



const app = express();



app.post('/', async (req: Request, res: Response) => {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });



&nbsp; result.pipeUIMessageStreamToResponse(res);

});



app.listen(8080, () => {

&nbsp; console.log(`Example app listening on port ${8080}`);

});

```

\### Sending Custom Data

`pipeUIMessageStreamToResponse` can be used to send custom data to the client.

```ts filename='index.ts'

import {

&nbsp; createUIMessageStream,

&nbsp; pipeUIMessageStreamToResponse,

&nbsp; streamText,

} from 'ai';

import express, { Request, Response } from 'express';



const app = express();



app.post('/custom-data-parts', async (req: Request, res: Response) => {

&nbsp; pipeUIMessageStreamToResponse({

&nbsp;   response: res,

&nbsp;   stream: createUIMessageStream({

&nbsp;     execute: async ({ writer }) => {

&nbsp;       writer.write({ type: 'start' });



&nbsp;       writer.write({

&nbsp;         type: 'data-custom',

&nbsp;         data: {

&nbsp;           custom: 'Hello, world!',

&nbsp;         },

&nbsp;       });



&nbsp;       const result = streamText({

&nbsp;         model: 'openai/gpt-4o',

&nbsp;         prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;       });



&nbsp;       writer.merge(result.toUIMessageStream({ sendStart: false }));

&nbsp;     },

&nbsp;   }),

&nbsp; });

});



app.listen(8080, () => {

&nbsp; console.log(`Example app listening on port ${8080}`);

});

```

\### Text Stream

You can send a text stream to the client using `pipeTextStreamToResponse`.

```ts filename='index.ts'

import { streamText } from 'ai';

import express, { Request, Response } from 'express';



const app = express();



app.post('/', async (req: Request, res: Response) => {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });



&nbsp; result.pipeTextStreamToResponse(res);

});



app.listen(8080, () => {

&nbsp; console.log(`Example app listening on port ${8080}`);

});

```

\## Troubleshooting

\- Streaming not working when \[proxied](/docs/troubleshooting/streaming-not-working-when-proxied)

---

title: Hono

description: Example of using the AI SDK in a Hono server.

tags: \['api servers', 'streaming']

---

\# Hono

You can use the AI SDK in a \[Hono](https://hono.dev/) server to generate and stream text and objects to the client.

\## Examples

The examples start a simple HTTP server that listens on port 8080. You can e.g. test it using `curl`:

```bash

curl -X POST http://localhost:8080

```

<Note>

&nbsp; The examples use the Vercel AI Gateway. Ensure that your AI Gateway API key is

&nbsp; set in the `AI\_GATEWAY\_API\_KEY` environment variable.

</Note>

\*\*Full example\*\*: \[github.com/vercel/ai/examples/hono](https://github.com/vercel/ai/tree/main/examples/hono)

\### UI Message Stream

You can use the `toUIMessageStreamResponse` method to create a properly formatted streaming response.

```ts filename='index.ts'

import { serve } from '@hono/node-server';

import { streamText } from 'ai';

import { Hono } from 'hono';



const app = new Hono();



app.post('/', async c => {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });

&nbsp; return result.toUIMessageStreamResponse();

});



serve({ fetch: app.fetch, port: 8080 });

```

\### Text Stream

You can use the `toTextStreamResponse` method to return a text stream response.

```ts filename='index.ts'

import { serve } from '@hono/node-server';

import { streamText } from 'ai';

import { Hono } from 'hono';



const app = new Hono();



app.post('/text', async c => {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Write a short poem about coding.',

&nbsp; });

&nbsp; return result.toTextStreamResponse();

});



serve({ fetch: app.fetch, port: 8080 });

```

\### Sending Custom Data

You can use `createUIMessageStream` and `createUIMessageStreamResponse` to send custom data to the client.

```ts filename='index.ts'

import { serve } from '@hono/node-server';

import {

&nbsp; createUIMessageStream,

&nbsp; createUIMessageStreamResponse,

&nbsp; streamText,

} from 'ai';

import { Hono } from 'hono';



const app = new Hono();



app.post('/stream-data', async c => {

&nbsp; // immediately start streaming the response

&nbsp; const stream = createUIMessageStream({

&nbsp;   execute: ({ writer }) => {

&nbsp;     writer.write({ type: 'start' });



&nbsp;     writer.write({

&nbsp;       type: 'data-custom',

&nbsp;       data: {

&nbsp;         custom: 'Hello, world!',

&nbsp;       },

&nbsp;     });



&nbsp;     const result = streamText({

&nbsp;       model: 'openai/gpt-4o',

&nbsp;       prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;     });



&nbsp;     writer.merge(

&nbsp;       result.toUIMessageStream({

&nbsp;         sendStart: false,

&nbsp;         onError: error => {

&nbsp;           // Error messages are masked by default for security reasons.

&nbsp;           // If you want to expose the error message to the client, you can do so here:

&nbsp;           return error instanceof Error ? error.message : String(error);

&nbsp;         },

&nbsp;       }),

&nbsp;     );

&nbsp;   },

&nbsp; });

&nbsp; return createUIMessageStreamResponse({ stream });

});



serve({ fetch: app.fetch, port: 8080 });

```

\## Troubleshooting

\- Streaming not working when \[proxied](/docs/troubleshooting/streaming-not-working-when-proxied)

---

title: Fastify

description: Learn how to use the AI SDK in a Fastify server

tags: \['api servers', 'streaming']

---

\# Fastify

You can use the AI SDK in a \[Fastify](https://fastify.dev/) server to generate and stream text and objects to the client.

\## Examples

The examples start a simple HTTP server that listens on port 8080. You can e.g. test it using `curl`:

```bash

curl -X POST http://localhost:8080

```

<Note>

&nbsp; The examples use the Vercel AI Gateway. Ensure that your AI Gateway API key is

&nbsp; set in the `AI\_GATEWAY\_API\_KEY` environment variable.

</Note>

\*\*Full example\*\*: \[github.com/vercel/ai/examples/fastify](https://github.com/vercel/ai/tree/main/examples/fastify)

\### Data Stream

You can use the `toDataStream` method to get a data stream from the result and then pipe it to the response.

```ts filename='index.ts'

import { streamText } from 'ai';

import Fastify from 'fastify';



const fastify = Fastify({ logger: true });



fastify.post('/', async function (request, reply) {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });



&nbsp; // Mark the response as a v1 data stream:

&nbsp; reply.header('X-Vercel-AI-Data-Stream', 'v1');

&nbsp; reply.header('Content-Type', 'text/plain; charset=utf-8');



&nbsp; return reply.send(result.toDataStream({ data }));

});



fastify.listen({ port: 8080 });

```

\### Sending Custom Data

`createDataStream` can be used to send custom data to the client.

```ts filename='index.ts' highlight="8-11,18"

import { createDataStream, streamText } from 'ai';

import Fastify from 'fastify';



const fastify = Fastify({ logger: true });



fastify.post('/stream-data', async function (request, reply) {

&nbsp; // immediately start streaming the response

&nbsp; const dataStream = createDataStream({

&nbsp;   execute: async dataStreamWriter => {

&nbsp;     dataStreamWriter.writeData('initialized call');



&nbsp;     const result = streamText({

&nbsp;       model: 'openai/gpt-4o',

&nbsp;       prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;     });



&nbsp;     result.mergeIntoDataStream(dataStreamWriter);

&nbsp;   },

&nbsp;   onError: error => {

&nbsp;     // Error messages are masked by default for security reasons.

&nbsp;     // If you want to expose the error message to the client, you can do so here:

&nbsp;     return error instanceof Error ? error.message : String(error);

&nbsp;   },

&nbsp; });



&nbsp; // Mark the response as a v1 data stream:

&nbsp; reply.header('X-Vercel-AI-Data-Stream', 'v1');

&nbsp; reply.header('Content-Type', 'text/plain; charset=utf-8');



&nbsp; return reply.send(dataStream);

});



fastify.listen({ port: 8080 });

```

\### Text Stream

You can use the `textStream` property to get a text stream from the result and then pipe it to the response.

```ts filename='index.ts' highlight="15"

import { streamText } from 'ai';

import Fastify from 'fastify';



const fastify = Fastify({ logger: true });



fastify.post('/', async function (request, reply) {

&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-4o',

&nbsp;   prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; });



&nbsp; reply.header('Content-Type', 'text/plain; charset=utf-8');



&nbsp; return reply.send(result.textStream);

});



fastify.listen({ port: 8080 });

```

\## Troubleshooting

\- Streaming not working when \[proxied](/docs/troubleshooting/streaming-not-working-when-proxied)

---

title: Nest.js

description: Learn how to use the AI SDK in a Nest.js server

tags: \['api servers', 'streaming']

---

\# Nest.js

You can use the AI SDK in a \[Nest.js](https://nestjs.com/) server to generate and stream text and objects to the client.

\## Examples

The examples show how to implement a Nest.js controller that uses the AI SDK to stream text and objects to the client.

\*\*Full example\*\*: \[github.com/vercel/ai/examples/nest](https://github.com/vercel/ai/tree/main/examples/nest)

\### UI Message Stream

You can use the `pipeUIMessageStreamToResponse` method to pipe the stream data to the server response.

```ts filename='app.controller.ts'

import { Controller, Post, Res } from '@nestjs/common';

import { streamText } from 'ai';

import { Response } from 'express';



@Controller()

export class AppController {

&nbsp; @Post('/')

&nbsp; async root(@Res() res: Response) {

&nbsp;   const result = streamText({

&nbsp;     model: 'openai/gpt-4o',

&nbsp;     prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;   });



&nbsp;   result.pipeUIMessageStreamToResponse(res);

&nbsp; }

}

```

\### Sending Custom Data

`createUIMessageStream` and `pipeUIMessageStreamToResponse` can be used to send custom data to the client.

```ts filename='app.controller.ts'

import { Controller, Post, Res } from '@nestjs/common';

import {

&nbsp; createUIMessageStream,

&nbsp; streamText,

&nbsp; pipeUIMessageStreamToResponse,

} from 'ai';

import { Response } from 'express';



@Controller()

export class AppController {

&nbsp; @Post('/stream-data')

&nbsp; async streamData(@Res() response: Response) {

&nbsp;   const stream = createUIMessageStream({

&nbsp;     execute: ({ writer }) => {

&nbsp;       // write some data

&nbsp;       writer.write({ type: 'start' });



&nbsp;       writer.write({

&nbsp;         type: 'data-custom',

&nbsp;         data: {

&nbsp;           custom: 'Hello, world!',

&nbsp;         },

&nbsp;       });



&nbsp;       const result = streamText({

&nbsp;         model: 'openai/gpt-4o',

&nbsp;         prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;       });

&nbsp;       writer.merge(

&nbsp;         result.toUIMessageStream({

&nbsp;           sendStart: false,

&nbsp;           onError: error => {

&nbsp;             // Error messages are masked by default for security reasons.

&nbsp;             // If you want to expose the error message to the client, you can do so here:

&nbsp;             return error instanceof Error ? error.message : String(error);

&nbsp;           },

&nbsp;         }),

&nbsp;       );

&nbsp;     },

&nbsp;   });

&nbsp;   pipeUIMessageStreamToResponse({ stream, response });

&nbsp; }

}

```

\### Text Stream

You can use the `pipeTextStreamToResponse` method to get a text stream from the result and then pipe it to the response.

```ts filename='app.controller.ts'

import { Controller, Post, Res } from '@nestjs/common';

import { streamText } from 'ai';

import { Response } from 'express';



@Controller()

export class AppController {

&nbsp; @Post()

&nbsp; async example(@Res() res: Response) {

&nbsp;   const result = streamText({

&nbsp;     model: 'openai/gpt-4o',

&nbsp;     prompt: 'Invent a new holiday and describe its traditions.',

&nbsp;   });



&nbsp;   result.pipeTextStreamToResponse(res);

&nbsp; }

}

```

\## Troubleshooting

\- Streaming not working when \[proxied](/docs/troubleshooting/streaming-not-working-when-proxied)

---

title: AI SDK by Vercel

description: The AI SDK is the TypeScript toolkit for building AI applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.

---

\# AI SDK

The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.

\## Why use the AI SDK?

Integrating large language models (LLMs) into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK standardizes integrating artificial intelligence (AI) models across \[supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.

For example, here’s how you can generate text with various models using the AI SDK:

<PreviewSwitchProviders />

The AI SDK has two main libraries:

\- \*\*\[AI SDK Core](/docs/ai-sdk-core):\*\* A unified API for generating text, structured objects, tool calls, and building agents with LLMs.

\- \*\*\[AI SDK UI](/docs/ai-sdk-ui):\*\* A set of framework-agnostic hooks for quickly building chat and generative user interface.

\## Model Providers

The AI SDK supports \[multiple model providers](/providers).

<OfficialModelCards />

\## Templates

We've built some \[templates](https://vercel.com/templates?type=ai) that include AI SDK integrations for different use cases, providers, and frameworks. You can use these templates to get started with your AI-powered application.

\### Starter Kits

<Templates type="starter-kits" />

\### Feature Exploration

<Templates type="feature-exploration" />

\### Frameworks

<Templates type="frameworks" />

\### Generative UI

<Templates type="generative-ui" />

\### Security

<Templates type="security" />

\## Join our Community

If you have questions about anything related to the AI SDK, you're always welcome to ask our community on \[the Vercel Community](https://community.vercel.com/c/ai-sdk/62).

\## `llms.txt` (for Cursor, Windsurf, Copilot, Claude etc.)

You can access the entire AI SDK documentation in Markdown format at \[ai-sdk.dev/llms.txt](/llms.txt). This can be used to ask any LLM (assuming it has a big enough context window) questions about the AI SDK based on the most up-to-date documentation.

\### Example Usage

For instance, to prompt an LLM with questions about the AI SDK:

1\. Copy the documentation contents from \[ai-sdk.dev/llms.txt](/llms.txt)

2\. Use the following prompt format:

```prompt

Documentation:

{paste documentation here}

---

Based on the above documentation, answer the following:

{your question}

```

---

title: Overview

description: Learn how to build agents with the AI SDK.

---

\# Agents

Agents are \*\*large language models (LLMs)\*\* that use \*\*tools\*\* in a \*\*loop\*\* to accomplish tasks.

These components work together:

\- \*\*LLMs\*\* process input and decide the next action

\- \*\*Tools\*\* extend capabilities beyond text generation (reading files, calling APIs, writing to databases)

\- \*\*Loop\*\* orchestrates execution through:

&nbsp; - \*\*Context management\*\* - Maintaining conversation history and deciding what the model sees (input) at each step

&nbsp; - \*\*Stopping conditions\*\* - Determining when the loop (task) is complete

\## ToolLoopAgent Class

The ToolLoopAgent class handles these three components. Here's an agent that uses multiple tools in a loop to accomplish a task:

```ts

import { ToolLoopAgent, stepCountIs, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const weatherAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location (in Fahrenheit)',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp;   convertFahrenheitToCelsius: tool({

&nbsp;     description: 'Convert temperature from Fahrenheit to Celsius',

&nbsp;     inputSchema: z.object({

&nbsp;       temperature: z.number().describe('Temperature in Fahrenheit'),

&nbsp;     }),

&nbsp;     execute: async ({ temperature }) => {

&nbsp;       const celsius = Math.round((temperature - 32) \* (5 / 9));

&nbsp;       return { celsius };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; // Agent's default behavior is to stop after a maximum of 20 steps

&nbsp; // stopWhen: stepCountIs(20),

});



const result = await weatherAgent.generate({

&nbsp; prompt: 'What is the weather in San Francisco in celsius?',

});



console.log(result.text); // agent's final answer

console.log(result.steps); // steps taken by the agent

```

The agent automatically:

1\. Calls the `weather` tool to get the temperature in Fahrenheit

2\. Calls `convertFahrenheitToCelsius` to convert it

3\. Generates a final text response with the result

The Agent class handles the loop, context management, and stopping conditions.

\## Why Use the Agent Class?

The Agent class is the recommended approach for building agents with the AI SDK because it:

\- \*\*Reduces boilerplate\*\* - Manages loops and message arrays

\- \*\*Improves reusability\*\* - Define once, use throughout your application

\- \*\*Simplifies maintenance\*\* - Single place to update agent configuration

For most use cases, start with the Agent class. Use core functions (`generateText`, `streamText`) when you need explicit control over each step for complex structured workflows.

\## Structured Workflows

Agents are flexible and powerful, but non-deterministic. When you need reliable, repeatable outcomes with explicit control flow, use core functions with structured workflow patterns combining:

\- Conditional statements for explicit branching

\- Standard functions for reusable logic

\- Error handling for robustness

\- Explicit control flow for predictability

\[Explore workflow patterns](/docs/agents/workflows) to learn more about building structured, reliable systems.

\## Next Steps

\- \*\*\[Building Agents](/docs/agents/building-agents)\*\* - Guide to creating agents with the Agent class

\- \*\*\[Workflow Patterns](/docs/agents/workflows)\*\* - Structured patterns using core functions for complex workflows

\- \*\*\[Loop Control](/docs/agents/loop-control)\*\* - Execution control with stopWhen and prepareStep

---

title: Building Agents

description: Complete guide to creating agents with the Agent class.

---

\# Building Agents

The Agent class provides a structured way to encapsulate LLM configuration, tools, and behavior into reusable components. It handles the agent loop for you, allowing the LLM to call tools multiple times in sequence to accomplish complex tasks. Define agents once and use them across your application.

\## Why Use the ToolLoopAgent Class?

When building AI applications, you often need to:

\- \*\*Reuse configurations\*\* - Same model settings, tools, and prompts across different parts of your application

\- \*\*Maintain consistency\*\* - Ensure the same behavior and capabilities throughout your codebase

\- \*\*Simplify API routes\*\* - Reduce boilerplate in your endpoints

\- \*\*Type safety\*\* - Get full TypeScript support for your agent's tools and outputs

The ToolLoopAgent class provides a single place to define your agent's behavior.

\## Creating an Agent

Define an agent by instantiating the ToolLoopAgent class with your desired configuration:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const myAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: 'You are a helpful assistant.',

&nbsp; tools: {

&nbsp;   // Your tools here

&nbsp; },

});

```

\## Configuration Options

The Agent class accepts all the same settings as `generateText` and `streamText`. Configure:

\### Model and System Instructions

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: 'You are an expert software engineer.',

});

```

\### Tools

Provide tools that the agent can use to accomplish tasks:

```ts

import { ToolLoopAgent, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const codeAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   runCode: tool({

&nbsp;     description: 'Execute Python code',

&nbsp;     inputSchema: z.object({

&nbsp;       code: z.string(),

&nbsp;     }),

&nbsp;     execute: async ({ code }) => {

&nbsp;       // Execute code and return result

&nbsp;       return { output: 'Code executed successfully' };

&nbsp;     },

&nbsp;   }),

&nbsp; },

});

```

\### Loop Control

By default, agents run for 20 steps (`stopWhen: stepCountIs(20)`). In each step, the model either generates text or calls a tool. If it generates text, the agent completes. If it calls a tool, the AI SDK executes that tool.

To let agents call multiple tools in sequence, configure `stopWhen` to allow more steps. After each tool execution, the agent triggers a new generation where the model can call another tool or generate text:

```ts

import { ToolLoopAgent, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; stopWhen: stepCountIs(20), // Allow up to 20 steps

});

```

Each step represents one generation (which results in either text or a tool call). The loop continues until:

\- A finish reasoning other than tool-calls is returned, or

\- A tool that is invoked does not have an execute function, or

\- A tool call needs approval, or

\- A stop condition is met

You can combine multiple conditions:

```ts

import { ToolLoopAgent, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; stopWhen: \[

&nbsp;   stepCountIs(20), // Maximum 20 steps

&nbsp;   yourCustomCondition(), // Custom logic for when to stop

&nbsp; ],

});

```

Learn more about \[loop control and stop conditions](/docs/agents/loop-control).

\### Tool Choice

Control how the agent uses tools:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   // your tools here

&nbsp; },

&nbsp; toolChoice: 'required', // Force tool use

&nbsp; // or toolChoice: 'none' to disable tools

&nbsp; // or toolChoice: 'auto' (default) to let the model decide

});

```

You can also force the use of a specific tool:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   weather: weatherTool,

&nbsp;   cityAttractions: attractionsTool,

&nbsp; },

&nbsp; toolChoice: {

&nbsp;   type: 'tool',

&nbsp;   toolName: 'weather', // Force the weather tool to be used

&nbsp; },

});

```

\### Structured Output

Define structured output schemas:

```ts

import { ToolLoopAgent, Output, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const analysisAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     sentiment: z.enum(\['positive', 'neutral', 'negative']),

&nbsp;     summary: z.string(),

&nbsp;     keyPoints: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; stopWhen: stepCountIs(10),

});



const { output } = await analysisAgent.generate({

&nbsp; prompt: 'Analyze customer feedback from the last quarter',

});

```

\## Define Agent Behavior with System Instructions

System instructions define your agent's behavior, personality, and constraints. They set the context for all interactions and guide how the agent responds to user queries and uses tools.

\### Basic System Instructions

Set the agent's role and expertise:

```ts

const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions:

&nbsp;   'You are an expert data analyst. You provide clear insights from complex data.',

});

```

\### Detailed Behavioral Instructions

Provide specific guidelines for agent behavior:

```ts

const codeReviewAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: `You are a senior software engineer conducting code reviews.



&nbsp; Your approach:

&nbsp; - Focus on security vulnerabilities first

&nbsp; - Identify performance bottlenecks

&nbsp; - Suggest improvements for readability and maintainability

&nbsp; - Be constructive and educational in your feedback

&nbsp; - Always explain why something is an issue and how to fix it`,

});

```

\### Constrain Agent Behavior

Set boundaries and ensure consistent behavior:

```ts

const customerSupportAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: `You are a customer support specialist for an e-commerce platform.



&nbsp; Rules:

&nbsp; - Never make promises about refunds without checking the policy

&nbsp; - Always be empathetic and professional

&nbsp; - If you don't know something, say so and offer to escalate

&nbsp; - Keep responses concise and actionable

&nbsp; - Never share internal company information`,

&nbsp; tools: {

&nbsp;   checkOrderStatus,

&nbsp;   lookupPolicy,

&nbsp;   createTicket,

&nbsp; },

});

```

\### Tool Usage Instructions

Guide how the agent should use available tools:

```ts

const researchAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: `You are a research assistant with access to search and document tools.



&nbsp; When researching:

&nbsp; 1. Always start with a broad search to understand the topic

&nbsp; 2. Use document analysis for detailed information

&nbsp; 3. Cross-reference multiple sources before drawing conclusions

&nbsp; 4. Cite your sources when presenting information

&nbsp; 5. If information conflicts, present both viewpoints`,

&nbsp; tools: {

&nbsp;   webSearch,

&nbsp;   analyzeDocument,

&nbsp;   extractQuotes,

&nbsp; },

});

```

\### Format and Style Instructions

Control the output format and communication style:

```ts

const technicalWriterAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: `You are a technical documentation writer.



&nbsp; Writing style:

&nbsp; - Use clear, simple language

&nbsp; - Avoid jargon unless necessary

&nbsp; - Structure information with headers and bullet points

&nbsp; - Include code examples where relevant

&nbsp; - Write in second person ("you" instead of "the user")



&nbsp; Always format responses in Markdown.`,

});

```

\## Using an Agent

Once defined, you can use your agent in three ways:

\### Generate Text

Use `generate()` for one-time text generation:

```ts

const result = await myAgent.generate({

&nbsp; prompt: 'What is the weather like?',

});



console.log(result.text);

```

\### Stream Text

Use `stream()` for streaming responses:

```ts

const stream = myAgent.stream({

&nbsp; prompt: 'Tell me a story',

});



for await (const chunk of stream.textStream) {

&nbsp; console.log(chunk);

}

```

\### Respond to UI Messages

Use `createAgentUIStreamResponse()` to create API responses for client applications:

```ts

// In your API route (e.g., app/api/chat/route.ts)

import { createAgentUIStreamResponse } from 'ai';



export async function POST(request: Request) {

&nbsp; const { messages } = await request.json();



&nbsp; return createAgentUIStreamResponse({

&nbsp;   agent: myAgent,

&nbsp;   messages,

&nbsp; });

}

```

\### Track Step Progress

Use `onStepFinish` to track each step's progress, including token usage:

```ts

const result = await myAgent.generate({

&nbsp; prompt: 'Research and summarize the latest AI trends',

&nbsp; onStepFinish: async ({ usage, finishReason, toolCalls }) => {

&nbsp;   console.log('Step completed:', {

&nbsp;     inputTokens: usage.inputTokens,

&nbsp;     outputTokens: usage.outputTokens,

&nbsp;     finishReason,

&nbsp;     toolsUsed: toolCalls?.map(tc => tc.toolName),

&nbsp;   });

&nbsp; },

});

```

You can also define `onStepFinish` in the constructor for agent-wide tracking. When both constructor and method callbacks are provided, both are called (constructor first, then the method callback):

```ts

const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; onStepFinish: async ({ usage }) => {

&nbsp;   // Agent-wide logging

&nbsp;   console.log('Agent step:', usage.totalTokens);

&nbsp; },

});



// Method-level callback runs after constructor callback

const result = await agent.generate({

&nbsp; prompt: 'Hello',

&nbsp; onStepFinish: async ({ usage }) => {

&nbsp;   // Per-call tracking (e.g., for billing)

&nbsp;   await trackUsage(usage);

&nbsp; },

});

```

\## End-to-end Type Safety

You can infer types for your agent's `UIMessage`s:

```ts

import { ToolLoopAgent, InferAgentUIMessage } from 'ai';



const myAgent = new ToolLoopAgent({

&nbsp; // ... configuration

});



// Infer the UIMessage type for UI components or persistence

export type MyAgentUIMessage = InferAgentUIMessage<typeof myAgent>;

```

Use this type in your client components with `useChat`:

```tsx filename="components/chat.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import type { MyAgentUIMessage } from '@/agent/my-agent';



export function Chat() {

&nbsp; const { messages } = useChat<MyAgentUIMessage>();

&nbsp; // Full type safety for your messages and tools

}

```

\## Next Steps

Now that you understand building agents, you can:

\- Explore \[workflow patterns](/docs/agents/workflows) for structured patterns using core functions

\- Learn about \[loop control](/docs/agents/loop-control) for advanced execution control

\- See \[manual loop examples](/cookbook/node/manual-agent-loop) for custom workflow implementations

---

title: Workflow Patterns

description: Learn workflow patterns for building reliable agents with the AI SDK.

---

\# Workflow Patterns

Combine the building blocks from the \[overview](/docs/agents/overview) with these patterns to add structure and reliability to your agents:

\- \[Sequential Processing](#sequential-processing-chains) - Steps executed in order

\- \[Parallel Processing](#parallel-processing) - Independent tasks run simultaneously

\- \[Evaluation/Feedback Loops](#evaluator-optimizer) - Results checked and improved iteratively

\- \[Orchestration](#orchestrator-worker) - Coordinating multiple components

\- \[Routing](#routing) - Directing work based on context

\## Choose Your Approach

Consider these key factors:

\- \*\*Flexibility vs Control\*\* - How much freedom does the LLM need vs how tightly you must constrain its actions?

\- \*\*Error Tolerance\*\* - What are the consequences of mistakes in your use case?

\- \*\*Cost Considerations\*\* - More complex systems typically mean more LLM calls and higher costs

\- \*\*Maintenance\*\* - Simpler architectures are easier to debug and modify

\*\*Start with the simplest approach that meets your needs\*\*. Add complexity only when required by:

1\. Breaking down tasks into clear steps

2\. Adding tools for specific capabilities

3\. Implementing feedback loops for quality control

4\. Introducing multiple agents for complex workflows

Let's look at examples of these patterns in action.

\## Patterns with Examples

These patterns, adapted from \[Anthropic's guide on building effective agents](https://www.anthropic.com/research/building-effective-agents), serve as building blocks you can combine to create comprehensive workflows. Each pattern addresses specific aspects of task execution. Combine them thoughtfully to build reliable solutions for complex problems.

\## Sequential Processing (Chains)

The simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. Use this pattern for tasks with well-defined sequences, like content generation pipelines or data transformation processes.

```ts

import { generateText, generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



async function generateMarketingCopy(input: string) {

&nbsp; const model = \_\_MODEL\_\_;



&nbsp; // First step: Generate marketing copy

&nbsp; const { text: copy } = await generateText({

&nbsp;   model,

&nbsp;   prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,

&nbsp; });



&nbsp; // Perform quality check on copy

&nbsp; const { object: qualityMetrics } = await generateObject({

&nbsp;   model,

&nbsp;   schema: z.object({

&nbsp;     hasCallToAction: z.boolean(),

&nbsp;     emotionalAppeal: z.number().min(1).max(10),

&nbsp;     clarity: z.number().min(1).max(10),

&nbsp;   }),

&nbsp;   prompt: `Evaluate this marketing copy for:

&nbsp;   1. Presence of call to action (true/false)

&nbsp;   2. Emotional appeal (1-10)

&nbsp;   3. Clarity (1-10)



&nbsp;   Copy to evaluate: ${copy}`,

&nbsp; });



&nbsp; // If quality check fails, regenerate with more specific instructions

&nbsp; if (

&nbsp;   !qualityMetrics.hasCallToAction ||

&nbsp;   qualityMetrics.emotionalAppeal < 7 ||

&nbsp;   qualityMetrics.clarity < 7

&nbsp; ) {

&nbsp;   const { text: improvedCopy } = await generateText({

&nbsp;     model,

&nbsp;     prompt: `Rewrite this marketing copy with:

&nbsp;     ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}

&nbsp;     ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}

&nbsp;     ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}



&nbsp;     Original copy: ${copy}`,

&nbsp;   });

&nbsp;   return { copy: improvedCopy, qualityMetrics };

&nbsp; }



&nbsp; return { copy, qualityMetrics };

}

```

\## Routing

This pattern lets the model decide which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. Use this when handling varied inputs that require different processing approaches. In the example below, the first LLM call's results determine the second call's model size and system prompt.

```ts

import { generateObject, generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



async function handleCustomerQuery(query: string) {

&nbsp; const model = \_\_MODEL\_\_;



&nbsp; // First step: Classify the query type

&nbsp; const { object: classification } = await generateObject({

&nbsp;   model,

&nbsp;   schema: z.object({

&nbsp;     reasoning: z.string(),

&nbsp;     type: z.enum(\['general', 'refund', 'technical']),

&nbsp;     complexity: z.enum(\['simple', 'complex']),

&nbsp;   }),

&nbsp;   prompt: `Classify this customer query:

&nbsp;   ${query}



&nbsp;   Determine:

&nbsp;   1. Query type (general, refund, or technical)

&nbsp;   2. Complexity (simple or complex)

&nbsp;   3. Brief reasoning for classification`,

&nbsp; });



&nbsp; // Route based on classification

&nbsp; // Set model and system prompt based on query type and complexity

&nbsp; const { text: response } = await generateText({

&nbsp;   model:

&nbsp;     classification.complexity === 'simple'

&nbsp;       ? 'openai/gpt-4o-mini'

&nbsp;       : 'openai/o4-mini',

&nbsp;   system: {

&nbsp;     general:

&nbsp;       'You are an expert customer service agent handling general inquiries.',

&nbsp;     refund:

&nbsp;       'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',

&nbsp;     technical:

&nbsp;       'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',

&nbsp;   }\[classification.type],

&nbsp;   prompt: query,

&nbsp; });



&nbsp; return { response, classification };

}

```

\## Parallel Processing

Break down tasks into independent subtasks that execute simultaneously. This pattern uses parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyze multiple documents or process different aspects of a single input concurrently (like code review).

```ts

import { generateText, generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



// Example: Parallel code review with multiple specialized reviewers

async function parallelCodeReview(code: string) {

&nbsp; const model = \_\_MODEL\_\_;



&nbsp; // Run parallel reviews

&nbsp; const \[securityReview, performanceReview, maintainabilityReview] =

&nbsp;   await Promise.all(\[

&nbsp;     generateObject({

&nbsp;       model,

&nbsp;       system:

&nbsp;         'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',

&nbsp;       schema: z.object({

&nbsp;         vulnerabilities: z.array(z.string()),

&nbsp;         riskLevel: z.enum(\['low', 'medium', 'high']),

&nbsp;         suggestions: z.array(z.string()),

&nbsp;       }),

&nbsp;       prompt: `Review this code:

&nbsp;     ${code}`,

&nbsp;     }),



&nbsp;     generateObject({

&nbsp;       model,

&nbsp;       system:

&nbsp;         'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',

&nbsp;       schema: z.object({

&nbsp;         issues: z.array(z.string()),

&nbsp;         impact: z.enum(\['low', 'medium', 'high']),

&nbsp;         optimizations: z.array(z.string()),

&nbsp;       }),

&nbsp;       prompt: `Review this code:

&nbsp;     ${code}`,

&nbsp;     }),



&nbsp;     generateObject({

&nbsp;       model,

&nbsp;       system:

&nbsp;         'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',

&nbsp;       schema: z.object({

&nbsp;         concerns: z.array(z.string()),

&nbsp;         qualityScore: z.number().min(1).max(10),

&nbsp;         recommendations: z.array(z.string()),

&nbsp;       }),

&nbsp;       prompt: `Review this code:

&nbsp;     ${code}`,

&nbsp;     }),

&nbsp;   ]);



&nbsp; const reviews = \[

&nbsp;   { ...securityReview.object, type: 'security' },

&nbsp;   { ...performanceReview.object, type: 'performance' },

&nbsp;   { ...maintainabilityReview.object, type: 'maintainability' },

&nbsp; ];



&nbsp; // Aggregate results using another model instance

&nbsp; const { text: summary } = await generateText({

&nbsp;   model,

&nbsp;   system: 'You are a technical lead summarizing multiple code reviews.',

&nbsp;   prompt: `Synthesize these code review results into a concise summary with key actions:

&nbsp;   ${JSON.stringify(reviews, null, 2)}`,

&nbsp; });



&nbsp; return { reviews, summary };

}

```

\## Orchestrator-Worker

A primary model (orchestrator) coordinates the execution of specialized workers. Each worker optimizes for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.

```ts

import { generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



async function implementFeature(featureRequest: string) {

&nbsp; // Orchestrator: Plan the implementation

&nbsp; const { object: implementationPlan } = await generateObject({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   schema: z.object({

&nbsp;     files: z.array(

&nbsp;       z.object({

&nbsp;         purpose: z.string(),

&nbsp;         filePath: z.string(),

&nbsp;         changeType: z.enum(\['create', 'modify', 'delete']),

&nbsp;       }),

&nbsp;     ),

&nbsp;     estimatedComplexity: z.enum(\['low', 'medium', 'high']),

&nbsp;   }),

&nbsp;   system:

&nbsp;     'You are a senior software architect planning feature implementations.',

&nbsp;   prompt: `Analyze this feature request and create an implementation plan:

&nbsp;   ${featureRequest}`,

&nbsp; });



&nbsp; // Workers: Execute the planned changes

&nbsp; const fileChanges = await Promise.all(

&nbsp;   implementationPlan.files.map(async file => {

&nbsp;     // Each worker is specialized for the type of change

&nbsp;     const workerSystemPrompt = {

&nbsp;       create:

&nbsp;         'You are an expert at implementing new files following best practices and project patterns.',

&nbsp;       modify:

&nbsp;         'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',

&nbsp;       delete:

&nbsp;         'You are an expert at safely removing code while ensuring no breaking changes.',

&nbsp;     }\[file.changeType];



&nbsp;     const { object: change } = await generateObject({

&nbsp;       model: \_\_MODEL\_\_,

&nbsp;       schema: z.object({

&nbsp;         explanation: z.string(),

&nbsp;         code: z.string(),

&nbsp;       }),

&nbsp;       system: workerSystemPrompt,

&nbsp;       prompt: `Implement the changes for ${file.filePath} to support:

&nbsp;       ${file.purpose}



&nbsp;       Consider the overall feature context:

&nbsp;       ${featureRequest}`,

&nbsp;     });



&nbsp;     return {

&nbsp;       file,

&nbsp;       implementation: change,

&nbsp;     };

&nbsp;   }),

&nbsp; );



&nbsp; return {

&nbsp;   plan: implementationPlan,

&nbsp;   changes: fileChanges,

&nbsp; };

}

```

\## Evaluator-Optimizer

Add quality control to workflows with dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow proceeds, retries with adjusted parameters, or takes corrective action. This creates robust workflows capable of self-improvement and error recovery.

```ts

import { generateText, generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



async function translateWithFeedback(text: string, targetLanguage: string) {

&nbsp; let currentTranslation = '';

&nbsp; let iterations = 0;

&nbsp; const MAX\_ITERATIONS = 3;



&nbsp; // Initial translation

&nbsp; const { text: translation } = await generateText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   system: 'You are an expert literary translator.',

&nbsp;   prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:

&nbsp;   ${text}`,

&nbsp; });



&nbsp; currentTranslation = translation;



&nbsp; // Evaluation-optimization loop

&nbsp; while (iterations < MAX\_ITERATIONS) {

&nbsp;   // Evaluate current translation

&nbsp;   const { object: evaluation } = await generateObject({

&nbsp;     model: \_\_MODEL\_\_,

&nbsp;     schema: z.object({

&nbsp;       qualityScore: z.number().min(1).max(10),

&nbsp;       preservesTone: z.boolean(),

&nbsp;       preservesNuance: z.boolean(),

&nbsp;       culturallyAccurate: z.boolean(),

&nbsp;       specificIssues: z.array(z.string()),

&nbsp;       improvementSuggestions: z.array(z.string()),

&nbsp;     }),

&nbsp;     system: 'You are an expert in evaluating literary translations.',

&nbsp;     prompt: `Evaluate this translation:



&nbsp;     Original: ${text}

&nbsp;     Translation: ${currentTranslation}



&nbsp;     Consider:

&nbsp;     1. Overall quality

&nbsp;     2. Preservation of tone

&nbsp;     3. Preservation of nuance

&nbsp;     4. Cultural accuracy`,

&nbsp;   });



&nbsp;   // Check if quality meets threshold

&nbsp;   if (

&nbsp;     evaluation.qualityScore >= 8 \&\&

&nbsp;     evaluation.preservesTone \&\&

&nbsp;     evaluation.preservesNuance \&\&

&nbsp;     evaluation.culturallyAccurate

&nbsp;   ) {

&nbsp;     break;

&nbsp;   }



&nbsp;   // Generate improved translation based on feedback

&nbsp;   const { text: improvedTranslation } = await generateText({

&nbsp;     model: \_\_MODEL\_\_,

&nbsp;     system: 'You are an expert literary translator.',

&nbsp;     prompt: `Improve this translation based on the following feedback:

&nbsp;     ${evaluation.specificIssues.join('\\n')}

&nbsp;     ${evaluation.improvementSuggestions.join('\\n')}



&nbsp;     Original: ${text}

&nbsp;     Current Translation: ${currentTranslation}`,

&nbsp;   });



&nbsp;   currentTranslation = improvedTranslation;

&nbsp;   iterations++;

&nbsp; }



&nbsp; return {

&nbsp;   finalTranslation: currentTranslation,

&nbsp;   iterationsRequired: iterations,

&nbsp; };

}

```

---

title: Loop Control

description: Control agent execution with built-in loop management using stopWhen and prepareStep

---

\# Loop Control

You can control both the execution flow and the settings at each step of the agent loop. The loop continues until:

\- A finish reasoning other than tool-calls is returned, or

\- A tool that is invoked does not have an execute function, or

\- A tool call needs approval, or

\- A stop condition is met

The AI SDK provides built-in loop control through two parameters: `stopWhen` for defining stopping conditions and `prepareStep` for modifying settings (model, tools, messages, and more) between steps.

\## Stop Conditions

The `stopWhen` parameter controls when to stop execution when there are tool results in the last step. By default, agents stop after 20 steps using `stepCountIs(20)`.

When you provide `stopWhen`, the agent continues executing after tool calls until a stopping condition is met. When the condition is an array, execution stops when any of the conditions are met.

\### Use Built-in Conditions

The AI SDK provides several built-in stopping conditions:

```ts

import { ToolLoopAgent, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   // your tools

&nbsp; },

&nbsp; stopWhen: stepCountIs(20), // Default state: stop after 20 steps maximum

});



const result = await agent.generate({

&nbsp; prompt: 'Analyze this dataset and create a summary report',

});

```

\### Combine Multiple Conditions

Combine multiple stopping conditions. The loop stops when it meets any condition:

```ts

import { ToolLoopAgent, stepCountIs, hasToolCall } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   // your tools

&nbsp; },

&nbsp; stopWhen: \[

&nbsp;   stepCountIs(20), // Maximum 20 steps

&nbsp;   hasToolCall('someTool'), // Stop after calling 'someTool'

&nbsp; ],

});



const result = await agent.generate({

&nbsp; prompt: 'Research and analyze the topic',

});

```

\### Create Custom Conditions

Build custom stopping conditions for specific requirements:

```ts

import { ToolLoopAgent, StopCondition, ToolSet } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const tools = {

&nbsp; // your tools

} satisfies ToolSet;



const hasAnswer: StopCondition<typeof tools> = ({ steps }) => {

&nbsp; // Stop when the model generates text containing "ANSWER:"

&nbsp; return steps.some(step => step.text?.includes('ANSWER:')) ?? false;

};



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools,

&nbsp; stopWhen: hasAnswer,

});



const result = await agent.generate({

&nbsp; prompt: 'Find the answer and respond with "ANSWER: \[your answer]"',

});

```

Custom conditions receive step information across all steps:

```ts

const budgetExceeded: StopCondition<typeof tools> = ({ steps }) => {

&nbsp; const totalUsage = steps.reduce(

&nbsp;   (acc, step) => ({

&nbsp;     inputTokens: acc.inputTokens + (step.usage?.inputTokens ?? 0),

&nbsp;     outputTokens: acc.outputTokens + (step.usage?.outputTokens ?? 0),

&nbsp;   }),

&nbsp;   { inputTokens: 0, outputTokens: 0 },

&nbsp; );



&nbsp; const costEstimate =

&nbsp;   (totalUsage.inputTokens \* 0.01 + totalUsage.outputTokens \* 0.03) / 1000;

&nbsp; return costEstimate > 0.5; // Stop if cost exceeds $0.50

};

```

\## Prepare Step

The `prepareStep` callback runs before each step in the loop and defaults to the initial settings if you don't return any changes. Use it to modify settings, manage context, or implement dynamic behavior based on execution history.

\### Dynamic Model Selection

Switch models based on step requirements:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: 'openai/gpt-4o-mini', // Default model

&nbsp; tools: {

&nbsp;   // your tools

&nbsp; },

&nbsp; prepareStep: async ({ stepNumber, messages }) => {

&nbsp;   // Use a stronger model for complex reasoning after initial steps

&nbsp;   if (stepNumber > 2 \&\& messages.length > 10) {

&nbsp;     return {

&nbsp;       model: \_\_MODEL\_\_,

&nbsp;     };

&nbsp;   }

&nbsp;   // Continue with default settings

&nbsp;   return {};

&nbsp; },

});



const result = await agent.generate({

&nbsp; prompt: '...',

});

```

\### Context Management

Manage growing conversation history in long-running loops:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   // your tools

&nbsp; },

&nbsp; prepareStep: async ({ messages }) => {

&nbsp;   // Keep only recent messages to stay within context limits

&nbsp;   if (messages.length > 20) {

&nbsp;     return {

&nbsp;       messages: \[

&nbsp;         messages\[0], // Keep system instructions

&nbsp;         ...messages.slice(-10), // Keep last 10 messages

&nbsp;       ],

&nbsp;     };

&nbsp;   }

&nbsp;   return {};

&nbsp; },

});



const result = await agent.generate({

&nbsp; prompt: '...',

});

```

\### Tool Selection

Control which tools are available at each step:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   search: searchTool,

&nbsp;   analyze: analyzeTool,

&nbsp;   summarize: summarizeTool,

&nbsp; },

&nbsp; prepareStep: async ({ stepNumber, steps }) => {

&nbsp;   // Search phase (steps 0-2)

&nbsp;   if (stepNumber <= 2) {

&nbsp;     return {

&nbsp;       activeTools: \['search'],

&nbsp;       toolChoice: 'required',

&nbsp;     };

&nbsp;   }



&nbsp;   // Analysis phase (steps 3-5)

&nbsp;   if (stepNumber <= 5) {

&nbsp;     return {

&nbsp;       activeTools: \['analyze'],

&nbsp;     };

&nbsp;   }



&nbsp;   // Summary phase (step 6+)

&nbsp;   return {

&nbsp;     activeTools: \['summarize'],

&nbsp;     toolChoice: 'required',

&nbsp;   };

&nbsp; },

});



const result = await agent.generate({

&nbsp; prompt: '...',

});

```

You can also force a specific tool to be used:

```ts

prepareStep: async ({ stepNumber }) => {

&nbsp; if (stepNumber === 0) {

&nbsp;   // Force the search tool to be used first

&nbsp;   return {

&nbsp;     toolChoice: { type: 'tool', toolName: 'search' },

&nbsp;   };

&nbsp; }



&nbsp; if (stepNumber === 5) {

&nbsp;   // Force the summarize tool after analysis

&nbsp;   return {

&nbsp;     toolChoice: { type: 'tool', toolName: 'summarize' },

&nbsp;   };

&nbsp; }



&nbsp; return {};

};

```

\### Message Modification

Transform messages before sending them to the model:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   // your tools

&nbsp; },

&nbsp; prepareStep: async ({ messages, stepNumber }) => {

&nbsp;   // Summarize tool results to reduce token usage

&nbsp;   const processedMessages = messages.map(msg => {

&nbsp;     if (msg.role === 'tool' \&\& msg.content.length > 1000) {

&nbsp;       return {

&nbsp;         ...msg,

&nbsp;         content: summarizeToolResult(msg.content),

&nbsp;       };

&nbsp;     }

&nbsp;     return msg;

&nbsp;   });



&nbsp;   return { messages: processedMessages };

&nbsp; },

});



const result = await agent.generate({

&nbsp; prompt: '...',

});

```

\## Access Step Information

Both `stopWhen` and `prepareStep` receive detailed information about the current execution:

```ts

prepareStep: async ({

&nbsp; model, // Current model configuration

&nbsp; stepNumber, // Current step number (0-indexed)

&nbsp; steps, // All previous steps with their results

&nbsp; messages, // Messages to be sent to the model

}) => {

&nbsp; // Access previous tool calls and results

&nbsp; const previousToolCalls = steps.flatMap(step => step.toolCalls);

&nbsp; const previousResults = steps.flatMap(step => step.toolResults);



&nbsp; // Make decisions based on execution history

&nbsp; if (previousToolCalls.some(call => call.toolName === 'dataAnalysis')) {

&nbsp;   return {

&nbsp;     toolChoice: { type: 'tool', toolName: 'reportGenerator' },

&nbsp;   };

&nbsp; }



&nbsp; return {};

},

```

\## Forced Tool Calling

You can force the agent to always use tools by combining `toolChoice: 'required'` with a `done` tool that has no `execute` function. This pattern ensures the agent uses tools for every step and stops only when it explicitly signals completion.

```ts

import { ToolLoopAgent, tool } from 'ai';

import { z } from 'zod';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   search: searchTool,

&nbsp;   analyze: analyzeTool,

&nbsp;   done: tool({

&nbsp;     description: 'Signal that you have finished your work',

&nbsp;     inputSchema: z.object({

&nbsp;       answer: z.string().describe('The final answer'),

&nbsp;     }),

&nbsp;     // No execute function - stops the agent when called

&nbsp;   }),

&nbsp; },

&nbsp; toolChoice: 'required', // Force tool calls at every step

});



const result = await agent.generate({

&nbsp; prompt: 'Research and analyze this topic, then provide your answer.',

});



// extract answer from done tool call

const toolCall = result.staticToolCalls\[0]; // tool call from final step

if (toolCall?.toolName === 'done') {

&nbsp; console.log(toolCall.input.answer);

}

```

Key aspects of this pattern:

\- \*\*`toolChoice: 'required'`\*\*: Forces the model to call a tool at every step instead of generating text directly. This ensures the agent follows a structured workflow.

\- \*\*`done` tool without `execute`\*\*: A tool that has no `execute` function acts as a termination signal. When the agent calls this tool, the loop stops because there's no function to execute.

\- \*\*Accessing results\*\*: The final answer is available in `result.staticToolCalls`, which contains tool calls that weren't executed.

This pattern is useful when you want the agent to always use specific tools for operations (like code execution or data retrieval) rather than attempting to answer directly.

\## Manual Loop Control

For scenarios requiring complete control over the agent loop, you can use AI SDK Core functions (`generateText` and `streamText`) to implement your own loop management instead of using `stopWhen` and `prepareStep`. This approach provides maximum flexibility for complex workflows.

\### Implementing a Manual Loop

Build your own agent loop when you need full control over execution:

```ts

import { generateText, ModelMessage } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const messages: ModelMessage\[] = \[{ role: 'user', content: '...' }];



let step = 0;

const maxSteps = 10;



while (step < maxSteps) {

&nbsp; const result = await generateText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages,

&nbsp;   tools: {

&nbsp;     // your tools here

&nbsp;   },

&nbsp; });



&nbsp; messages.push(...result.response.messages);



&nbsp; if (result.text) {

&nbsp;   break; // Stop when model generates text

&nbsp; }



&nbsp; step++;

}

```

This manual approach gives you complete control over:

\- Message history management

\- Step-by-step decision making

\- Custom stopping conditions

\- Dynamic tool and model selection

\- Error handling and recovery

\[Learn more about manual agent loops in the cookbook](/cookbook/node/manual-agent-loop).

---

title: Configuring Call Options

description: Pass type-safe runtime inputs to dynamically configure agent behavior.

---

\# Configuring Call Options

Call options allow you to pass type-safe structured inputs to your agent. Use them to dynamically modify any agent setting based on the specific request.

\## Why Use Call Options?

When you need agent behavior to change based on runtime context:

\- \*\*Add dynamic context\*\* - Inject retrieved documents, user preferences, or session data into prompts

\- \*\*Select models dynamically\*\* - Choose faster or more capable models based on request complexity

\- \*\*Configure tools per request\*\* - Pass user location to search tools or adjust tool behavior

\- \*\*Customize provider options\*\* - Set reasoning effort, temperature, or other provider-specific settings

Without call options, you'd need to create multiple agents or handle configuration logic outside the agent.

\## How It Works

Define call options in three steps:

1\. \*\*Define the schema\*\* - Specify what inputs you accept using `callOptionsSchema`

2\. \*\*Configure with `prepareCall`\*\* - Use those inputs to modify agent settings

3\. \*\*Pass options at runtime\*\* - Provide the options when calling `generate()` or `stream()`

\## Basic Example

Add user context to your agent's prompt at runtime:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const supportAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; callOptionsSchema: z.object({

&nbsp;   userId: z.string(),

&nbsp;   accountType: z.enum(\['free', 'pro', 'enterprise']),

&nbsp; }),

&nbsp; instructions: 'You are a helpful customer support agent.',

&nbsp; prepareCall: ({ options, ...settings }) => ({

&nbsp;   ...settings,

&nbsp;   instructions:

&nbsp;     settings.instructions +

&nbsp;     `\\nUser context:

\- Account type: ${options.accountType}

\- User ID: ${options.userId}



Adjust your response based on the user's account level.`,

&nbsp; }),

});



// Call the agent with specific user context

const result = await supportAgent.generate({

&nbsp; prompt: 'How do I upgrade my account?',

&nbsp; options: {

&nbsp;   userId: 'user\_123',

&nbsp;   accountType: 'free',

&nbsp; },

});

```

The `options` parameter is now required and type-checked. If you don't provide it or pass incorrect types, TypeScript will error.

\## Modifying Agent Settings

Use `prepareCall` to modify any agent setting. Return only the settings you want to change.

\### Dynamic Model Selection

Choose models based on request characteristics:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_, // Default model

&nbsp; callOptionsSchema: z.object({

&nbsp;   complexity: z.enum(\['simple', 'complex']),

&nbsp; }),

&nbsp; prepareCall: ({ options, ...settings }) => ({

&nbsp;   ...settings,

&nbsp;   model:

&nbsp;     options.complexity === 'simple' ? 'openai/gpt-4o-mini' : 'openai/o1-mini',

&nbsp; }),

});



// Use faster model for simple queries

await agent.generate({

&nbsp; prompt: 'What is 2+2?',

&nbsp; options: { complexity: 'simple' },

});



// Use more capable model for complex reasoning

await agent.generate({

&nbsp; prompt: 'Explain quantum entanglement',

&nbsp; options: { complexity: 'complex' },

});

```

\### Dynamic Tool Configuration

Configure tools based on runtime context:

```ts

import { openai } from '@ai-sdk/openai';

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const newsAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; callOptionsSchema: z.object({

&nbsp;   userCity: z.string().optional(),

&nbsp;   userRegion: z.string().optional(),

&nbsp; }),

&nbsp; tools: {

&nbsp;   web\_search: openai.tools.webSearch(),

&nbsp; },

&nbsp; prepareCall: ({ options, ...settings }) => ({

&nbsp;   ...settings,

&nbsp;   tools: {

&nbsp;     web\_search: openai.tools.webSearch({

&nbsp;       searchContextSize: 'low',

&nbsp;       userLocation: {

&nbsp;         type: 'approximate',

&nbsp;         city: options.userCity,

&nbsp;         region: options.userRegion,

&nbsp;         country: 'US',

&nbsp;       },

&nbsp;     }),

&nbsp;   },

&nbsp; }),

});



await newsAgent.generate({

&nbsp; prompt: 'What are the top local news stories?',

&nbsp; options: {

&nbsp;   userCity: 'San Francisco',

&nbsp;   userRegion: 'California',

&nbsp; },

});

```

\### Provider-Specific Options

Configure provider settings dynamically:

```ts

import { openai, OpenAIProviderOptions } from '@ai-sdk/openai';

import { ToolLoopAgent } from 'ai';

import { z } from 'zod';



const agent = new ToolLoopAgent({

&nbsp; model: 'openai/o3',

&nbsp; callOptionsSchema: z.object({

&nbsp;   taskDifficulty: z.enum(\['low', 'medium', 'high']),

&nbsp; }),

&nbsp; prepareCall: ({ options, ...settings }) => ({

&nbsp;   ...settings,

&nbsp;   providerOptions: {

&nbsp;     openai: {

&nbsp;       reasoningEffort: options.taskDifficulty,

&nbsp;     } satisfies OpenAIProviderOptions,

&nbsp;   },

&nbsp; }),

});



await agent.generate({

&nbsp; prompt: 'Analyze this complex scenario...',

&nbsp; options: { taskDifficulty: 'high' },

});

```

\## Advanced Patterns

\### Retrieval Augmented Generation (RAG)

Fetch relevant context and inject it into your prompt:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const ragAgent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; callOptionsSchema: z.object({

&nbsp;   query: z.string(),

&nbsp; }),

&nbsp; prepareCall: async ({ options, ...settings }) => {

&nbsp;   // Fetch relevant documents (this can be async)

&nbsp;   const documents = await vectorSearch(options.query);



&nbsp;   return {

&nbsp;     ...settings,

&nbsp;     instructions: `Answer questions using the following context:



${documents.map(doc => doc.content).join('\\n\\n')}`,

&nbsp;   };

&nbsp; },

});



await ragAgent.generate({

&nbsp; prompt: 'What is our refund policy?',

&nbsp; options: { query: 'refund policy' },

});

```

The `prepareCall` function can be async, enabling you to fetch data before configuring the agent.

\### Combining Multiple Modifications

Modify multiple settings together:

```ts

import { ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; callOptionsSchema: z.object({

&nbsp;   userRole: z.enum(\['admin', 'user']),

&nbsp;   urgency: z.enum(\['low', 'high']),

&nbsp; }),

&nbsp; tools: {

&nbsp;   readDatabase: readDatabaseTool,

&nbsp;   writeDatabase: writeDatabaseTool,

&nbsp; },

&nbsp; prepareCall: ({ options, ...settings }) => ({

&nbsp;   ...settings,

&nbsp;   // Upgrade model for urgent requests

&nbsp;   model: options.urgency === 'high' ? \_\_MODEL\_\_ : settings.model,

&nbsp;   // Limit tools based on user role

&nbsp;   activeTools:

&nbsp;     options.userRole === 'admin'

&nbsp;       ? \['readDatabase', 'writeDatabase']

&nbsp;       : \['readDatabase'],

&nbsp;   // Adjust instructions

&nbsp;   instructions: `You are a ${options.userRole} assistant.

${options.userRole === 'admin' ? 'You have full database access.' : 'You have read-only access.'}`,

&nbsp; }),

});



await agent.generate({

&nbsp; prompt: 'Update the user record',

&nbsp; options: {

&nbsp;   userRole: 'admin',

&nbsp;   urgency: 'high',

&nbsp; },

});

```

\## Using with createAgentUIStreamResponse

Pass call options through API routes to your agent:

```ts filename="app/api/chat/route.ts"

import { createAgentUIStreamResponse } from 'ai';

import { myAgent } from '@/ai/agents/my-agent';



export async function POST(request: Request) {

&nbsp; const { messages, userId, accountType } = await request.json();



&nbsp; return createAgentUIStreamResponse({

&nbsp;   agent: myAgent,

&nbsp;   messages,

&nbsp;   options: {

&nbsp;     userId,

&nbsp;     accountType,

&nbsp;   },

&nbsp; });

}

```

\## Next Steps

\- Learn about \[loop control](/docs/agents/loop-control) for execution management

\- Explore \[workflow patterns](/docs/agents/workflows) for complex multi-step processes

---

title: Agents

description: An overview of building agents with the AI SDK.

---

\# Agents

The following section show you how to build agents with the AI SDK - systems where large language models (LLMs) use tools in a loop to accomplish tasks.

<IndexCards

&nbsp; cards={\[

&nbsp; {

&nbsp; title: 'Overview',

&nbsp; description: 'Learn what agents are and why to use the Agent class.',

&nbsp; href: '/docs/agents/overview',

&nbsp; },

&nbsp; {

&nbsp; title: 'Building Agents',

&nbsp; description: 'Complete guide to creating agents with the Agent class.',

&nbsp; href: '/docs/agents/building-agents',

&nbsp; },

&nbsp; {

&nbsp; title: 'Workflow Patterns',

&nbsp; description:

&nbsp; 'Structured patterns using core functions for complex workflows.',

&nbsp; href: '/docs/agents/workflows',

&nbsp; },

&nbsp; {

&nbsp; title: 'Loop Control',

&nbsp; description: 'Advanced execution control with stopWhen and prepareStep.',

&nbsp; href: '/docs/agents/loop-control',

&nbsp; },

&nbsp; {

&nbsp; title: 'Configuring Call Options',

&nbsp; description:

&nbsp; 'Pass type-safe runtime inputs to dynamically configure agent behavior.',

&nbsp; href: '/docs/agents/configuring-call-options',

&nbsp; },

&nbsp; ]}

/>

---

title: Overview

description: An overview of AI SDK Core.

---

\# AI SDK Core

Large Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale.

They are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.

AI SDK Core \*\*simplifies working with LLMs by offering a standardized way of integrating them into your app\*\* - so you can focus on building great AI applications for your users, not waste time on technical details.

For example, here’s how you can generate text with various models using the AI SDK:

<PreviewSwitchProviders />

\## AI SDK Core Functions

AI SDK Core has various functions designed for \[text generation](./generating-text), \[structured data generation](./generating-structured-data), and \[tool usage](./tools-and-tool-calling).

These functions take a standardized approach to setting up \[prompts](./prompts) and \[settings](./settings), making it easier to work with different models.

\- \[`generateText`](/docs/ai-sdk-core/generating-text): Generates text and \[tool calls](./tools-and-tool-calling).

&nbsp; This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.

\- \[`streamText`](/docs/ai-sdk-core/generating-text): Stream text and tool calls.

&nbsp; You can use the `streamText` function for interactive use cases such as \[chat bots](/docs/ai-sdk-ui/chatbot) and \[content streaming](/docs/ai-sdk-ui/completion).

\- \[`generateObject`](/docs/ai-sdk-core/generating-structured-data): Generates a typed, structured object that matches a \[Zod](https://zod.dev/) schema.

&nbsp; You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.

\- \[`streamObject`](/docs/ai-sdk-core/generating-structured-data): Stream a structured object that matches a Zod schema.

&nbsp; You can use this function to \[stream generated UIs](/docs/ai-sdk-ui/object-generation).

\## API Reference

Please check out the \[AI SDK Core API Reference](/docs/reference/ai-sdk-core) for more details on each function.

---

title: Generating Text

description: Learn how to generate text with the AI SDK.

---

\# Generating and Streaming Text

Large language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process.

For example, you can ask a model to come up with a recipe, draft an email, or summarize a document.

The AI SDK Core provides two functions to generate text and stream it from LLMs:

\- \[`generateText`](#generatetext): Generates text for a given prompt and model.

\- \[`streamText`](#streamtext): Streams text from a given prompt and model.

Advanced LLM features such as \[tool calling](./tools-and-tool-calling) and \[structured data generation](./generating-structured-data) are built on top of text generation.

\## `generateText`

You can generate text using the \[`generateText`](/docs/reference/ai-sdk-core/generate-text) function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.

```tsx

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { text } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

You can use more \[advanced prompts](./prompts) to generate text with more complex instructions and content:

```tsx

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { text } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; system:

&nbsp;   'You are a professional writer. ' +

&nbsp;   'You write simple, clear, and concise content.',

&nbsp; prompt: `Summarize the following article in 3-5 sentences: ${article}`,

});

```

The result object of `generateText` contains several promises that resolve when all required data is available:

\- `result.content`: The content that was generated in the last step.

\- `result.text`: The generated text.

\- `result.reasoning`: The full reasoning that the model has generated in the last step.

\- `result.reasoningText`: The reasoning text of the model (only available for some models).

\- `result.files`: The files that were generated in the last step.

\- `result.sources`: Sources that have been used as references in the last step (only available for some models).

\- `result.toolCalls`: The tool calls that were made in the last step.

\- `result.toolResults`: The results of the tool calls from the last step.

\- `result.finishReason`: The reason the model finished generating text.

\- `result.rawFinishReason`: The raw reason why the generation finished (from the provider).

\- `result.usage`: The usage of the model during the final step of text generation.

\- `result.totalUsage`: The total usage across all steps (for multi-step generations).

\- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).

\- `result.request`: Additional request information.

\- `result.response`: Additional response information, including response messages and body.

\- `result.providerMetadata`: Additional provider-specific metadata.

\- `result.steps`: Details for all steps, useful for getting information about intermediate steps.

\- `result.output`: The generated structured output using the `output` specification.

\### Accessing response headers \& body

Sometimes you need access to the full response from the model provider,

e.g. to access some provider-specific headers or body content.

You can access the raw response headers and body using the `response` property:

```ts

import { generateText } from 'ai';



const result = await generateText({

&nbsp; // ...

});



console.log(JSON.stringify(result.response.headers, null, 2));

console.log(JSON.stringify(result.response.body, null, 2));

```

\### `onFinish` callback

When using `generateText`, you can provide an `onFinish` callback that is triggered after the last step is finished (

\[API Reference](/docs/reference/ai-sdk-core/generate-text#on-finish)

).

It contains the text, usage information, finish reason, messages, steps, total usage, and more:

```tsx highlight="6-8"

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; onFinish({ text, finishReason, usage, response, steps, totalUsage }) {

&nbsp;   // your own logic, e.g. for saving the chat history or recording usage



&nbsp;   const messages = response.messages; // messages that were generated

&nbsp; },

});

```

\## `streamText`

Depending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.

AI SDK Core provides the \[`streamText`](/docs/reference/ai-sdk-core/stream-text) function which simplifies streaming text from LLMs:

```ts

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

});



// example: use textStream as an async iterable

for await (const textPart of result.textStream) {

&nbsp; console.log(textPart);

}

```

<Note>

&nbsp; `result.textStream` is both a `ReadableStream` and an `AsyncIterable`.

</Note>

<Note type="warning">

&nbsp; `streamText` immediately starts streaming and suppresses errors to prevent

&nbsp; server crashes. Use the `onError` callback to log errors.

</Note>

You can use `streamText` on its own or in combination with \[AI SDK

UI](/examples/next-pages/basics/streaming-text-generation) and \[AI SDK

RSC](/examples/next-app/basics/streaming-text-generation).

The result object contains several helper functions to make the integration into \[AI SDK UI](/docs/ai-sdk-ui) easier:

\- `result.toUIMessageStreamResponse()`: Creates a UI Message stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.

\- `result.pipeUIMessageStreamToResponse()`: Writes UI Message stream delta output to a Node.js response-like object.

\- `result.toTextStreamResponse()`: Creates a simple text stream HTTP response.

\- `result.pipeTextStreamToResponse()`: Writes text delta output to a Node.js response-like object.

<Note>

&nbsp; `streamText` is using backpressure and only generates tokens as they are

&nbsp; requested. You need to consume the stream in order for it to finish.

</Note>

It also provides several promises that resolve when the stream is finished:

\- `result.content`: The content that was generated in the last step.

\- `result.text`: The generated text.

\- `result.reasoning`: The full reasoning that the model has generated.

\- `result.reasoningText`: The reasoning text of the model (only available for some models).

\- `result.files`: Files that have been generated by the model in the last step.

\- `result.sources`: Sources that have been used as references in the last step (only available for some models).

\- `result.toolCalls`: The tool calls that have been executed in the last step.

\- `result.toolResults`: The tool results that have been generated in the last step.

\- `result.finishReason`: The reason the model finished generating text.

\- `result.rawFinishReason`: The raw reason why the generation finished (from the provider).

\- `result.usage`: The usage of the model during the final step of text generation.

\- `result.totalUsage`: The total usage across all steps (for multi-step generations).

\- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).

\- `result.steps`: Details for all steps, useful for getting information about intermediate steps.

\- `result.request`: Additional request information from the last step.

\- `result.response`: Additional response information from the last step.

\- `result.providerMetadata`: Additional provider-specific metadata from the last step.

\### `onError` callback

`streamText` immediately starts streaming to enable sending data without waiting for the model.

Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.

To log errors, you can provide an `onError` callback that is triggered when an error occurs.

```tsx highlight="6-8"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; onError({ error }) {

&nbsp;   console.error(error); // your error logging logic here

&nbsp; },

});

```

\### `onChunk` callback

When using `streamText`, you can provide an `onChunk` callback that is triggered for each chunk of the stream.

It receives the following chunk types:

\- `text`

\- `reasoning`

\- `source`

\- `tool-call`

\- `tool-input-start`

\- `tool-input-delta`

\- `tool-result`

\- `raw`

```tsx highlight="6-11"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; onChunk({ chunk }) {

&nbsp;   // implement your own logic here, e.g.:

&nbsp;   if (chunk.type === 'text') {

&nbsp;     console.log(chunk.text);

&nbsp;   }

&nbsp; },

});

```

\### `onFinish` callback

When using `streamText`, you can provide an `onFinish` callback that is triggered when the stream is finished (

\[API Reference](/docs/reference/ai-sdk-core/stream-text#on-finish)

).

It contains the text, usage information, finish reason, messages, steps, total usage, and more:

```tsx highlight="6-8"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; onFinish({ text, finishReason, usage, response, steps, totalUsage }) {

&nbsp;   // your own logic, e.g. for saving the chat history or recording usage



&nbsp;   const messages = response.messages; // messages that were generated

&nbsp; },

});

```

\### `fullStream` property

You can read a stream with all events using the `fullStream` property.

This can be useful if you want to implement your own UI or handle the stream in a different way.

Here is an example of how to use the `fullStream` property:

```tsx

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   cityAttractions: {

&nbsp;     inputSchema: z.object({ city: z.string() }),

&nbsp;     execute: async ({ city }) => ({

&nbsp;       attractions: \['attraction1', 'attraction2', 'attraction3'],

&nbsp;     }),

&nbsp;   },

&nbsp; },

&nbsp; prompt: 'What are some San Francisco tourist attractions?',

});



for await (const part of result.fullStream) {

&nbsp; switch (part.type) {

&nbsp;   case 'start': {

&nbsp;     // handle start of stream

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'start-step': {

&nbsp;     // handle start of step

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'text-start': {

&nbsp;     // handle text start

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'text-delta': {

&nbsp;     // handle text delta here

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'text-end': {

&nbsp;     // handle text end

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'reasoning-start': {

&nbsp;     // handle reasoning start

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'reasoning-delta': {

&nbsp;     // handle reasoning delta here

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'reasoning-end': {

&nbsp;     // handle reasoning end

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'source': {

&nbsp;     // handle source here

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'file': {

&nbsp;     // handle file here

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'tool-call': {

&nbsp;     switch (part.toolName) {

&nbsp;       case 'cityAttractions': {

&nbsp;         // handle tool call here

&nbsp;         break;

&nbsp;       }

&nbsp;     }

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'tool-input-start': {

&nbsp;     // handle tool input start

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'tool-input-delta': {

&nbsp;     // handle tool input delta

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'tool-input-end': {

&nbsp;     // handle tool input end

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'tool-result': {

&nbsp;     switch (part.toolName) {

&nbsp;       case 'cityAttractions': {

&nbsp;         // handle tool result here

&nbsp;         break;

&nbsp;       }

&nbsp;     }

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'tool-error': {

&nbsp;     // handle tool error

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'finish-step': {

&nbsp;     // handle finish step

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'finish': {

&nbsp;     // handle finish here

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'error': {

&nbsp;     // handle error here

&nbsp;     break;

&nbsp;   }

&nbsp;   case 'raw': {

&nbsp;     // handle raw value

&nbsp;     break;

&nbsp;   }

&nbsp; }

}

```

\### Stream transformation

You can use the `experimental\_transform` option to transform the stream.

This is useful for e.g. filtering, changing, or smoothing the text stream.

The transformations are applied before the callbacks are invoked and the promises are resolved.

If you e.g. have a transformation that changes all text to uppercase, the `onFinish` callback will receive the transformed text.

\#### Smoothing streams

The AI SDK Core provides a \[`smoothStream` function](/docs/reference/ai-sdk-core/smooth-stream) that

can be used to smooth out text and reasoning streaming.

```tsx highlight="6"

import { smoothStream, streamText } from 'ai';



const result = streamText({

&nbsp; model,

&nbsp; prompt,

&nbsp; experimental\_transform: smoothStream(),

});

```

\#### Custom transformations

You can also implement your own custom transformations.

The transformation function receives the tools that are available to the model,

and returns a function that is used to transform the stream.

Tools can either be generic or limited to the tools that you are using.

Here is an example of how to implement a custom transformation that converts

all text to uppercase:

```ts

const upperCaseTransform =

&nbsp; <TOOLS extends ToolSet>() =>

&nbsp; (options: { tools: TOOLS; stopStream: () => void }) =>

&nbsp;   new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({

&nbsp;     transform(chunk, controller) {

&nbsp;       controller.enqueue(

&nbsp;         // for text chunks, convert the text to uppercase:

&nbsp;         chunk.type === 'text'

&nbsp;           ? { ...chunk, text: chunk.text.toUpperCase() }

&nbsp;           : chunk,

&nbsp;       );

&nbsp;     },

&nbsp;   });

```

You can also stop the stream using the `stopStream` function.

This is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.

When you invoke `stopStream`, it is important to simulate the `step-finish` and `finish` events to guarantee that a well-formed stream is returned

and all callbacks are invoked.

```ts

const stopWordTransform =

&nbsp; <TOOLS extends ToolSet>() =>

&nbsp; ({ stopStream }: { stopStream: () => void }) =>

&nbsp;   new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({

&nbsp;     // note: this is a simplified transformation for testing;

&nbsp;     // in a real-world version more there would need to be

&nbsp;     // stream buffering and scanning to correctly emit prior text

&nbsp;     // and to detect all STOP occurrences.

&nbsp;     transform(chunk, controller) {

&nbsp;       if (chunk.type !== 'text') {

&nbsp;         controller.enqueue(chunk);

&nbsp;         return;

&nbsp;       }



&nbsp;       if (chunk.text.includes('STOP')) {

&nbsp;         // stop the stream

&nbsp;         stopStream();



&nbsp;         // simulate the finish-step event

&nbsp;         controller.enqueue({

&nbsp;           type: 'finish-step',

&nbsp;           finishReason: 'stop',

&nbsp;           logprobs: undefined,

&nbsp;           usage: {

&nbsp;             completionTokens: NaN,

&nbsp;             promptTokens: NaN,

&nbsp;             totalTokens: NaN,

&nbsp;           },

&nbsp;           request: {},

&nbsp;           response: {

&nbsp;             id: 'response-id',

&nbsp;             modelId: 'mock-model-id',

&nbsp;             timestamp: new Date(0),

&nbsp;           },

&nbsp;           warnings: \[],

&nbsp;           isContinued: false,

&nbsp;         });



&nbsp;         // simulate the finish event

&nbsp;         controller.enqueue({

&nbsp;           type: 'finish',

&nbsp;           finishReason: 'stop',

&nbsp;           logprobs: undefined,

&nbsp;           usage: {

&nbsp;             completionTokens: NaN,

&nbsp;             promptTokens: NaN,

&nbsp;             totalTokens: NaN,

&nbsp;           },

&nbsp;           response: {

&nbsp;             id: 'response-id',

&nbsp;             modelId: 'mock-model-id',

&nbsp;             timestamp: new Date(0),

&nbsp;           },

&nbsp;         });



&nbsp;         return;

&nbsp;       }



&nbsp;       controller.enqueue(chunk);

&nbsp;     },

&nbsp;   });

```

\#### Multiple transformations

You can also provide multiple transformations. They are applied in the order they are provided.

```tsx highlight="4"

const result = streamText({

&nbsp; model,

&nbsp; prompt,

&nbsp; experimental\_transform: \[firstTransform, secondTransform],

});

```

\## Sources

Some providers such as \[Perplexity](/providers/ai-sdk-providers/perplexity#sources) and

\[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.

Currently sources are limited to web pages that ground the response.

You can access them using the `sources` property of the result.

Each `url` source contains the following properties:

\- `id`: The ID of the source.

\- `url`: The URL of the source.

\- `title`: The optional title of the source.

\- `providerMetadata`: Provider metadata for the source.

When you use `generateText`, you can access the sources using the `sources` property:

```ts

const result = await generateText({

&nbsp; model: 'google/gemini-2.5-flash',

&nbsp; tools: {

&nbsp;   google\_search: google.tools.googleSearch({}),

&nbsp; },

&nbsp; prompt: 'List the top 5 San Francisco news from the past week.',

});



for (const source of result.sources) {

&nbsp; if (source.sourceType === 'url') {

&nbsp;   console.log('ID:', source.id);

&nbsp;   console.log('Title:', source.title);

&nbsp;   console.log('URL:', source.url);

&nbsp;   console.log('Provider metadata:', source.providerMetadata);

&nbsp;   console.log();

&nbsp; }

}

```

When you use `streamText`, you can access the sources using the `fullStream` property:

```tsx

const result = streamText({

&nbsp; model: 'google/gemini-2.5-flash',

&nbsp; tools: {

&nbsp;   google\_search: google.tools.googleSearch({}),

&nbsp; },

&nbsp; prompt: 'List the top 5 San Francisco news from the past week.',

});



for await (const part of result.fullStream) {

&nbsp; if (part.type === 'source' \&\& part.sourceType === 'url') {

&nbsp;   console.log('ID:', part.id);

&nbsp;   console.log('Title:', part.title);

&nbsp;   console.log('URL:', part.url);

&nbsp;   console.log('Provider metadata:', part.providerMetadata);

&nbsp;   console.log();

&nbsp; }

}

```

The sources are also available in the `result.sources` promise.

\## Examples

You can see `generateText` and `streamText` in action using various frameworks in the following examples:

\### `generateText`

<ExampleLinks

&nbsp; examples={\[

&nbsp; {

&nbsp; title: 'Learn to generate text in Node.js',

&nbsp; link: '/examples/node/generating-text/generate-text',

&nbsp; },

&nbsp; {

&nbsp; title:

&nbsp; 'Learn to generate text in Next.js with Route Handlers (AI SDK UI)',

&nbsp; link: '/examples/next-pages/basics/generating-text',

&nbsp; },

&nbsp; {

&nbsp; title:

&nbsp; 'Learn to generate text in Next.js with Server Actions (AI SDK RSC)',

&nbsp; link: '/examples/next-app/basics/generating-text',

&nbsp; },

&nbsp; ]}

/>

\### `streamText`

<ExampleLinks

&nbsp; examples={\[

&nbsp; {

&nbsp; title: 'Learn to stream text in Node.js',

&nbsp; link: '/examples/node/generating-text/stream-text',

&nbsp; },

&nbsp; {

&nbsp; title: 'Learn to stream text in Next.js with Route Handlers (AI SDK UI)',

&nbsp; link: '/examples/next-pages/basics/streaming-text-generation',

&nbsp; },

&nbsp; {

&nbsp; title: 'Learn to stream text in Next.js with Server Actions (AI SDK RSC)',

&nbsp; link: '/examples/next-app/basics/streaming-text-generation',

&nbsp; },

&nbsp; ]}

/>

---

title: Generating Structured Data

description: Learn how to generate structured data with the AI SDK.

---

\# Generating Structured Data

While text generation can be useful, your use case will likely call for generating structured data.

For example, you might want to extract information from text, classify data, or generate synthetic data.

Many language models are capable of generating structured data, often defined as using "JSON modes" or "tools".

However, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.

The AI SDK standardises structured object generation across model providers

using the `output` property on \[`generateText`](/docs/reference/ai-sdk-core/generate-text)

and \[`streamText`](/docs/reference/ai-sdk-core/stream-text).

You can use \[Zod schemas](/docs/reference/ai-sdk-core/zod-schema), \[Valibot](/docs/reference/ai-sdk-core/valibot-schema), or \[JSON schemas](/docs/reference/ai-sdk-core/json-schema) to specify the shape of the data that you want,

and the AI model will generate data that conforms to that structure.

<Note>

&nbsp; Structured output generation is part of the `generateText` and `streamText`

&nbsp; flow. This means you can combine it with tool calling in the same request.

</Note>

\## Generating Structured Outputs

Use `generateText` with `Output.object()` to generate structured data from a prompt.

The schema is also used to validate the generated data, ensuring type safety and correctness.

```ts

import { generateText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { output } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     recipe: z.object({

&nbsp;       name: z.string(),

&nbsp;       ingredients: z.array(

&nbsp;         z.object({ name: z.string(), amount: z.string() }),

&nbsp;       ),

&nbsp;       steps: z.array(z.string()),

&nbsp;     }),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

<Note>

&nbsp; Structured output generation counts as a step in the AI SDK's multi-turn

&nbsp; execution model (where each model call or tool execution is one step). When

&nbsp; combining with tools, account for this in your `stopWhen` configuration.

</Note>

\### Accessing response headers \& body

Sometimes you need access to the full response from the model provider,

e.g. to access some provider-specific headers or body content.

You can access the raw response headers and body using the `response` property:

```ts

import { generateText, Output } from 'ai';



const result = await generateText({

&nbsp; // ...

&nbsp; output: Output.object({ schema }),

});



console.log(JSON.stringify(result.response.headers, null, 2));

console.log(JSON.stringify(result.response.body, null, 2));

```

\## Stream Structured Outputs

Given the added complexity of returning structured data, model response time can be unacceptable for your interactive use case.

With `streamText` and `output`, you can stream the model's structured response as it is generated.

```ts

import { streamText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { partialOutputStream } = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     recipe: z.object({

&nbsp;       name: z.string(),

&nbsp;       ingredients: z.array(

&nbsp;         z.object({ name: z.string(), amount: z.string() }),

&nbsp;       ),

&nbsp;       steps: z.array(z.string()),

&nbsp;     }),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});



// use partialOutputStream as an async iterable

for await (const partialObject of partialOutputStream) {

&nbsp; console.log(partialObject);

}

```

You can consume the structured output on the client with the \[`useObject`](/docs/reference/ai-sdk-ui/use-object) hook.

\### Error Handling in Streams

`streamText` starts streaming immediately. When errors occur during streaming, they become part of the stream rather than thrown exceptions (to prevent stream crashes).

To handle errors, provide an `onError` callback:

```tsx highlight="5-7"

import { streamText, Output } from 'ai';



const result = streamText({

&nbsp; // ...

&nbsp; output: Output.object({ schema }),

&nbsp; onError({ error }) {

&nbsp;   console.error(error); // log to your error tracking service

&nbsp; },

});

```

For non-streaming error handling with `generateText`, see the \[Error Handling](#error-handling) section below.

\## Output Types

The AI SDK supports multiple ways of specifying the expected structure of generated data via the `Output` object. You can select from various strategies for structured/text generation and validation.

\### `Output.text()`

Use `Output.text()` to generate plain text from a model. This option doesn't enforce any schema on the result: you simply receive the model's text as a string. This is the default behavior when no `output` is specified.

```ts

import { generateText, Output } from 'ai';



const { output } = await generateText({

&nbsp; // ...

&nbsp; output: Output.text(),

&nbsp; prompt: 'Tell me a joke.',

});

// output will be a string (the joke)

```

\### `Output.object()`

Use `Output.object({ schema })` to generate a structured object based on a schema (for example, a Zod schema). The output is type-validated to ensure the returned result matches the schema.

```ts

import { generateText, Output } from 'ai';

import { z } from 'zod';



const { output } = await generateText({

&nbsp; // ...

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     name: z.string(),

&nbsp;     age: z.number().nullable(),

&nbsp;     labels: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate information for a test user.',

});

// output will be an object matching the schema above

```

<Note>

&nbsp; Partial outputs streamed via `streamText` cannot be validated against your

&nbsp; provided schema, as incomplete data may not yet conform to the expected

&nbsp; structure.

</Note>

\### `Output.array()`

Use `Output.array({ element })` to specify that you expect an array of typed objects from the model, where each element should conform to a schema (defined in the `element` property).

```ts

import { generateText, Output } from 'ai';

import { z } from 'zod';



const { output } = await generateText({

&nbsp; // ...

&nbsp; output: Output.array({

&nbsp;   element: z.object({

&nbsp;     location: z.string(),

&nbsp;     temperature: z.number(),

&nbsp;     condition: z.string(),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'List the weather for San Francisco and Paris.',

});

// output will be an array of objects like:

// \[

//   { location: 'San Francisco', temperature: 70, condition: 'Sunny' },

//   { location: 'Paris', temperature: 65, condition: 'Cloudy' },

// ]

```

When streaming arrays with `streamText`, you can use `elementStream` to receive each completed element as it is generated:

```ts

import { streamText, Output } from 'ai';

import { z } from 'zod';



const { elementStream } = streamText({

&nbsp; // ...

&nbsp; output: Output.array({

&nbsp;   element: z.object({

&nbsp;     name: z.string(),

&nbsp;     class: z.string(),

&nbsp;     description: z.string(),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',

});



for await (const hero of elementStream) {

&nbsp; console.log(hero); // Each hero is complete and validated

}

```

<Note>

&nbsp; Each element emitted by `elementStream` is complete and validated against your

&nbsp; element schema. This differs from `partialOutputStream`, which streams the

&nbsp; entire partial array including incomplete elements.

</Note>

\### `Output.choice()`

Use `Output.choice({ options })` when you expect the model to choose from a specific set of string options, such as for classification or fixed-enum answers.

```ts

import { generateText, Output } from 'ai';



const { output } = await generateText({

&nbsp; // ...

&nbsp; output: Output.choice({

&nbsp;   options: \['sunny', 'rainy', 'snowy'],

&nbsp; }),

&nbsp; prompt: 'Is the weather sunny, rainy, or snowy today?',

});

// output will be one of: 'sunny', 'rainy', or 'snowy'

```

You can provide any set of string options, and the output will always be a single string value that matches one of the specified options. The AI SDK validates that the result matches one of your options, and will throw if the model returns something invalid.

This is especially useful for making classification-style generations or forcing valid values for API compatibility.

\### `Output.json()`

Use `Output.json()` when you want to generate and parse unstructured JSON values from the model, without enforcing a specific schema. This is useful if you want to capture arbitrary objects, flexible structures, or when you want to rely on the model's natural output rather than rigid validation.

```ts

import { generateText, Output } from 'ai';



const { output } = await generateText({

&nbsp; // ...

&nbsp; output: Output.json(),

&nbsp; prompt:

&nbsp;   'For each city, return the current temperature and weather condition as a JSON object.',

});



// output could be any valid JSON, for example:

// {

//   "San Francisco": { "temperature": 70, "condition": "Sunny" },

//   "Paris": { "temperature": 65, "condition": "Cloudy" }

// }

```

With `Output.json`, the AI SDK only checks that the response is valid JSON; it doesn't validate the structure or types of the values. If you need schema validation, use the `.object` or `.array` outputs instead.

For more advanced validation or different structures, see \[the Output API reference](/docs/reference/ai-sdk-core/output).

\## Generating Structured Outputs with Tools

One of the key advantages of using structured output with `generateText` and `streamText` is the ability to combine it with tool calling.

```ts

import { generateText, Output, tool, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { output } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather for a location',

&nbsp;     inputSchema: z.object({ location: z.string() }),

&nbsp;     execute: async ({ location }) => {

&nbsp;       // fetch weather data

&nbsp;       return { temperature: 72, condition: 'sunny' };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     summary: z.string(),

&nbsp;     recommendation: z.string(),

&nbsp;   }),

&nbsp; }),

&nbsp; stopWhen: stepCountIs(5),

&nbsp; prompt: 'What should I wear in San Francisco today?',

});

```

<Note>

&nbsp; When using tools with structured output, remember that generating the

&nbsp; structured output counts as a step. Configure `stopWhen` to allow enough steps

&nbsp; for both tool execution and output generation.

</Note>

\## Property Descriptions

You can add `.describe("...")` to individual schema properties to give the model hints about what each property is for. This helps improve the quality and accuracy of generated structured data:

```ts highlight="5,9"

import { generateText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { output } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     name: z.string().describe('The name of the recipe'),

&nbsp;     ingredients: z

&nbsp;       .array(

&nbsp;         z.object({

&nbsp;           name: z.string(),

&nbsp;           amount: z

&nbsp;             .string()

&nbsp;             .describe('The amount of the ingredient (grams or ml)'),

&nbsp;         }),

&nbsp;       )

&nbsp;       .describe('List of ingredients with amounts'),

&nbsp;     steps: z.array(z.string()).describe('Step-by-step cooking instructions'),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

Property descriptions are particularly useful for:

\- Clarifying ambiguous property names

\- Specifying expected formats or conventions

\- Providing context for complex nested structures

\## Output Name and Description

You can optionally specify a `name` and `description` for the output. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.

```ts highlight="6-7"

import { generateText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { output } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: Output.object({

&nbsp;   name: 'Recipe',

&nbsp;   description: 'A recipe for a dish.',

&nbsp;   schema: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

This works with all output types that support structured generation:

\- `Output.object({ name, description, schema })`

\- `Output.array({ name, description, element })`

\- `Output.choice({ name, description, options })`

\- `Output.json({ name, description })`

\## Accessing Reasoning

You can access the reasoning used by the language model to generate the object via the `reasoning` property on the result. This property contains a string with the model's thought process, if available.

```ts

import { generateText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const result = await generateText({

&nbsp; model: \_\_MODEL\_\_, // must be a reasoning model

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     recipe: z.object({

&nbsp;       name: z.string(),

&nbsp;       ingredients: z.array(

&nbsp;         z.object({

&nbsp;           name: z.string(),

&nbsp;           amount: z.string(),

&nbsp;         }),

&nbsp;       ),

&nbsp;       steps: z.array(z.string()),

&nbsp;     }),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});



console.log(result.reasoning);

```

\## Error Handling

When `generateText` with structured output cannot generate a valid object, it throws a \[`AI\_NoObjectGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-object-generated-error).

This error occurs when the AI provider fails to generate a parsable object that conforms to the schema.

It can arise due to the following reasons:

\- The model failed to generate a response.

\- The model generated a response that could not be parsed.

\- The model generated a response that could not be validated against the schema.

The error preserves the following information to help you log the issue:

\- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.

\- `response`: Metadata about the language model response, including response id, timestamp, and model.

\- `usage`: Request token usage.

\- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.

```ts

import { generateText, Output, NoObjectGeneratedError } from 'ai';



try {

&nbsp; await generateText({

&nbsp;   model,

&nbsp;   output: Output.object({ schema }),

&nbsp;   prompt,

&nbsp; });

} catch (error) {

&nbsp; if (NoObjectGeneratedError.isInstance(error)) {

&nbsp;   console.log('NoObjectGeneratedError');

&nbsp;   console.log('Cause:', error.cause);

&nbsp;   console.log('Text:', error.text);

&nbsp;   console.log('Response:', error.response);

&nbsp;   console.log('Usage:', error.usage);

&nbsp; }

}

```

\## generateObject and streamObject (Legacy)

<Note type="warning">

&nbsp; `generateObject` and `streamObject` are deprecated. Use `generateText` and

&nbsp; `streamText` with the `output` property instead. The legacy functions will be

&nbsp; removed in a future major version.

</Note>

The `generateObject` and `streamObject` functions are the legacy way to generate structured data. They work similarly to `generateText` and `streamText` with `Output.object()`, but as standalone functions.

\### generateObject

```ts

import { generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

\### streamObject

```ts

import { streamObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { partialObjectStream } = streamObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});



for await (const partialObject of partialObjectStream) {

&nbsp; console.log(partialObject);

}

```

\### Schema Name and Description (Legacy)

You can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.

```ts highlight="4-5"

import { generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { object } = await generateObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; schemaName: 'Recipe',

&nbsp; schemaDescription: 'A recipe for a dish.',

&nbsp; schema: z.object({

&nbsp;   name: z.string(),

&nbsp;   ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),

&nbsp;   steps: z.array(z.string()),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

\### Output Strategy (Legacy)

The legacy functions support different output strategies via the `output` parameter:

\#### Array

Generate an array of objects. The schema specifies the shape of an array element.

```ts highlight="7"

import { streamObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const { elementStream } = streamObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: 'array',

&nbsp; schema: z.object({

&nbsp;   name: z.string(),

&nbsp;   class: z

&nbsp;     .string()

&nbsp;     .describe('Character class, e.g. warrior, mage, or thief.'),

&nbsp;   description: z.string(),

&nbsp; }),

&nbsp; prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',

});



for await (const hero of elementStream) {

&nbsp; console.log(hero);

}

```

\#### Enum

Generate a specific enum value for classification tasks.

```ts highlight="5-6"

import { generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { object } = await generateObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: 'enum',

&nbsp; enum: \['action', 'comedy', 'drama', 'horror', 'sci-fi'],

&nbsp; prompt:

&nbsp;   'Classify the genre of this movie plot: ' +

&nbsp;   '"A group of astronauts travel through a wormhole in search of a ' +

&nbsp;   'new habitable planet for humanity."',

});

```

\#### No Schema

Generate unstructured JSON without a schema.

```ts highlight="6"

import { generateObject } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { object } = await generateObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; output: 'no-schema',

&nbsp; prompt: 'Generate a lasagna recipe.',

});

```

\### Repairing Invalid JSON (Legacy)

<Note type="warning">

&nbsp; The `repairText` function is experimental and may change in the future.

</Note>

Sometimes the model will generate invalid or malformed JSON.

You can use the `repairText` function to attempt to repair the JSON.

```ts highlight="7-10"

import { generateObject } from 'ai';



const { object } = await generateObject({

&nbsp; model,

&nbsp; schema,

&nbsp; prompt,

&nbsp; experimental\_repairText: async ({ text, error }) => {

&nbsp;   // example: add a closing brace to the text

&nbsp;   return text + '}';

&nbsp; },

});

```

\## More Examples

You can see `generateObject` and `streamObject` in action using various frameworks in the following examples:

\### `generateObject`

<ExampleLinks

&nbsp; examples={\[

&nbsp; {

&nbsp; title: 'Learn to generate objects in Node.js',

&nbsp; link: '/examples/node/generating-structured-data/generate-object',

&nbsp; },

&nbsp; {

&nbsp; title:

&nbsp; 'Learn to generate objects in Next.js with Route Handlers (AI SDK UI)',

&nbsp; link: '/examples/next-pages/basics/generating-object',

&nbsp; },

&nbsp; {

&nbsp; title:

&nbsp; 'Learn to generate objects in Next.js with Server Actions (AI SDK RSC)',

&nbsp; link: '/examples/next-app/basics/generating-object',

&nbsp; },

&nbsp; ]}

/>

\### `streamText` with Output

<ExampleLinks

&nbsp; examples={\[

&nbsp; {

&nbsp; title: 'Learn to stream objects in Node.js',

&nbsp; link: '/examples/node/streaming-structured-data/stream-object',

&nbsp; },

&nbsp; {

&nbsp; title:

&nbsp; 'Learn to stream objects in Next.js with Route Handlers (AI SDK UI)',

&nbsp; link: '/examples/next-pages/basics/streaming-object-generation',

&nbsp; },

&nbsp; {

&nbsp; title:

&nbsp; 'Learn to stream objects in Next.js with Server Actions (AI SDK RSC)',

&nbsp; link: '/examples/next-app/basics/streaming-object-generation',

&nbsp; },

&nbsp; ]}

/>

---

title: Tool Calling

description: Learn about tool calling and multi-step calls (using stopWhen) with AI SDK Core.

---

\# Tool Calling

As covered under Foundations, \[tools](/docs/foundations/tools) are objects that can be called by the model to perform a specific task.

AI SDK Core tools contain several core elements:

\- \*\*`description`\*\*: An optional description of the tool that can influence when the tool is picked.

\- \*\*`inputSchema`\*\*: A \[Zod schema](/docs/foundations/tools#schemas) or a \[JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the input parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.

\- \*\*`execute`\*\*: An optional async function that is called with the inputs from the tool call. It produces a value of type `RESULT` (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.

\- \*\*`strict`\*\*: \_(optional, boolean)\_ Enables strict tool calling when supported by the provider

<Note className="mb-2">

&nbsp; You can use the \[`tool`](/docs/reference/ai-sdk-core/tool) helper function to

&nbsp; infer the types of the `execute` parameters.

</Note>

The `tools` parameter of `generateText` and `streamText` is an object that has the tool names as keys and the tools as values:

```ts highlight="6-17"

import { z } from 'zod';

import { generateText, tool, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; stopWhen: stepCountIs(5),

&nbsp; prompt: 'What is the weather in San Francisco?',

});

```

<Note>

&nbsp; When a model uses a tool, it is called a "tool call" and the output of the

&nbsp; tool is called a "tool result".

</Note>

Tool calling is not restricted to only text generation.

You can also use it to render user interfaces (Generative UI).

\## Strict Mode

When enabled, language model providers that support strict tool calling will only generate tool calls that are valid according to your defined `inputSchema`.

This increases the reliability of tool calling.

However, not all schemas may be supported in strict mode, and what is supported depends on the specific provider.

By default, strict mode is disabled. You can enable it per-tool by setting `strict: true`:

```ts

tool({

&nbsp; description: 'Get the weather in a location',

&nbsp; inputSchema: z.object({

&nbsp;   location: z.string(),

&nbsp; }),

&nbsp; strict: true, // Enable strict validation for this tool

&nbsp; execute: async ({ location }) => ({

&nbsp;   // ...

&nbsp; }),

});

```

<Note>

&nbsp; Not all providers or models support strict mode. For those that do not, this

&nbsp; option is ignored.

</Note>

\## Input Examples

You can specify example inputs for your tools to help guide the model on how input data should be structured.

When supported by providers, input examples can help when JSON schema itself does not fully specify the intended

usage or when there are optional values.

```ts

tool({

&nbsp; description: 'Get the weather in a location',

&nbsp; inputSchema: z.object({

&nbsp;   location: z.string().describe('The location to get the weather for'),

&nbsp; }),

&nbsp; inputExamples: \[

&nbsp;   { input: { location: 'San Francisco' } },

&nbsp;   { input: { location: 'London' } },

&nbsp; ],

&nbsp; execute: async ({ location }) => {

&nbsp;   // ...

&nbsp; },

});

```

<Note>

&nbsp; Only the Anthropic providers supports tool input examples natively. Other

&nbsp; providers ignore the setting.

</Note>

\## Tool Execution Approval

By default, tools with an `execute` function run automatically as the model calls them. You can require approval before execution by setting `needsApproval`:

```ts highlight="13"

import { tool } from 'ai';

import { z } from 'zod';



const runCommand = tool({

&nbsp; description: 'Run a shell command',

&nbsp; inputSchema: z.object({

&nbsp;   command: z.string().describe('The shell command to execute'),

&nbsp; }),

&nbsp; needsApproval: true,

&nbsp; execute: async ({ command }) => {

&nbsp;   // your command execution logic here

&nbsp; },

});

```

This is useful for tools that perform sensitive operations like executing commands, processing payments, modifying data, and more potentially dangerous actions.

\### How It Works

When a tool requires approval, `generateText` and `streamText` don't pause execution. Instead, they complete and return `tool-approval-request` parts in the result content. This means the approval flow requires two calls to the model: the first returns the approval request, and the second (after receiving the approval response) either executes the tool or informs the model that approval was denied.

Here's the complete flow:

1\. Call `generateText` with a tool that has `needsApproval: true`

2\. Model generates a tool call

3\. `generateText` returns with `tool-approval-request` parts in `result.content`

4\. Your app requests an approval and collects the user's decision

5\. Add a `tool-approval-response` to the messages array

6\. Call `generateText` again with the updated messages

7\. If approved, the tool runs and returns a result. If denied, the model sees the denial and responds accordingly.

\### Handling Approval Requests

After calling `generateText` or `streamText`, check `result.content` for `tool-approval-request` parts:

```ts

import { type ModelMessage, generateText } from 'ai';



const messages: ModelMessage\[] = \[

&nbsp; { role: 'user', content: 'Remove the most recent file' },

];

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: { runCommand },

&nbsp; messages,

});



messages.push(...result.response.messages);



for (const part of result.content) {

&nbsp; if (part.type === 'tool-approval-request') {

&nbsp;   console.log(part.approvalId); // Unique ID for this approval request

&nbsp;   console.log(part.toolCall); // Contains toolName, input, etc.

&nbsp; }

}

```

To respond, create a `tool-approval-response` and add it to your messages:

```ts

import { type ToolApprovalResponse } from 'ai';



const approvals: ToolApprovalResponse\[] = \[];



for (const part of result.content) {

&nbsp; if (part.type === 'tool-approval-request') {

&nbsp;   const response: ToolApprovalResponse = {

&nbsp;     type: 'tool-approval-response',

&nbsp;     approvalId: part.approvalId,

&nbsp;     approved: true, // or false to deny

&nbsp;     reason: 'User confirmed the command', // Optional context for the model

&nbsp;   };

&nbsp;   approvals.push(response);

&nbsp; }

}



// add approvals to messages

messages.push({ role: 'tool', content: approvals });

```

Then call `generateText` again with the updated messages. If approved, the tool executes. If denied, the model receives the denial and can respond accordingly.

<Note>

&nbsp; When a tool execution is denied, consider adding a system instruction like

&nbsp; "When a tool execution is not approved, do not retry it" to prevent the model

&nbsp; from attempting the same call again.

</Note>

\### Dynamic Approval

You can make approval decisions based on tool input by providing an async function:

```ts

const paymentTool = tool({

&nbsp; description: 'Process a payment',

&nbsp; inputSchema: z.object({

&nbsp;   amount: z.number(),

&nbsp;   recipient: z.string(),

&nbsp; }),

&nbsp; needsApproval: async ({ amount }) => amount > 1000,

&nbsp; execute: async ({ amount, recipient }) => {

&nbsp;   return await processPayment(amount, recipient);

&nbsp; },

});

```

In this example, only transactions over $1000 require approval. Smaller transactions execute automatically.

\### Tool Execution Approval with useChat

When using `useChat`, the approval flow is handled through UI state. See \[Chatbot Tool Usage](/docs/ai-sdk-ui/chatbot-tool-usage#tool-execution-approval) for details on handling approvals in your UI with `addToolApprovalResponse`.

\## Multi-Step Calls (using stopWhen)

With the `stopWhen` setting, you can enable multi-step calls in `generateText` and `streamText`. When `stopWhen` is set and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the stopping condition is met.

<Note>

&nbsp; The `stopWhen` conditions are only evaluated when the last step contains tool

&nbsp; results.

</Note>

By default, when you use `generateText` or `streamText`, it triggers a single generation. This works well for many use cases where you can rely on the model's training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, its generation is complete and that step is finished.

You may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.

You can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model's training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.

\### Example

In the following example, there are two steps:

1\. \*\*Step 1\*\*

&nbsp; 1. The prompt `'What is the weather in San Francisco?'` is sent to the model.

&nbsp; 1. The model generates a tool call.

&nbsp; 1. The tool call is executed.

1\. \*\*Step 2\*\*

&nbsp; 1. The tool result is sent to the model.

&nbsp; 1. The model generates a response considering the tool result.

```ts highlight="18-19"

import { z } from 'zod';

import { generateText, tool, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { text, steps } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; stopWhen: stepCountIs(5), // stop after a maximum of 5 steps if tools were called

&nbsp; prompt: 'What is the weather in San Francisco?',

});

```

<Note>You can use `streamText` in a similar way.</Note>

\### Steps

To access intermediate tool calls and results, you can use the `steps` property in the result object

or the `streamText` `onFinish` callback.

It contains all the text, tool calls, tool results, and more from each step.

\#### Example: Extract tool results from all steps

```ts highlight="3,9-10"

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { steps } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; stopWhen: stepCountIs(10),

&nbsp; // ...

});



// extract all tool calls from the steps:

const allToolCalls = steps.flatMap(step => step.toolCalls);

```

\### `onStepFinish` callback

When using `generateText` or `streamText`, you can provide an `onStepFinish` callback that

is triggered when a step is finished,

i.e. all text deltas, tool calls, and tool results for the step are available.

When you have multiple steps, the callback is triggered for each step.

```tsx highlight="5-7"

import { generateText } from 'ai';



const result = await generateText({

&nbsp; // ...

&nbsp; onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {

&nbsp;   // your own logic, e.g. for saving the chat history or recording usage

&nbsp; },

});

```

\### `prepareStep` callback

The `prepareStep` callback is called before a step is started.

It is called with the following parameters:

\- `model`: The model that was passed into `generateText`.

\- `stopWhen`: The stopping condition that was passed into `generateText`.

\- `stepNumber`: The number of the step that is being executed.

\- `steps`: The steps that have been executed so far.

\- `messages`: The messages that will be sent to the model for the current step.

\- `experimental\_context`: The context passed via the `experimental\_context` setting (experimental).

You can use it to provide different settings for a step, including modifying the input messages.

```tsx highlight="5-7"

import { generateText } from 'ai';



const result = await generateText({

&nbsp; // ...

&nbsp; prepareStep: async ({ model, stepNumber, steps, messages }) => {

&nbsp;   if (stepNumber === 0) {

&nbsp;     return {

&nbsp;       // use a different model for this step:

&nbsp;       model: modelForThisParticularStep,

&nbsp;       // force a tool choice for this step:

&nbsp;       toolChoice: { type: 'tool', toolName: 'tool1' },

&nbsp;       // limit the tools that are available for this step:

&nbsp;       activeTools: \['tool1'],

&nbsp;     };

&nbsp;   }



&nbsp;   // when nothing is returned, the default settings are used

&nbsp; },

});

```

\#### Message Modification for Longer Agentic Loops

In longer agentic loops, you can use the `messages` parameter to modify the input messages for each step. This is particularly useful for prompt compression:

```tsx

prepareStep: async ({ stepNumber, steps, messages }) => {

&nbsp; // Compress conversation history for longer loops

&nbsp; if (messages.length > 20) {

&nbsp;   return {

&nbsp;     messages: messages.slice(-10),

&nbsp;   };

&nbsp; }



&nbsp; return {};

},

```

\#### Provider Options for Step Configuration

You can use `providerOptions` in `prepareStep` to pass provider-specific configuration for each step. This is useful for features like Anthropic's code execution container persistence:

```tsx

import { forwardAnthropicContainerIdFromLastStep } from '@ai-sdk/anthropic';



// Propagate container ID from previous step for code execution continuity

prepareStep: forwardAnthropicContainerIdFromLastStep,

```

\## Response Messages

Adding the generated assistant and tool messages to your conversation history is a common task,

especially if you are using multi-step tool calls.

Both `generateText` and `streamText` have a `response.messages` property that you can use to

add the assistant and tool messages to your conversation history.

It is also available in the `onFinish` callback of `streamText`.

The `response.messages` property contains an array of `ModelMessage` objects that you can add to your conversation history:

```ts

import { generateText, ModelMessage } from 'ai';



const messages: ModelMessage\[] = \[

&nbsp; // ...

];



const { response } = await generateText({

&nbsp; // ...

&nbsp; messages,

});



// add the response messages to your conversation history:

messages.push(...response.messages); // streamText: ...((await response).messages)

```

\## Dynamic Tools

AI SDK Core supports dynamic tools for scenarios where tool schemas are not known at compile time. This is useful for:

\- MCP (Model Context Protocol) tools without schemas

\- User-defined functions at runtime

\- Tools loaded from external sources

\### Using dynamicTool

The `dynamicTool` helper creates tools with unknown input/output types:

```ts

import { dynamicTool } from 'ai';

import { z } from 'zod';



const customTool = dynamicTool({

&nbsp; description: 'Execute a custom function',

&nbsp; inputSchema: z.object({}),

&nbsp; execute: async input => {

&nbsp;   // input is typed as 'unknown'

&nbsp;   // You need to validate/cast it at runtime

&nbsp;   const { action, parameters } = input as any;



&nbsp;   // Execute your dynamic logic

&nbsp;   return { result: `Executed ${action}` };

&nbsp; },

});

```

\### Type-Safe Handling

When using both static and dynamic tools, use the `dynamic` flag for type narrowing:

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   // Static tool with known types

&nbsp;   weather: weatherTool,

&nbsp;   // Dynamic tool

&nbsp;   custom: dynamicTool({

&nbsp;     /\* ... \*/

&nbsp;   }),

&nbsp; },

&nbsp; onStepFinish: ({ toolCalls, toolResults }) => {

&nbsp;   // Type-safe iteration

&nbsp;   for (const toolCall of toolCalls) {

&nbsp;     if (toolCall.dynamic) {

&nbsp;       // Dynamic tool: input is 'unknown'

&nbsp;       console.log('Dynamic:', toolCall.toolName, toolCall.input);

&nbsp;       continue;

&nbsp;     }



&nbsp;     // Static tool: full type inference

&nbsp;     switch (toolCall.toolName) {

&nbsp;       case 'weather':

&nbsp;         console.log(toolCall.input.location); // typed as string

&nbsp;         break;

&nbsp;     }

&nbsp;   }

&nbsp; },

});

```

\## Preliminary Tool Results

You can return an `AsyncIterable` over multiple results.

In this case, the last value from the iterable is the final tool result.

This can be used in combination with generator functions to e.g. stream status information

during the tool execution:

```ts

tool({

&nbsp; description: 'Get the current weather.',

&nbsp; inputSchema: z.object({

&nbsp;   location: z.string(),

&nbsp; }),

&nbsp; async \*execute({ location }) {

&nbsp;   yield {

&nbsp;     status: 'loading' as const,

&nbsp;     text: `Getting weather for ${location}`,

&nbsp;     weather: undefined,

&nbsp;   };



&nbsp;   await new Promise(resolve => setTimeout(resolve, 3000));



&nbsp;   const temperature = 72 + Math.floor(Math.random() \* 21) - 10;



&nbsp;   yield {

&nbsp;     status: 'success' as const,

&nbsp;     text: `The weather in ${location} is ${temperature}°F`,

&nbsp;     temperature,

&nbsp;   };

&nbsp; },

});

```

\## Tool Choice

You can use the `toolChoice` setting to influence when a tool is selected.

It supports the following settings:

\- `auto` (default): the model can choose whether and which tools to call.

\- `required`: the model must call a tool. It can choose which tool to call.

\- `none`: the model must not call tools

\- `{ type: 'tool', toolName: string (typed) }`: the model must call the specified tool

```ts highlight="18"

import { z } from 'zod';

import { generateText, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; toolChoice: 'required', // force the model to call a tool

&nbsp; prompt: 'What is the weather in San Francisco?',

});

```

\## Tool Execution Options

When tools are called, they receive additional options as a second parameter.

\### Tool Call ID

The ID of the tool call is forwarded to the tool execution.

You can use it e.g. when sending tool-call related information with stream data.

```ts highlight="14-20"

import {

&nbsp; streamText,

&nbsp; tool,

&nbsp; createUIMessageStream,

&nbsp; createUIMessageStreamResponse,

} from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages } = await req.json();



&nbsp; const stream = createUIMessageStream({

&nbsp;   execute: ({ writer }) => {

&nbsp;     const result = streamText({

&nbsp;       // ...

&nbsp;       messages,

&nbsp;       tools: {

&nbsp;         myTool: tool({

&nbsp;           // ...

&nbsp;           execute: async (args, { toolCallId }) => {

&nbsp;             // return e.g. custom status for tool call

&nbsp;             writer.write({

&nbsp;               type: 'data-tool-status',

&nbsp;               id: toolCallId,

&nbsp;               data: {

&nbsp;                 name: 'myTool',

&nbsp;                 status: 'in-progress',

&nbsp;               },

&nbsp;             });

&nbsp;             // ...

&nbsp;           },

&nbsp;         }),

&nbsp;       },

&nbsp;     });



&nbsp;     writer.merge(result.toUIMessageStream());

&nbsp;   },

&nbsp; });



&nbsp; return createUIMessageStreamResponse({ stream });

}

```

\### Messages

The messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution.

You can access them in the second parameter of the `execute` function.

In multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.

```ts highlight="8-9"

import { generateText, tool } from 'ai';



const result = await generateText({

&nbsp; // ...

&nbsp; tools: {

&nbsp;   myTool: tool({

&nbsp;     // ...

&nbsp;     execute: async (args, { messages }) => {

&nbsp;       // use the message history in e.g. calls to other language models

&nbsp;       return { ... };

&nbsp;     },

&nbsp;   }),

&nbsp; },

});

```

\### Abort Signals

The abort signals from `generateText` and `streamText` are forwarded to the tool execution.

You can access them in the second parameter of the `execute` function and e.g. abort long-running computations or forward them to fetch calls inside tools.

```ts highlight="6,11,14"

import { z } from 'zod';

import { generateText, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; abortSignal: myAbortSignal, // signal that will be forwarded to tools

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({ location: z.string() }),

&nbsp;     execute: async ({ location }, { abortSignal }) => {

&nbsp;       return fetch(

&nbsp;         `https://api.weatherapi.com/v1/current.json?q=${location}`,

&nbsp;         { signal: abortSignal }, // forward the abort signal to fetch

&nbsp;       );

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'What is the weather in San Francisco?',

});

```

\### Context (experimental)

You can pass in arbitrary context from `generateText` or `streamText` via the `experimental\_context` setting.

This context is available in the `experimental\_context` tool execution option.

```ts

const result = await generateText({

&nbsp; // ...

&nbsp; tools: {

&nbsp;   someTool: tool({

&nbsp;     // ...

&nbsp;     execute: async (input, { experimental\_context: context }) => {

&nbsp;       const typedContext = context as { example: string }; // or use type validation library

&nbsp;       // ...

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; experimental\_context: { example: '123' },

});

```

\## Tool Input Lifecycle Hooks

The following tool input lifecycle hooks are available:

\- \*\*`onInputStart`\*\*: Called when the model starts generating the input (arguments) for the tool call

\- \*\*`onInputDelta`\*\*: Called for each chunk of text as the input is streamed

\- \*\*`onInputAvailable`\*\*: Called when the complete input is available and validated

`onInputStart` and `onInputDelta` are only called in streaming contexts (when using `streamText`). They are not called when using `generateText`.

\### Example

```ts highlight="15-23"

import { streamText, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The location to get the weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;     }),

&nbsp;     onInputStart: () => {

&nbsp;       console.log('Tool call starting');

&nbsp;     },

&nbsp;     onInputDelta: ({ inputTextDelta }) => {

&nbsp;       console.log('Received input chunk:', inputTextDelta);

&nbsp;     },

&nbsp;     onInputAvailable: ({ input }) => {

&nbsp;       console.log('Complete input:', input);

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'What is the weather in San Francisco?',

});

```

\## Types

Modularizing your code often requires defining types to ensure type safety and reusability.

To enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.

You can use them to strongly type your variables, function parameters, and return types

in parts of the code that are not directly related to `streamText` or `generateText`.

Each tool call is typed with `ToolCall<NAME extends string, ARGS>`, depending

on the tool that has been invoked.

Similarly, the tool results are typed with `ToolResult<NAME extends string, ARGS, RESULT>`.

The tools in `streamText` and `generateText` are defined as a `ToolSet`.

The type inference helpers `TypedToolCall<TOOLS extends ToolSet>`

and `TypedToolResult<TOOLS extends ToolSet>` can be used to

extract the tool call and tool result types from the tools.

```ts highlight="18-19,23-24"

import { TypedToolCall, TypedToolResult, generateText, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



const myToolSet = {

&nbsp; firstTool: tool({

&nbsp;   description: 'Greets the user',

&nbsp;   inputSchema: z.object({ name: z.string() }),

&nbsp;   execute: async ({ name }) => `Hello, ${name}!`,

&nbsp; }),

&nbsp; secondTool: tool({

&nbsp;   description: 'Tells the user their age',

&nbsp;   inputSchema: z.object({ age: z.number() }),

&nbsp;   execute: async ({ age }) => `You are ${age} years old!`,

&nbsp; }),

};



type MyToolCall = TypedToolCall<typeof myToolSet>;

type MyToolResult = TypedToolResult<typeof myToolSet>;



async function generateSomething(prompt: string): Promise<{

&nbsp; text: string;

&nbsp; toolCalls: Array<MyToolCall>; // typed tool calls

&nbsp; toolResults: Array<MyToolResult>; // typed tool results

}> {

&nbsp; return generateText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   tools: myToolSet,

&nbsp;   prompt,

&nbsp; });

}

```

\## Handling Errors

The AI SDK has three tool-call related errors:

\- \[`NoSuchToolError`](/docs/reference/ai-sdk-errors/ai-no-such-tool-error): the model tries to call a tool that is not defined in the tools object

\- \[`InvalidToolInputError`](/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error): the model calls a tool with inputs that do not match the tool's input schema

\- \[`ToolCallRepairError`](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error): an error that occurred during tool call repair

When tool execution fails (errors thrown by your tool's `execute` function), the AI SDK adds them as `tool-error` content parts to enable automated LLM roundtrips in multi-step scenarios.

\### `generateText`

`generateText` throws errors for tool schema validation issues and other errors, and can be handled using a `try`/`catch` block. Tool execution errors appear as `tool-error` parts in the result steps:

```ts

try {

&nbsp; const result = await generateText({

&nbsp;   //...

&nbsp; });

} catch (error) {

&nbsp; if (NoSuchToolError.isInstance(error)) {

&nbsp;   // handle the no such tool error

&nbsp; } else if (InvalidToolInputError.isInstance(error)) {

&nbsp;   // handle the invalid tool inputs error

&nbsp; } else {

&nbsp;   // handle other errors

&nbsp; }

}

```

Tool execution errors are available in the result steps:

```ts

const { steps } = await generateText({

&nbsp; // ...

});



// check for tool errors in the steps

const toolErrors = steps.flatMap(step =>

&nbsp; step.content.filter(part => part.type === 'tool-error'),

);



toolErrors.forEach(toolError => {

&nbsp; console.log('Tool error:', toolError.error);

&nbsp; console.log('Tool name:', toolError.toolName);

&nbsp; console.log('Tool input:', toolError.input);

});

```

\### `streamText`

`streamText` sends errors as part of the full stream. Tool execution errors appear as `tool-error` parts, while other errors appear as `error` parts.

When using `toUIMessageStreamResponse`, you can pass an `onError` function to extract the error message from the error part and forward it as part of the stream response:

```ts

const result = streamText({

&nbsp; // ...

});



return result.toUIMessageStreamResponse({

&nbsp; onError: error => {

&nbsp;   if (NoSuchToolError.isInstance(error)) {

&nbsp;     return 'The model tried to call a unknown tool.';

&nbsp;   } else if (InvalidToolInputError.isInstance(error)) {

&nbsp;     return 'The model called a tool with invalid inputs.';

&nbsp;   } else {

&nbsp;     return 'An unknown error occurred.';

&nbsp;   }

&nbsp; },

});

```

\## Tool Call Repair

<Note type="warning">

&nbsp; The tool call repair feature is experimental and may change in the future.

</Note>

Language models sometimes fail to generate valid tool calls,

especially when the input schema is complex or the model is smaller.

If you use multiple steps, those failed tool calls will be sent back to the LLM

in the next step to give it an opportunity to fix it.

However, you may want to control how invalid tool calls are repaired without requiring

additional steps that pollute the message history.

You can use the `experimental\_repairToolCall` function to attempt to repair the tool call

with a custom function.

You can use different strategies to repair the tool call:

\- Use a model with structured outputs to generate the inputs.

\- Send the messages, system prompt, and tool schema to a stronger model to generate the inputs.

\- Provide more specific repair instructions based on which tool was called.

\### Example: Use a model with structured outputs for repair

```ts

import { openai } from '@ai-sdk/openai';

import { generateObject, generateText, NoSuchToolError, tool } from 'ai';



const result = await generateText({

&nbsp; model,

&nbsp; tools,

&nbsp; prompt,



&nbsp; experimental\_repairToolCall: async ({

&nbsp;   toolCall,

&nbsp;   tools,

&nbsp;   inputSchema,

&nbsp;   error,

&nbsp; }) => {

&nbsp;   if (NoSuchToolError.isInstance(error)) {

&nbsp;     return null; // do not attempt to fix invalid tool names

&nbsp;   }



&nbsp;   const tool = tools\[toolCall.toolName as keyof typeof tools];



&nbsp;   const { object: repairedArgs } = await generateObject({

&nbsp;     model: \_\_MODEL\_\_,

&nbsp;     schema: tool.inputSchema,

&nbsp;     prompt: \[

&nbsp;       `The model tried to call the tool "${toolCall.toolName}"` +

&nbsp;         ` with the following inputs:`,

&nbsp;       JSON.stringify(toolCall.input),

&nbsp;       `The tool accepts the following schema:`,

&nbsp;       JSON.stringify(inputSchema(toolCall)),

&nbsp;       'Please fix the inputs.',

&nbsp;     ].join('\\n'),

&nbsp;   });



&nbsp;   return { ...toolCall, input: JSON.stringify(repairedArgs) };

&nbsp; },

});

```

\### Example: Use the re-ask strategy for repair

```ts

import { openai } from '@ai-sdk/openai';

import { generateObject, generateText, NoSuchToolError, tool } from 'ai';



const result = await generateText({

&nbsp; model,

&nbsp; tools,

&nbsp; prompt,



&nbsp; experimental\_repairToolCall: async ({

&nbsp;   toolCall,

&nbsp;   tools,

&nbsp;   error,

&nbsp;   messages,

&nbsp;   system,

&nbsp; }) => {

&nbsp;   const result = await generateText({

&nbsp;     model,

&nbsp;     system,

&nbsp;     messages: \[

&nbsp;       ...messages,

&nbsp;       {

&nbsp;         role: 'assistant',

&nbsp;         content: \[

&nbsp;           {

&nbsp;             type: 'tool-call',

&nbsp;             toolCallId: toolCall.toolCallId,

&nbsp;             toolName: toolCall.toolName,

&nbsp;             input: toolCall.input,

&nbsp;           },

&nbsp;         ],

&nbsp;       },

&nbsp;       {

&nbsp;         role: 'tool' as const,

&nbsp;         content: \[

&nbsp;           {

&nbsp;             type: 'tool-result',

&nbsp;             toolCallId: toolCall.toolCallId,

&nbsp;             toolName: toolCall.toolName,

&nbsp;             output: error.message,

&nbsp;           },

&nbsp;         ],

&nbsp;       },

&nbsp;     ],

&nbsp;     tools,

&nbsp;   });



&nbsp;   const newToolCall = result.toolCalls.find(

&nbsp;     newToolCall => newToolCall.toolName === toolCall.toolName,

&nbsp;   );



&nbsp;   return newToolCall != null

&nbsp;     ? {

&nbsp;         toolCallType: 'function' as const,

&nbsp;         toolCallId: toolCall.toolCallId,

&nbsp;         toolName: toolCall.toolName,

&nbsp;         input: JSON.stringify(newToolCall.input),

&nbsp;       }

&nbsp;     : null;

&nbsp; },

});

```

\## Active Tools

Language models can only handle a limited number of tools at a time, depending on the model.

To allow for static typing using a large number of tools and limiting the available tools to the model at the same time,

the AI SDK provides the `activeTools` property.

It is an array of tool names that are currently active.

By default, the value is `undefined` and all tools are active.

```ts highlight="7"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { text } = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: myToolSet,

&nbsp; activeTools: \['firstTool'],

});

```

\## Multi-modal Tool Results

<Note type="warning">

&nbsp; Multi-modal tool results are experimental and only supported by Anthropic and

&nbsp; OpenAI.

</Note>

In order to send multi-modal tool results, e.g. screenshots, back to the model,

they need to be converted into a specific format.

AI SDK Core tools have an optional `toModelOutput` function

that converts the tool result into a content part.

Here is an example for converting a screenshot into a content part:

```ts highlight="22-27"

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools: {

&nbsp;   computer: anthropic.tools.computer\_20241022({

&nbsp;     // ...

&nbsp;     async execute({ action, coordinate, text }) {

&nbsp;       switch (action) {

&nbsp;         case 'screenshot': {

&nbsp;           return {

&nbsp;             type: 'image',

&nbsp;             data: fs

&nbsp;               .readFileSync('./data/screenshot-editor.png')

&nbsp;               .toString('base64'),

&nbsp;           };

&nbsp;         }

&nbsp;         default: {

&nbsp;           return `executed ${action}`;

&nbsp;         }

&nbsp;       }

&nbsp;     },



&nbsp;     // map to tool result content for LLM consumption:

&nbsp;     toModelOutput({ output }) {

&nbsp;       return {

&nbsp;         type: 'content',

&nbsp;         value:

&nbsp;           typeof output === 'string'

&nbsp;             ? \[{ type: 'text', text: output }]

&nbsp;             : \[{ type: 'media', data: output.data, mediaType: 'image/png' }],

&nbsp;       };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; // ...

});

```

\## Extracting Tools

Once you start having many tools, you might want to extract them into separate files.

The `tool` helper function is crucial for this, because it ensures correct type inference.

Here is an example of an extracted tool:

```ts filename="tools/weather-tool.ts" highlight="1,4-5"

import { tool } from 'ai';

import { z } from 'zod';



// the `tool` helper function ensures correct type inference:

export const weatherTool = tool({

&nbsp; description: 'Get the weather in a location',

&nbsp; inputSchema: z.object({

&nbsp;   location: z.string().describe('The location to get the weather for'),

&nbsp; }),

&nbsp; execute: async ({ location }) => ({

&nbsp;   location,

&nbsp;   temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp; }),

});

```

\## MCP Tools

The AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools.

MCP enables your AI applications to discover and use tools across various services through a standardized interface.

For detailed information about MCP tools, including initialization, transport options, and usage patterns, see the \[MCP Tools documentation](/docs/ai-sdk-core/mcp-tools).

\### AI SDK Tools vs MCP Tools

In most cases, you should define your own AI SDK tools for production applications. They provide full control, type safety, and optimal performance. MCP tools are best suited for rapid development iteration and scenarios where users bring their own tools.

| Aspect | AI SDK Tools | MCP Tools |

| ---------------------- | --------------------------------------------------------- | ----------------------------------------------------- |

| \*\*Type Safety\*\* | Full static typing end-to-end | Dynamic discovery at runtime |

| \*\*Execution\*\* | Same process as your request (low latency) | Separate server (network overhead) |

| \*\*Prompt Control\*\* | Full control over descriptions and schemas | Controlled by MCP server owner |

| \*\*Schema Control\*\* | You define and optimize for your model | Controlled by MCP server owner |

| \*\*Version Management\*\* | Full visibility over updates | Can update independently (version skew risk) |

| \*\*Authentication\*\* | Same process, no additional auth required | Separate server introduces additional auth complexity |

| \*\*Best For\*\* | Production applications requiring control and performance | Development iteration, user-provided tools |

\## Examples

You can see tools in action using various frameworks in the following examples:

<ExampleLinks

&nbsp; examples={\[

&nbsp; {

&nbsp; title: 'Learn to use tools in Node.js',

&nbsp; link: '/cookbook/node/call-tools',

&nbsp; },

&nbsp; {

&nbsp; title: 'Learn to use tools in Next.js with Route Handlers',

&nbsp; link: '/cookbook/next/call-tools',

&nbsp; },

&nbsp; {

&nbsp; title: 'Learn to use MCP tools in Node.js',

&nbsp; link: '/cookbook/node/mcp-tools',

&nbsp; },

&nbsp; ]}

/>

---

title: Model Context Protocol (MCP)

description: Learn how to connect to Model Context Protocol (MCP) servers and use their tools with AI SDK Core.

---

\# Model Context Protocol (MCP)

The AI SDK supports connecting to \[Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers to access their tools, resources, and prompts.

This enables your AI applications to discover and use capabilities across various services through a standardized interface.

<Note>

&nbsp; If you're using OpenAI's Responses API, you can also use the built-in

&nbsp; `openai.tools.mcp` tool, which provides direct MCP server integration without

&nbsp; needing to convert tools. See the \[OpenAI provider

&nbsp; documentation](/providers/ai-sdk-providers/openai#mcp-tool) for details.

</Note>

\## Initializing an MCP Client

We recommend using HTTP transport (like `StreamableHTTPClientTransport`) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.

Create an MCP client using one of the following transport options:

\- \*\*HTTP transport (Recommended)\*\*: Either configure HTTP directly via the client using `transport: { type: 'http', ... }`, or use MCP's official TypeScript SDK `StreamableHTTPClientTransport`

\- SSE (Server-Sent Events): An alternative HTTP-based transport

\- `stdio`: For local development only. Uses standard input/output streams for local MCP servers

\### HTTP Transport (Recommended)

For production deployments, we recommend using the HTTP transport. You can configure it directly on the client:

```typescript

import { createMCPClient } from '@ai-sdk/mcp';



const mcpClient = await createMCPClient({

&nbsp; transport: {

&nbsp;   type: 'http',

&nbsp;   url: 'https://your-server.com/mcp',



&nbsp;   // optional: configure HTTP headers

&nbsp;   headers: { Authorization: 'Bearer my-api-key' },



&nbsp;   // optional: provide an OAuth client provider for automatic authorization

&nbsp;   authProvider: myOAuthClientProvider,

&nbsp; },

});

```

Alternatively, you can use `StreamableHTTPClientTransport` from MCP's official TypeScript SDK:

```typescript

import { createMCPClient } from '@ai-sdk/mcp';

import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/streamableHttp.js';



const url = new URL('https://your-server.com/mcp');

const mcpClient = await createMCPClient({

&nbsp; transport: new StreamableHTTPClientTransport(url, {

&nbsp;   sessionId: 'session\_123',

&nbsp; }),

});

```

\### SSE Transport

SSE provides an alternative HTTP-based transport option. Configure it with a `type` and `url` property. You can also provide an `authProvider` for OAuth:

```typescript

import { createMCPClient } from '@ai-sdk/mcp';



const mcpClient = await createMCPClient({

&nbsp; transport: {

&nbsp;   type: 'sse',

&nbsp;   url: 'https://my-server.com/sse',



&nbsp;   // optional: configure HTTP headers

&nbsp;   headers: { Authorization: 'Bearer my-api-key' },



&nbsp;   // optional: provide an OAuth client provider for automatic authorization

&nbsp;   authProvider: myOAuthClientProvider,

&nbsp; },

});

```

\### Stdio Transport (Local Servers)

<Note type="warning">

&nbsp; The stdio transport should only be used for local servers.

</Note>

The Stdio transport can be imported from either the MCP SDK or the AI SDK:

```typescript

import { createMCPClient } from '@ai-sdk/mcp';

import { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';

// Or use the AI SDK's stdio transport:

// import { Experimental\_StdioMCPTransport as StdioClientTransport } from '@ai-sdk/mcp/mcp-stdio';



const mcpClient = await createMCPClient({

&nbsp; transport: new StdioClientTransport({

&nbsp;   command: 'node',

&nbsp;   args: \['src/stdio/dist/server.js'],

&nbsp; }),

});

```

\### Custom Transport

You can also bring your own transport by implementing the `MCPTransport` interface for specific requirements not covered by the standard transports.

<Note>

&nbsp; The client returned by the `createMCPClient` function is a

&nbsp; lightweight client intended for use in tool conversion. It currently does not

&nbsp; support all features of the full MCP client, such as: session

&nbsp; management, resumable streams, and receiving notifications.

Authorization via OAuth is supported when using the AI SDK MCP HTTP or SSE

transports by providing an `authProvider`.

</Note>

\### Closing the MCP Client

After initialization, you should close the MCP client based on your usage pattern:

\- For short-lived usage (e.g., single requests), close the client when the response is finished

\- For long-running clients (e.g., command line apps), keep the client open but ensure it's closed when the application terminates

When streaming responses, you can close the client when the LLM response has finished. For example, when using `streamText`, you should use the `onFinish` callback:

```typescript

const mcpClient = await createMCPClient({

&nbsp; // ...

});



const tools = await mcpClient.tools();



const result = await streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; tools,

&nbsp; prompt: 'What is the weather in Brooklyn, New York?',

&nbsp; onFinish: async () => {

&nbsp;   await mcpClient.close();

&nbsp; },

});

```

When generating responses without streaming, you can use try/finally or cleanup functions in your framework:

```typescript

let mcpClient: MCPClient | undefined;



try {

&nbsp; mcpClient = await createMCPClient({

&nbsp;   // ...

&nbsp; });

} finally {

&nbsp; await mcpClient?.close();

}

```

\## Using MCP Tools

The client's `tools` method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:

\### Schema Discovery

With schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:

```typescript
const tools = await mcpClient.tools();
```

This approach is simpler to implement and automatically stays in sync with server changes. However, you won't have TypeScript type safety during development, and all tools from the server will be loaded

\### Schema Definition

For better type safety and control, you can define the tools and their input schemas explicitly in your client code:

```typescript

import { z } from 'zod';



const tools = await mcpClient.tools({

&nbsp; schemas: {

&nbsp;   'get-data': {

&nbsp;     inputSchema: z.object({

&nbsp;       query: z.string().describe('The data query'),

&nbsp;       format: z.enum(\['json', 'text']).optional(),

&nbsp;     }),

&nbsp;   },

&nbsp;   // For tools with zero inputs, you should use an empty object:

&nbsp;   'tool-with-no-args': {

&nbsp;     inputSchema: z.object({}),

&nbsp;   },

&nbsp; },

});

```

This approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define `schemas`, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs

\### Typed Tool Outputs

When MCP servers return `structuredContent` (per the \[MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/tools#structured-content)), you can define an `outputSchema` to get typed tool results:

```typescript

import { z } from 'zod';



const tools = await mcpClient.tools({

&nbsp; schemas: {

&nbsp;   'get-weather': {

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string(),

&nbsp;     }),

&nbsp;     // Define outputSchema for typed results

&nbsp;     outputSchema: z.object({

&nbsp;       temperature: z.number(),

&nbsp;       conditions: z.string(),

&nbsp;       humidity: z.number(),

&nbsp;     }),

&nbsp;   },

&nbsp; },

});



const result = await tools\['get-weather'].execute(

&nbsp; { location: 'New York' },

&nbsp; { messages: \[], toolCallId: 'weather-1' },

);



console.log(`Temperature: ${result.temperature}°C`);

```

When `outputSchema` is provided:

\- The client extracts `structuredContent` from the tool result

\- The output is validated against your schema at runtime

\- You get full TypeScript type safety for the result

If the server doesn't return `structuredContent`, the client falls back to parsing JSON from the text content. If neither is available or validation fails, an error is thrown.

<Note>

&nbsp; Without `outputSchema`, the tool returns the raw `CallToolResult` object

&nbsp; containing `content` and optional `isError` fields.

</Note>

\## Using MCP Resources

According to the \[MCP specification](https://modelcontextprotocol.io/docs/learn/server-concepts#resources), resources are \*\*application-driven\*\* data sources that provide context to the model. Unlike tools (which are model-controlled), your application decides when to fetch and pass resources as context.

The MCP client provides three methods for working with resources:

\### Listing Resources

List all available resources from the MCP server:

```typescript
const resources = await mcpClient.listResources();
```

\### Reading Resource Contents

Read the contents of a specific resource by its URI:

```typescript

const resourceData = await mcpClient.readResource({

&nbsp; uri: 'file:///example/document.txt',

});

```

\### Listing Resource Templates

Resource templates are dynamic URI patterns that allow flexible queries. List all available templates:

```typescript
const templates = await mcpClient.listResourceTemplates();
```

\## Using MCP Prompts

<Note type="warning">

&nbsp; MCP Prompts is an experimental feature and may change in the future.

</Note>

According to the MCP specification, prompts are user-controlled templates that servers expose for clients to list and retrieve with optional arguments.

\### Listing Prompts

```typescript

const prompts = await mcpClient.experimental\_listPrompts();

```

\### Getting a Prompt

Retrieve prompt messages, optionally passing arguments defined by the server:

```typescript

const prompt = await mcpClient.experimental\_getPrompt({

&nbsp; name: 'code\_review',

&nbsp; arguments: { code: 'function add(a, b) { return a + b; }' },

});

```

\## Handling Elicitation Requests

Elicitation is a mechanism where MCP servers can request additional information from the client during tool execution. For example, a server might need user input to complete a registration form or confirmation for a sensitive operation.

<Note type="warning">

&nbsp; It is up to the client application to handle elicitation requests properly.

&nbsp; The MCP client simply surfaces these requests from the server to your

&nbsp; application code.

</Note>

\### Enabling Elicitation Support

To enable elicitation, you need to advertise the capability when creating the MCP client:

```typescript

const mcpClient = await createMCPClient({

&nbsp; transport: {

&nbsp;   type: 'sse',

&nbsp;   url: 'https://your-server.com/sse',

&nbsp; },

&nbsp; capabilities: {

&nbsp;   elicitation: {},

&nbsp; },

});

```

\### Registering an Elicitation Handler

Use the `onElicitationRequest` method to register a handler that will be called when the server requests input:

```typescript

import { ElicitationRequestSchema } from '@ai-sdk/mcp';



mcpClient.onElicitationRequest(ElicitationRequestSchema, async request => {

&nbsp; // request.params.message: A message describing what input is needed

&nbsp; // request.params.requestedSchema: JSON schema defining the expected input structure



&nbsp; // Get input from the user (implement according to your application's needs)

&nbsp; const userInput = await getInputFromUser(

&nbsp;   request.params.message,

&nbsp;   request.params.requestedSchema,

&nbsp; );



&nbsp; // Return the result with one of three actions:

&nbsp; return {

&nbsp;   action: 'accept', // or 'decline' or 'cancel'

&nbsp;   content: userInput, // only required when action is 'accept'

&nbsp; };

});

```

\### Elicitation Response Actions

Your handler must return an object with an `action` field that can be one of:

\- `'accept'`: User provided the requested information. Must include `content` with the data.

\- `'decline'`: User chose not to provide the information.

\- `'cancel'`: User cancelled the operation entirely.

\## Examples

You can see MCP in action in the following examples:

<ExampleLinks

&nbsp; examples={\[

&nbsp; {

&nbsp; title: 'Learn to use MCP tools in Node.js',

&nbsp; link: '/cookbook/node/mcp-tools',

&nbsp; },

&nbsp; {

&nbsp; title: 'Learn to handle MCP elicitation requests in Node.js',

&nbsp; link: '/cookbook/node/mcp-elicitation',

&nbsp; },

&nbsp; ]}

/>

---

title: Prompt Engineering

description: Learn how to develop prompts with AI SDK Core.

---

\# Prompt Engineering

\## Tips

\### Prompts for Tools

When you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.

Here are a few tips to help you get the best results:

1\. Use a model that is strong at tool calling, such as `gpt-5` or `gpt-4.1`. Weaker models will often struggle to call tools effectively and flawlessly.

1\. Keep the number of tools low, e.g. to 5 or less.

1\. Keep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.

1\. Use semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.

1\. Add `.describe("...")` to your Zod schema properties to give the model hints about what a particular property is for.

1\. When the output of a tool might be unclear to the model and there are dependencies between tools, use the `description` field of a tool to provide information about the output of the tool execution.

1\. You can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.

In general, the goal should be to give the model all information it needs in a clear way.

\### Tool \& Structured Data Schemas

The mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.

\#### Zod Dates

Zod expects JavaScript Date objects, but models return dates as strings.

You can specify and validate the date format using `z.string().datetime()` or `z.string().date()`,

and then use a Zod transformer to convert the string to a Date object.

```ts highlight="7-10"

const result = await generateObject({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; schema: z.object({

&nbsp;   events: z.array(

&nbsp;     z.object({

&nbsp;       event: z.string(),

&nbsp;       date: z

&nbsp;         .string()

&nbsp;         .date()

&nbsp;         .transform(value => new Date(value)),

&nbsp;     }),

&nbsp;   ),

&nbsp; }),

&nbsp; prompt: 'List 5 important events from the year 2000.',

});

```

\#### Optional Parameters

When working with tools that have optional parameters, you may encounter compatibility issues with certain providers that use strict schema validation.

<Note>

&nbsp; This is particularly relevant for OpenAI models with structured outputs

&nbsp; (strict mode).

</Note>

For maximum compatibility, optional parameters should use `.nullable()` instead of `.optional()`:

```ts highlight="6,7,16,17"

// This may fail with strict schema validation

const failingTool = tool({

&nbsp; description: 'Execute a command',

&nbsp; inputSchema: z.object({

&nbsp;   command: z.string(),

&nbsp;   workdir: z.string().optional(), // This can cause errors

&nbsp;   timeout: z.string().optional(),

&nbsp; }),

});



// This works with strict schema validation

const workingTool = tool({

&nbsp; description: 'Execute a command',

&nbsp; inputSchema: z.object({

&nbsp;   command: z.string(),

&nbsp;   workdir: z.string().nullable(), // Use nullable instead

&nbsp;   timeout: z.string().nullable(),

&nbsp; }),

});

```

\#### Temperature Settings

For tool calls and object generation, it's recommended to use `temperature: 0` to ensure deterministic and consistent results:

```ts highlight="3"

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; temperature: 0, // Recommended for tool calls

&nbsp; tools: {

&nbsp;   myTool: tool({

&nbsp;     description: 'Execute a command',

&nbsp;     inputSchema: z.object({

&nbsp;       command: z.string(),

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'Execute the ls command',

});

```

Lower temperature values reduce randomness in model outputs, which is particularly important when the model needs to:

\- Generate structured data with specific formats

\- Make precise tool calls with correct parameters

\- Follow strict schemas consistently

\## Debugging

\### Inspecting Warnings

Not all providers support all AI SDK features.

Providers either throw exceptions or return warnings when they do not support a feature.

To check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Hello, world!',

});



console.log(result.warnings);

```

\### HTTP Request Bodies

You can inspect the raw HTTP request bodies for models that expose them, e.g. \[OpenAI](/providers/ai-sdk-providers/openai).

This allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.

Request bodies are available via the `request.body` property of the response:

```ts highlight="6"

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Hello, world!',

});



console.log(result.request.body);

```

---

title: Settings

description: Learn how to configure the AI SDK.

---

\# Settings

Large language models (LLMs) typically provide settings to augment their output.

All AI SDK functions support the following common settings in addition to the model, the \[prompt](./prompts), and additional provider-specific settings:

```ts highlight="3-5"

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; maxOutputTokens: 512,

&nbsp; temperature: 0.3,

&nbsp; maxRetries: 5,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

});

```

<Note>

&nbsp; Some providers do not support all common settings. If you use a setting with a

&nbsp; provider that does not support it, a warning will be generated. You can check

&nbsp; the `warnings` property in the result object to see if any warnings were

&nbsp; generated.

</Note>

\### `maxOutputTokens`

Maximum number of tokens to generate.

\### `temperature`

Temperature setting.

The value is passed through to the provider. The range depends on the provider and model.

For most providers, `0` means almost deterministic results, and higher values mean more randomness.

It is recommended to set either `temperature` or `topP`, but not both.

<Note>In AI SDK 5.0, temperature is no longer set to `0` by default.</Note>

\### `topP`

Nucleus sampling.

The value is passed through to the provider. The range depends on the provider and model.

For most providers, nucleus sampling is a number between 0 and 1.

E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.

It is recommended to set either `temperature` or `topP`, but not both.

\### `topK`

Only sample from the top K options for each subsequent token.

Used to remove "long tail" low probability responses.

Recommended for advanced use cases only. You usually only need to use `temperature`.

\### `presencePenalty`

The presence penalty affects the likelihood of the model to repeat information that is already in the prompt.

The value is passed through to the provider. The range depends on the provider and model.

For most providers, `0` means no penalty.

\### `frequencyPenalty`

The frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.

The value is passed through to the provider. The range depends on the provider and model.

For most providers, `0` means no penalty.

\### `stopSequences`

The stop sequences to use for stopping the text generation.

If set, the model will stop generating text when one of the stop sequences is generated.

Providers may have limits on the number of stop sequences.

\### `seed`

It is the seed (integer) to use for random sampling.

If set and supported by the model, calls will generate deterministic results.

\### `maxRetries`

Maximum number of retries. Set to 0 to disable retries. Default: `2`.

\### `abortSignal`

An optional abort signal that can be used to cancel the call.

The abort signal can e.g. be forwarded from a user interface to cancel the call,

or to define a timeout using `AbortSignal.timeout`.

\#### Example: AbortSignal.timeout

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; abortSignal: AbortSignal.timeout(5000), // 5 seconds

});

```

\### `timeout`

An optional timeout in milliseconds. The call will be aborted if it takes longer than the specified duration.

This is a convenience parameter that creates an abort signal internally. It can be used alongside `abortSignal` - if both are provided, the call will abort when either condition is met.

You can specify the timeout either as a number (milliseconds) or as an object with `totalMs`, `stepMs`, and/or `chunkMs` properties:

\- `totalMs`: The total timeout for the entire call including all steps.

\- `stepMs`: The timeout for each individual step (LLM call). This is useful for multi-step generations where you want to limit the time spent on each step independently.

\- `chunkMs`: The timeout between stream chunks (streaming only). The call will abort if no new chunk is received within this duration. This is useful for detecting stalled streams.

\#### Example: 5 second timeout (number format)

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; timeout: 5000, // 5 seconds

});

```

\#### Example: 5 second total timeout (object format)

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; timeout: { totalMs: 5000 }, // 5 seconds

});

```

\#### Example: 10 second step timeout

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; timeout: { stepMs: 10000 }, // 10 seconds per step

});

```

\#### Example: Combined total and step timeout

```ts

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; timeout: {

&nbsp;   totalMs: 60000, // 60 seconds total

&nbsp;   stepMs: 10000, // 10 seconds per step

&nbsp; },

});

```

\#### Example: Per-chunk timeout for streaming (streamText only)

```ts

const result = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; timeout: { chunkMs: 5000 }, // abort if no chunk received for 5 seconds

});

```

\### `headers`

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.

You can use the request headers to provide additional information to the provider,

depending on what the provider supports. For example, some observability providers support

headers such as `Prompt-Id`.

```ts

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; headers: {

&nbsp;   'Prompt-Id': 'my-prompt-id',

&nbsp; },

});

```

<Note>

&nbsp; The `headers` setting is for request-specific headers. You can also set

&nbsp; `headers` in the provider configuration. These headers will be sent with every

&nbsp; request made by the provider.

</Note>

---

title: Embeddings

description: Learn how to embed values with the AI SDK.

---

\# Embeddings

Embeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space.

In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.

\## Embedding a Single Value

The AI SDK provides the \[`embed`](/docs/reference/ai-sdk-core/embed) function to embed single values, which is useful for tasks such as finding similar words

or phrases or clustering text.

You can use it with embeddings models, e.g. `openai.embeddingModel('text-embedding-3-large')` or `mistral.embeddingModel('mistral-embed')`.

```tsx

import { embed } from 'ai';

import { openai } from '@ai-sdk/openai';



// 'embedding' is a single embedding object (number\[])

const { embedding } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

});

```

\## Embedding Many Values

When loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG),

it is often useful to embed many values at once (batch embedding).

The AI SDK provides the \[`embedMany`](/docs/reference/ai-sdk-core/embed-many) function for this purpose.

Similar to `embed`, you can use it with embeddings models,

e.g. `openai.embeddingModel('text-embedding-3-large')` or `mistral.embeddingModel('mistral-embed')`.

```tsx

import { openai } from '@ai-sdk/openai';

import { embedMany } from 'ai';



// 'embeddings' is an array of embedding objects (number\[]\[]).

// It is sorted in the same order as the input values.

const { embeddings } = await embedMany({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; values: \[

&nbsp;   'sunny day at the beach',

&nbsp;   'rainy afternoon in the city',

&nbsp;   'snowy night in the mountains',

&nbsp; ],

});

```

\## Embedding Similarity

After embedding values, you can calculate the similarity between them using the \[`cosineSimilarity`](/docs/reference/ai-sdk-core/cosine-similarity) function.

This is useful to e.g. find similar words or phrases in a dataset.

You can also rank and filter related items based on their similarity.

```ts highlight={"2,10"}

import { openai } from '@ai-sdk/openai';

import { cosineSimilarity, embedMany } from 'ai';



const { embeddings } = await embedMany({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; values: \['sunny day at the beach', 'rainy afternoon in the city'],

});



console.log(

&nbsp; `cosine similarity: ${cosineSimilarity(embeddings\[0], embeddings\[1])}`,

);

```

\## Token Usage

Many providers charge based on the number of tokens used to generate embeddings.

Both `embed` and `embedMany` provide token usage information in the `usage` property of the result object:

```ts highlight={"4,9"}

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding, usage } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

});



console.log(usage); // { tokens: 10 }

```

\## Settings

\### Provider Options

Embedding model settings can be configured using `providerOptions` for provider-specific parameters:

```ts highlight={"5-9"}

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     dimensions: 512, // Reduce embedding dimensions

&nbsp;   },

&nbsp; },

});

```

\### Parallel Requests

The `embedMany` function now supports parallel processing with configurable `maxParallelCalls` to optimize performance:

```ts highlight={"4"}

import { openai } from '@ai-sdk/openai';

import { embedMany } from 'ai';



const { embeddings, usage } = await embedMany({

&nbsp; maxParallelCalls: 2, // Limit parallel requests

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; values: \[

&nbsp;   'sunny day at the beach',

&nbsp;   'rainy afternoon in the city',

&nbsp;   'snowy night in the mountains',

&nbsp; ],

});

```

\### Retries

Both `embed` and `embedMany` accept an optional `maxRetries` parameter of type `number`

that you can use to set the maximum number of retries for the embedding process.

It defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.

```ts highlight={"7"}

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

&nbsp; maxRetries: 0, // Disable retries

});

```

\### Abort Signals and Timeouts

Both `embed` and `embedMany` accept an optional `abortSignal` parameter of

type \[`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)

that you can use to abort the embedding process or set a timeout.

```ts highlight={"7"}

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

&nbsp; abortSignal: AbortSignal.timeout(1000), // Abort after 1 second

});

```

\### Custom Headers

Both `embed` and `embedMany` accept an optional `headers` parameter of type `Record<string, string>`

that you can use to add custom headers to the embedding request.

```ts highlight={"7"}

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

&nbsp; headers: { 'X-Custom-Header': 'custom-value' },

});

```

\## Response Information

Both `embed` and `embedMany` return response information that includes the raw provider response:

```ts highlight={"4,9"}

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding, response } = await embed({

&nbsp; model: 'openai/text-embedding-3-small',

&nbsp; value: 'sunny day at the beach',

});



console.log(response); // Raw provider response

```

\## Embedding Middleware

You can enhance embedding models, e.g. to set default values, using

`wrapEmbeddingModel` and `EmbeddingModelV3Middleware`.

Here is an example that uses the built-in `defaultEmbeddingSettingsMiddleware`:

```ts

import {

&nbsp; customProvider,

&nbsp; defaultEmbeddingSettingsMiddleware,

&nbsp; embed,

&nbsp; wrapEmbeddingModel,

&nbsp; gateway,

} from 'ai';



const embeddingModelWithDefaults = wrapEmbeddingModel({

&nbsp; model: gateway.embeddingModel('google/gemini-embedding-001'),

&nbsp; middleware: defaultEmbeddingSettingsMiddleware({

&nbsp;   settings: {

&nbsp;     providerOptions: {

&nbsp;       google: {

&nbsp;         outputDimensionality: 256,

&nbsp;         taskType: 'CLASSIFICATION',

&nbsp;       },

&nbsp;     },

&nbsp;   },

&nbsp; }),

});

```

\## Embedding Providers \& Models

Several providers offer embedding models:

| Provider | Model | Embedding Dimensions |

| ----------------------------------------------------------------------------------------- | ------------------------------- | -------------------- |

| \[OpenAI](/providers/ai-sdk-providers/openai#embedding-models) | `text-embedding-3-large` | 3072 |

| \[OpenAI](/providers/ai-sdk-providers/openai#embedding-models) | `text-embedding-3-small` | 1536 |

| \[OpenAI](/providers/ai-sdk-providers/openai#embedding-models) | `text-embedding-ada-002` | 1536 |

| \[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `gemini-embedding-001` | 3072 |

| \[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `text-embedding-004` | 768 |

| \[Mistral](/providers/ai-sdk-providers/mistral#embedding-models) | `mistral-embed` | 1024 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-v3.0` | 1024 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-multilingual-v3.0` | 1024 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-light-v3.0` | 384 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-multilingual-light-v3.0` | 384 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-v2.0` | 4096 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-light-v2.0` | 1024 |

| \[Cohere](/providers/ai-sdk-providers/cohere#embedding-models) | `embed-multilingual-v2.0` | 768 |

| \[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models) | `amazon.titan-embed-text-v1` | 1536 |

| \[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models) | `amazon.titan-embed-text-v2:0` | 1024 |

---

title: Reranking

description: Learn how to rerank documents with the AI SDK.

---

\# Reranking

Reranking is a technique used to improve search relevance by reordering a set of documents based on their relevance to a query.

Unlike embedding-based similarity search, reranking models are specifically trained to understand the relationship between queries and documents,

often producing more accurate relevance scores.

\## Reranking Documents

The AI SDK provides the \[`rerank`](/docs/reference/ai-sdk-core/rerank) function to rerank documents based on their relevance to a query.

You can use it with reranking models, e.g. `cohere.reranking('rerank-v3.5')` or `bedrock.reranking('cohere.rerank-v3-5:0')`.

```tsx

import { rerank } from 'ai';

import { cohere } from '@ai-sdk/cohere';



const documents = \[

&nbsp; 'sunny day at the beach',

&nbsp; 'rainy afternoon in the city',

&nbsp; 'snowy night in the mountains',

];



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents,

&nbsp; query: 'talk about rain',

&nbsp; topN: 2, // Return top 2 most relevant documents

});



console.log(ranking);

// \[

//   { originalIndex: 1, score: 0.9, document: 'rainy afternoon in the city' },

//   { originalIndex: 0, score: 0.3, document: 'sunny day at the beach' }

// ]

```

\## Working with Object Documents

Reranking also supports structured documents (JSON objects), making it ideal for searching through databases, emails, or other structured content:

```tsx

import { rerank } from 'ai';

import { cohere } from '@ai-sdk/cohere';



const documents = \[

&nbsp; {

&nbsp;   from: 'Paul Doe',

&nbsp;   subject: 'Follow-up',

&nbsp;   text: 'We are happy to give you a discount of 20% on your next order.',

&nbsp; },

&nbsp; {

&nbsp;   from: 'John McGill',

&nbsp;   subject: 'Missing Info',

&nbsp;   text: 'Sorry, but here is the pricing information from Oracle: $5000/month',

&nbsp; },

];



const { ranking, rerankedDocuments } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents,

&nbsp; query: 'Which pricing did we get from Oracle?',

&nbsp; topN: 1,

});



console.log(rerankedDocuments\[0]);

// { from: 'John McGill', subject: 'Missing Info', text: '...' }

```

\## Understanding the Results

The `rerank` function returns a comprehensive result object:

```ts

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking, rerankedDocuments, originalDocuments } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

});



// ranking: sorted array of { originalIndex, score, document }

// rerankedDocuments: documents sorted by relevance (convenience property)

// originalDocuments: original documents array

```

Each item in the `ranking` array contains:

\- `originalIndex`: Position in the original documents array

\- `score`: Relevance score (typically 0-1, where higher is more relevant)

\- `document`: The original document

\## Settings

\### Top-N Results

Use `topN` to limit the number of results returned. This is useful for retrieving only the most relevant documents:

```ts highlight={"7"}

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['doc1', 'doc2', 'doc3', 'doc4', 'doc5'],

&nbsp; query: 'relevant information',

&nbsp; topN: 3, // Return only top 3 most relevant documents

});

```

\### Provider Options

Reranking model settings can be configured using `providerOptions` for provider-specific parameters:

```ts highlight={"8-12"}

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

&nbsp; providerOptions: {

&nbsp;   cohere: {

&nbsp;     maxTokensPerDoc: 1000, // Limit tokens per document

&nbsp;   },

&nbsp; },

});

```

\### Retries

The `rerank` function accepts an optional `maxRetries` parameter of type `number`

that you can use to set the maximum number of retries for the reranking process.

It defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.

```ts highlight={"7"}

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

&nbsp; maxRetries: 0, // Disable retries

});

```

\### Abort Signals and Timeouts

The `rerank` function accepts an optional `abortSignal` parameter of

type \[`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)

that you can use to abort the reranking process or set a timeout.

```ts highlight={"7"}

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

&nbsp; abortSignal: AbortSignal.timeout(5000), // Abort after 5 seconds

});

```

\### Custom Headers

The `rerank` function accepts an optional `headers` parameter of type `Record<string, string>`

that you can use to add custom headers to the reranking request.

```ts highlight={"7"}

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

&nbsp; headers: { 'X-Custom-Header': 'custom-value' },

});

```

\## Response Information

The `rerank` function returns response information that includes the raw provider response:

```ts highlight={"4,10"}

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking, response } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

});



console.log(response); // { id, timestamp, modelId, headers, body }

```

\## Reranking Providers \& Models

Several providers offer reranking models:

| Provider | Model |

| ----------------------------------------------------------------------------- | ------------------------------------- |

| \[Cohere](/providers/ai-sdk-providers/cohere#reranking-models) | `rerank-v3.5` |

| \[Cohere](/providers/ai-sdk-providers/cohere#reranking-models) | `rerank-english-v3.0` |

| \[Cohere](/providers/ai-sdk-providers/cohere#reranking-models) | `rerank-multilingual-v3.0` |

| \[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `amazon.rerank-v1:0` |

| \[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `cohere.rerank-v3-5:0` |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models) | `Salesforce/Llama-Rank-v1` |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models) | `mixedbread-ai/Mxbai-Rerank-Large-V2` |

---

title: Image Generation

description: Learn how to generate images with the AI SDK.

---

\# Image Generation

The AI SDK provides the \[`generateImage`](/docs/reference/ai-sdk-core/generate-image)

function to generate images based on a given prompt using an image model.

```tsx

import { generateImage } from 'ai';

import { openai } from '@ai-sdk/openai';



const { image } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

});

```

You can access the image data using the `base64` or `uint8Array` properties:

```tsx
const base64 = image.base64; // base64 image data

const uint8Array = image.uint8Array; // Uint8Array image data
```

\## Settings

\### Size and Aspect Ratio

Depending on the model, you can either specify the size or the aspect ratio.

\##### Size

The size is specified as a string in the format `{width}x{height}`.

Models only support a few sizes, and the supported sizes are different for each model and provider.

```tsx highlight={"7"}

import { generateImage } from 'ai';

import { openai } from '@ai-sdk/openai';



const { image } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; size: '1024x1024',

});

```

\##### Aspect Ratio

The aspect ratio is specified as a string in the format `{width}:{height}`.

Models only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.

```tsx highlight={"7"}

import { generateImage } from 'ai';

import { vertex } from '@ai-sdk/google-vertex';



const { image } = await generateImage({

&nbsp; model: vertex.image('imagen-4.0-generate-001'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; aspectRatio: '16:9',

});

```

\### Generating Multiple Images

`generateImage` also supports generating multiple images at once:

```tsx highlight={"7"}

import { generateImage } from 'ai';

import { openai } from '@ai-sdk/openai';



const { images } = await generateImage({

&nbsp; model: openai.image('dall-e-2'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; n: 4, // number of images to generate

});

```

<Note>

&nbsp; `generateImage` will automatically call the model as often as needed (in

&nbsp; parallel) to generate the requested number of images.

</Note>

Each image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the `n` parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).

If needed, you can override this behavior using the `maxImagesPerCall` setting when generating your image. This is particularly useful when working with new or custom models where the default batch size might not be optimal:

```tsx

const { images } = await generateImage({

&nbsp; model: openai.image('dall-e-2'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; maxImagesPerCall: 5, // Override the default batch size

&nbsp; n: 10, // Will make 2 calls of 5 images each

});

```

\### Providing a Seed

You can provide a seed to the `generateImage` function to control the output of the image generation process.

If supported by the model, the same seed will always produce the same image.

```tsx highlight={"7"}

import { generateImage } from 'ai';

import { openai } from '@ai-sdk/openai';



const { image } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; seed: 1234567890,

});

```

\### Provider-specific Settings

Image models often have provider- or even model-specific settings.

You can pass such settings to the `generateImage` function

using the `providerOptions` parameter. The options for the provider

(`openai` in the example below) become request body properties.

```tsx highlight={"9"}

import { generateImage } from 'ai';

import { openai } from '@ai-sdk/openai';



const { image } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; size: '1024x1024',

&nbsp; providerOptions: {

&nbsp;   openai: { style: 'vivid', quality: 'hd' },

&nbsp; },

});

```

\### Abort Signals and Timeouts

`generateImage` accepts an optional `abortSignal` parameter of

type \[`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)

that you can use to abort the image generation process or set a timeout.

```ts highlight={"7"}

import { openai } from '@ai-sdk/openai';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; abortSignal: AbortSignal.timeout(1000), // Abort after 1 second

});

```

\### Custom Headers

`generateImage` accepts an optional `headers` parameter of type `Record<string, string>`

that you can use to add custom headers to the image generation request.

```ts highlight={"7"}

import { openai } from '@ai-sdk/openai';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

&nbsp; headers: { 'X-Custom-Header': 'custom-value' },

});

```

\### Warnings

If the model returns warnings, e.g. for unsupported parameters, they will be available in the `warnings` property of the response.

```tsx

const { image, warnings } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt: 'Santa Claus driving a Cadillac',

});

```

\### Additional provider-specific meta data

Some providers expose additional meta data for the result overall or per image.

```tsx

const prompt = 'Santa Claus driving a Cadillac';



const { image, providerMetadata } = await generateImage({

&nbsp; model: openai.image('dall-e-3'),

&nbsp; prompt,

});



const revisedPrompt = providerMetadata.openai.images\[0]?.revisedPrompt;



console.log({

&nbsp; prompt,

&nbsp; revisedPrompt,

});

```

The outer key of the returned `providerMetadata` is the provider name. The inner values are the metadata. An `images` key is always present in the metadata and is an array with the same length as the top level `images` key.

\### Error Handling

When `generateImage` cannot generate a valid image, it throws a \[`AI\_NoImageGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-image-generated-error).

This error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:

\- The model failed to generate a response

\- The model generated a response that could not be parsed

The error preserves the following information to help you log the issue:

\- `responses`: Metadata about the image model responses, including timestamp, model, and headers.

\- `cause`: The cause of the error. You can use this for more detailed error handling

```ts

import { generateImage, NoImageGeneratedError } from 'ai';



try {

&nbsp; await generateImage({ model, prompt });

} catch (error) {

&nbsp; if (NoImageGeneratedError.isInstance(error)) {

&nbsp;   console.log('NoImageGeneratedError');

&nbsp;   console.log('Cause:', error.cause);

&nbsp;   console.log('Responses:', error.responses);

&nbsp; }

}

```

\## Image Middleware

You can enhance image models, e.g. to set default values or implement logging, using

`wrapImageModel` and `ImageModelV3Middleware`.

Here is an example that sets a default size when none is provided:

```ts

import { generateImage, wrapImageModel } from 'ai';

import { openai } from '@ai-sdk/openai';



const model = wrapImageModel({

&nbsp; model: openai.image('gpt-image-1'),

&nbsp; middleware: {

&nbsp;   specificationVersion: 'v3',

&nbsp;   transformParams: async ({ params }) => ({

&nbsp;     ...params,

&nbsp;     size: params.size ?? '1024x1024',

&nbsp;   }),

&nbsp; },

});



const { image } = await generateImage({

&nbsp; model,

&nbsp; prompt: 'Santa Claus driving a Cadillac',

});

```

\## Generating Images with Language Models

Some language models such as Google `gemini-2.5-flash-image-preview` support multi-modal outputs including images.

With such models, you can access the generated images using the `files` property of the response.

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: google('gemini-2.5-flash-image-preview'),

&nbsp; prompt: 'Generate an image of a comic cat',

});



for (const file of result.files) {

&nbsp; if (file.mediaType.startsWith('image/')) {

&nbsp;   // The file object provides multiple data formats:

&nbsp;   // Access images as base64 string, Uint8Array binary data, or check type

&nbsp;   // - file.base64: string (data URL format)

&nbsp;   // - file.uint8Array: Uint8Array (binary data)

&nbsp;   // - file.mediaType: string (e.g. "image/png")

&nbsp; }

}

```

\## Image Models

| Provider | Model | Support sizes (`width x height`) or aspect ratios (`width : height`) |

| ------------------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| \[xAI Grok](/providers/ai-sdk-providers/xai#image-models) | `grok-2-image` | 1024x768 (default) |

| \[OpenAI](/providers/ai-sdk-providers/openai#image-models) | `gpt-image-1` | 1024x1024, 1536x1024, 1024x1536 |

| \[OpenAI](/providers/ai-sdk-providers/openai#image-models) | `dall-e-3` | 1024x1024, 1792x1024, 1024x1792 |

| \[OpenAI](/providers/ai-sdk-providers/openai#image-models) | `dall-e-2` | 256x256, 512x512, 1024x1024 |

| \[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#image-models) | `amazon.nova-canvas-v1:0` | 320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/flux/dev` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/flux-lora` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/fast-sdxl` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/flux-pro/v1.1-ultra` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/ideogram/v2` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/recraft-v3` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/stable-diffusion-3.5-large` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Fal](/providers/ai-sdk-providers/fal#image-models) | `fal-ai/hyper-sdxl` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `stabilityai/sd3.5` | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21 |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-1.1-pro` | 256-1440 (multiples of 32) |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-1-schnell` | 256-1440 (multiples of 32) |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-1-dev` | 256-1440 (multiples of 32) |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-pro` | 256-1440 (multiples of 32) |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `stabilityai/sd3.5-medium` | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21 |

| \[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models) | `stabilityai/sdxl-turbo` | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21 |

| \[Replicate](/providers/ai-sdk-providers/replicate) | `black-forest-labs/flux-schnell` | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9 |

| \[Replicate](/providers/ai-sdk-providers/replicate) | `recraft-ai/recraft-v3` | 1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024 |

| \[Google](/providers/ai-sdk-providers/google#image-models) | `imagen-4.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Google](/providers/ai-sdk-providers/google#image-models) | `imagen-4.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Google](/providers/ai-sdk-providers/google#image-models) | `imagen-4.0-ultra-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-4.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-4.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-4.0-ultra-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-3.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/flux-1-dev-fp8` | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/flux-1-schnell-fp8` | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/playground-v2-5-1024px-aesthetic` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/japanese-stable-diffusion-xl` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/playground-v2-1024px-aesthetic` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/SSD-1B` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |

| \[Fireworks](/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/stable-diffusion-xl-1024-v1-0` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |

| \[Luma](/providers/ai-sdk-providers/luma#image-models) | `photon-1` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Luma](/providers/ai-sdk-providers/luma#image-models) | `photon-flash-1` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `stabilityai/stable-diffusion-xl-base-1.0` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-dev` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-dev-lora` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-schnell` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-canny` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-depth` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-redux` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1.1-pro` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-pro` | 512x512, 768x768, 1024x1024 |

| \[Together.ai](/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-schnell-Free` | 512x512, 768x768, 1024x1024 |

| \[Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-pro` | From 3:7 (portrait) to 7:3 (landscape) |

| \[Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-max` | From 3:7 (portrait) to 7:3 (landscape) |

| \[Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1-ultra` | From 3:7 (portrait) to 7:3 (landscape) |

| \[Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1` | From 3:7 (portrait) to 7:3 (landscape) |

| \[Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.0-fill` | From 3:7 (portrait) to 7:3 (landscape) |

Above are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.

---

title: Transcription

description: Learn how to transcribe audio with the AI SDK.

---

\# Transcription

<Note type="warning">Transcription is an experimental feature.</Note>

The AI SDK provides the \[`transcribe`](/docs/reference/ai-sdk-core/transcribe)

function to transcribe audio using a transcription model.

```ts

import { experimental\_transcribe as transcribe } from 'ai';

import { openai } from '@ai-sdk/openai';

import { readFile } from 'fs/promises';



const transcript = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: await readFile('audio.mp3'),

});

```

The `audio` property can be a `Uint8Array`, `ArrayBuffer`, `Buffer`, `string` (base64 encoded audio data), or a `URL`.

To access the generated transcript:

```ts
const text = transcript.text; // transcript text e.g. "Hello, world!"

const segments = transcript.segments; // array of segments with start and end times, if available

const language = transcript.language; // language of the transcript e.g. "en", if available

const durationInSeconds = transcript.durationInSeconds; // duration of the transcript in seconds, if available
```

\## Settings

\### Provider-Specific settings

Transcription models often have provider or model-specific settings which you can set using the `providerOptions` parameter.

```ts highlight="8-12"

import { experimental\_transcribe as transcribe } from 'ai';

import { openai } from '@ai-sdk/openai';

import { readFile } from 'fs/promises';



const transcript = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     timestampGranularities: \['word'],

&nbsp;   },

&nbsp; },

});

```

\### Abort Signals and Timeouts

`transcribe` accepts an optional `abortSignal` parameter of

type \[`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)

that you can use to abort the transcription process or set a timeout.

```ts highlight="8"

import { openai } from '@ai-sdk/openai';

import { experimental\_transcribe as transcribe } from 'ai';

import { readFile } from 'fs/promises';



const transcript = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; abortSignal: AbortSignal.timeout(1000), // Abort after 1 second

});

```

\### Custom Headers

`transcribe` accepts an optional `headers` parameter of type `Record<string, string>`

that you can use to add custom headers to the transcription request.

```ts highlight="8"

import { openai } from '@ai-sdk/openai';

import { experimental\_transcribe as transcribe } from 'ai';

import { readFile } from 'fs/promises';



const transcript = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; headers: { 'X-Custom-Header': 'custom-value' },

});

```

\### Warnings

Warnings (e.g. unsupported parameters) are available on the `warnings` property.

```ts

import { openai } from '@ai-sdk/openai';

import { experimental\_transcribe as transcribe } from 'ai';

import { readFile } from 'fs/promises';



const transcript = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: await readFile('audio.mp3'),

});



const warnings = transcript.warnings;

```

\### Error Handling

When `transcribe` cannot generate a valid transcript, it throws a \[`AI\_NoTranscriptGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error).

This error can arise for any the following reasons:

\- The model failed to generate a response

\- The model generated a response that could not be parsed

The error preserves the following information to help you log the issue:

\- `responses`: Metadata about the transcription model responses, including timestamp, model, and headers.

\- `cause`: The cause of the error. You can use this for more detailed error handling.

```ts

import {

&nbsp; experimental\_transcribe as transcribe,

&nbsp; NoTranscriptGeneratedError,

} from 'ai';

import { openai } from '@ai-sdk/openai';

import { readFile } from 'fs/promises';



try {

&nbsp; await transcribe({

&nbsp;   model: openai.transcription('whisper-1'),

&nbsp;   audio: await readFile('audio.mp3'),

&nbsp; });

} catch (error) {

&nbsp; if (NoTranscriptGeneratedError.isInstance(error)) {

&nbsp;   console.log('NoTranscriptGeneratedError');

&nbsp;   console.log('Cause:', error.cause);

&nbsp;   console.log('Responses:', error.responses);

&nbsp; }

}

```

\## Transcription Models

| Provider | Model |

| ------------------------------------------------------------------------- | ------------------------ |

| \[OpenAI](/providers/ai-sdk-providers/openai#transcription-models) | `whisper-1` |

| \[OpenAI](/providers/ai-sdk-providers/openai#transcription-models) | `gpt-4o-transcribe` |

| \[OpenAI](/providers/ai-sdk-providers/openai#transcription-models) | `gpt-4o-mini-transcribe` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe\_v1` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe\_v1\_experimental` |

| \[Groq](/providers/ai-sdk-providers/groq#transcription-models) | `whisper-large-v3-turbo` |

| \[Groq](/providers/ai-sdk-providers/groq#transcription-models) | `whisper-large-v3` |

| \[Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models) | `whisper-1` |

| \[Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models) | `gpt-4o-transcribe` |

| \[Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models) | `gpt-4o-mini-transcribe` |

| \[Rev.ai](/providers/ai-sdk-providers/revai#transcription-models) | `machine` |

| \[Rev.ai](/providers/ai-sdk-providers/revai#transcription-models) | `low\_cost` |

| \[Rev.ai](/providers/ai-sdk-providers/revai#transcription-models) | `fusion` |

| \[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models) | `base` (+ variants) |

| \[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models) | `enhanced` (+ variants) |

| \[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models) | `nova` (+ variants) |

| \[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models) | `nova-2` (+ variants) |

| \[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models) | `nova-3` (+ variants) |

| \[Gladia](/providers/ai-sdk-providers/gladia#transcription-models) | `default` |

| \[AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models) | `best` |

| \[AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models) | `nano` |

| \[Fal](/providers/ai-sdk-providers/fal#transcription-models) | `whisper` |

| \[Fal](/providers/ai-sdk-providers/fal#transcription-models) | `wizper` |

Above are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.

---

title: Speech

description: Learn how to generate speech from text with the AI SDK.

---

\# Speech

<Note type="warning">Speech is an experimental feature.</Note>

The AI SDK provides the \[`generateSpeech`](/docs/reference/ai-sdk-core/generate-speech)

function to generate speech from text using a speech model.

```ts

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { openai } from '@ai-sdk/openai';



const audio = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

&nbsp; voice: 'alloy',

});

```

\### Language Setting

You can specify the language for speech generation (provider support varies):

```ts

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { lmnt } from '@ai-sdk/lmnt';



const audio = await generateSpeech({

&nbsp; model: lmnt.speech('aurora'),

&nbsp; text: 'Hola, mundo!',

&nbsp; language: 'es', // Spanish

});

```

To access the generated audio:

```ts
const audio = audio.audioData; // audio data e.g. Uint8Array
```

\## Settings

\### Provider-Specific settings

You can set model-specific settings with the `providerOptions` parameter.

```ts highlight="7-11"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { openai } from '@ai-sdk/openai';



const audio = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     // ...

&nbsp;   },

&nbsp; },

});

```

\### Abort Signals and Timeouts

`generateSpeech` accepts an optional `abortSignal` parameter of

type \[`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)

that you can use to abort the speech generation process or set a timeout.

```ts highlight="7"

import { openai } from '@ai-sdk/openai';

import { experimental\_generateSpeech as generateSpeech } from 'ai';



const audio = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

&nbsp; abortSignal: AbortSignal.timeout(1000), // Abort after 1 second

});

```

\### Custom Headers

`generateSpeech` accepts an optional `headers` parameter of type `Record<string, string>`

that you can use to add custom headers to the speech generation request.

```ts highlight="7"

import { openai } from '@ai-sdk/openai';

import { experimental\_generateSpeech as generateSpeech } from 'ai';



const audio = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

&nbsp; headers: { 'X-Custom-Header': 'custom-value' },

});

```

\### Warnings

Warnings (e.g. unsupported parameters) are available on the `warnings` property.

```ts

import { openai } from '@ai-sdk/openai';

import { experimental\_generateSpeech as generateSpeech } from 'ai';



const audio = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

});



const warnings = audio.warnings;

```

\### Error Handling

When `generateSpeech` cannot generate a valid audio, it throws a \[`AI\_NoSpeechGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-speech-generated-error).

This error can arise for any the following reasons:

\- The model failed to generate a response

\- The model generated a response that could not be parsed

The error preserves the following information to help you log the issue:

\- `responses`: Metadata about the speech model responses, including timestamp, model, and headers.

\- `cause`: The cause of the error. You can use this for more detailed error handling.

```ts

import {

&nbsp; experimental\_generateSpeech as generateSpeech,

&nbsp; NoSpeechGeneratedError,

} from 'ai';

import { openai } from '@ai-sdk/openai';



try {

&nbsp; await generateSpeech({

&nbsp;   model: openai.speech('tts-1'),

&nbsp;   text: 'Hello, world!',

&nbsp; });

} catch (error) {

&nbsp; if (NoSpeechGeneratedError.isInstance(error)) {

&nbsp;   console.log('AI\_NoSpeechGeneratedError');

&nbsp;   console.log('Cause:', error.cause);

&nbsp;   console.log('Responses:', error.responses);

&nbsp; }

}

```

\## Speech Models

| Provider | Model |

| ------------------------------------------------------------------ | ------------------------ |

| \[OpenAI](/providers/ai-sdk-providers/openai#speech-models) | `tts-1` |

| \[OpenAI](/providers/ai-sdk-providers/openai#speech-models) | `tts-1-hd` |

| \[OpenAI](/providers/ai-sdk-providers/openai#speech-models) | `gpt-4o-mini-tts` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven\_v3` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven\_multilingual\_v2` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven\_flash\_v2\_5` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven\_flash\_v2` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven\_turbo\_v2\_5` |

| \[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven\_turbo\_v2` |

| \[LMNT](/providers/ai-sdk-providers/lmnt#speech-models) | `aurora` |

| \[LMNT](/providers/ai-sdk-providers/lmnt#speech-models) | `blizzard` |

| \[Hume](/providers/ai-sdk-providers/hume#speech-models) | `default` |

Above are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.

---

title: Language Model Middleware

description: Learn how to use middleware to enhance the behavior of language models

---

\# Language Model Middleware

Language model middleware is a way to enhance the behavior of language models

by intercepting and modifying the calls to the language model.

It can be used to add features like guardrails, RAG, caching, and logging

in a language model agnostic way. Such middleware can be developed and

distributed independently from the language models that they are applied to.

\## Using Language Model Middleware

You can use language model middleware with the `wrapLanguageModel` function.

It takes a language model and a language model middleware and returns a new

language model that incorporates the middleware.

```ts

import { wrapLanguageModel } from 'ai';



const wrappedLanguageModel = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: yourLanguageModelMiddleware,

});

```

The wrapped language model can be used just like any other language model, e.g. in `streamText`:

```ts highlight="2"

const result = streamText({

&nbsp; model: wrappedLanguageModel,

&nbsp; prompt: 'What cities are in the United States?',

});

```

\## Multiple middlewares

You can provide multiple middlewares to the `wrapLanguageModel` function.

The middlewares will be applied in the order they are provided.

```ts

const wrappedLanguageModel = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: \[firstMiddleware, secondMiddleware],

});



// applied as: firstMiddleware(secondMiddleware(yourModel))

```

\## Built-in Middleware

The AI SDK comes with several built-in middlewares that you can use to configure language models:

\- `extractReasoningMiddleware`: Extracts reasoning information from the generated text and exposes it as a `reasoning` property on the result.

\- `extractJsonMiddleware`: Extracts JSON from text content by stripping markdown code fences. Useful when using `Output.object()` with models that wrap JSON responses in code blocks.

\- `simulateStreamingMiddleware`: Simulates streaming behavior with responses from non-streaming language models.

\- `defaultSettingsMiddleware`: Applies default settings to a language model.

\- `addToolInputExamplesMiddleware`: Adds tool input examples to tool descriptions for providers that don't natively support the `inputExamples` property.

\### Extract Reasoning

Some providers and models expose reasoning information in the generated text using special tags,

e.g. \&lt;think\&gt; and \&lt;/think\&gt;.

The `extractReasoningMiddleware` function can be used to extract this reasoning information and expose it as a `reasoning` property on the result.

```ts

import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';



const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: extractReasoningMiddleware({ tagName: 'think' }),

});

```

You can then use that enhanced model in functions like `generateText` and `streamText`.

The `extractReasoningMiddleware` function also includes a `startWithReasoning` option.

When set to `true`, the reasoning tag will be prepended to the generated text.

This is useful for models that do not include the reasoning tag at the beginning of the response.

For more details, see the \[DeepSeek R1 guide](/docs/guides/r1#deepseek-r1-middleware).

\### Extract JSON

Some models wrap JSON responses in markdown code fences (e.g., ` ```json ... ``` `) even when you request structured output.

The `extractJsonMiddleware` function strips these code fences from the response, making it compatible with `Output.object()`.

```ts

import { wrapLanguageModel, extractJsonMiddleware, Output } from 'ai';

import { z } from 'zod';



const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: extractJsonMiddleware(),

});



const result = await generateText({

&nbsp; model,

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a recipe.',

});

```

You can also provide a custom transform function for models that use different formatting:

```ts

const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: extractJsonMiddleware({

&nbsp;   transform: text => text.replace(/^PREFIX/, '').replace(/SUFFIX$/, ''),

&nbsp; }),

});

```

\### Simulate Streaming

The `simulateStreamingMiddleware` function can be used to simulate streaming behavior with responses from non-streaming language models.

This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.

```ts

import { wrapLanguageModel, simulateStreamingMiddleware } from 'ai';



const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: simulateStreamingMiddleware(),

});

```

\### Default Settings

The `defaultSettingsMiddleware` function can be used to apply default settings to a language model.

```ts

import { wrapLanguageModel, defaultSettingsMiddleware } from 'ai';



const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: defaultSettingsMiddleware({

&nbsp;   settings: {

&nbsp;     temperature: 0.5,

&nbsp;     maxOutputTokens: 800,

&nbsp;     providerOptions: { openai: { store: false } },

&nbsp;   },

&nbsp; }),

});

```

\### Add Tool Input Examples

The `addToolInputExamplesMiddleware` function adds tool input examples to tool descriptions.

This is useful for providers that don't natively support the `inputExamples` property on tools.

The middleware serializes the examples into the tool's description text so models can still benefit from seeing example inputs.

```ts

import { wrapLanguageModel, addToolInputExamplesMiddleware } from 'ai';



const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: addToolInputExamplesMiddleware({

&nbsp;   examplesPrefix: 'Input Examples:',

&nbsp; }),

});

```

When you define a tool with `inputExamples`, the middleware will append them to the tool's description:

```ts

import { generateText, tool } from 'ai';

import { z } from 'zod';



const result = await generateText({

&nbsp; model, // wrapped model from above

&nbsp; tools: {

&nbsp;   weather: tool({

&nbsp;     description: 'Get the weather in a location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string(),

&nbsp;     }),

&nbsp;     inputExamples: \[

&nbsp;       { input: { location: 'San Francisco' } },

&nbsp;       { input: { location: 'London' } },

&nbsp;     ],

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'What is the weather in Tokyo?',

});

```

The tool description will be transformed to:

```

Get the weather in a location



Input Examples:

{"location":"San Francisco"}

{"location":"London"}

```

\#### Options

\- `examplesPrefix` (required): A prefix text to prepend before the examples.

\- `formatExample` (optional): A custom formatter function for each example. Receives the example object and its index. Default: `JSON.stringify(example.input)`.

\- `removeInputExamples` (optional): Whether to remove the `inputExamples` property from the tool after adding them to the description. Default: `true`.

```ts

const model = wrapLanguageModel({

&nbsp; model: yourModel,

&nbsp; middleware: addToolInputExamplesMiddleware({

&nbsp;   examplesPrefix: 'Input Examples:',

&nbsp;   formatExample: (example, index) =>

&nbsp;     `${index + 1}. ${JSON.stringify(example.input)}`,

&nbsp;   removeInputExamples: true,

&nbsp; }),

});

```

\## Community Middleware

The AI SDK provides a Language Model Middleware specification. Community members can develop middleware that adheres to this specification, making it compatible with the AI SDK ecosystem.

Here are some community middlewares that you can explore:

\### Custom tool call parser

The \[Custom tool call parser](https://github.com/minpeter/ai-sdk-tool-call-middleware) middleware extends tool call capabilities to models that don't natively support the OpenAI-style `tools` parameter. This includes many self-hosted and third-party models that lack native function calling features.

<Note>

&nbsp; Using this middleware on models that support native function calls may result

&nbsp; in unintended performance degradation, so check whether your model supports

&nbsp; native function calls before deciding to use it.

</Note>

This middleware enables function calling capabilities by converting function schemas into prompt instructions and parsing the model's responses into structured function calls. It works by transforming the JSON function definitions into natural language instructions the model can understand, then analyzing the generated text to extract function call attempts. This approach allows developers to use the same function calling API across different model providers, even with models that don't natively support the OpenAI-style function calling format, providing a consistent function calling experience regardless of the underlying model implementation.

The `@ai-sdk-tool/parser` package offers three middleware variants:

\- `createToolMiddleware`: A flexible function for creating custom tool call middleware tailored to specific models

\- `hermesToolMiddleware`: Ready-to-use middleware for Hermes \& Qwen format function calls

\- `gemmaToolMiddleware`: Pre-configured middleware for Gemma 3 model series function call format

Here's how you can enable function calls with Gemma models that don't support them natively:

```ts

import { wrapLanguageModel } from 'ai';

import { gemmaToolMiddleware } from '@ai-sdk-tool/parser';



const model = wrapLanguageModel({

&nbsp; model: openrouter('google/gemma-3-27b-it'),

&nbsp; middleware: gemmaToolMiddleware,

});

```

Find more examples at this \[link](https://github.com/minpeter/ai-sdk-tool-call-middleware/tree/main/examples/core/src).

\## Implementing Language Model Middleware

<Note>

&nbsp; Implementing language model middleware is advanced functionality and requires

&nbsp; a solid understanding of the \[language model

&nbsp; specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).

</Note>

You can implement any of the following three function to modify the behavior of the language model:

1\. `transformParams`: Transforms the parameters before they are passed to the language model, for both `doGenerate` and `doStream`.

2\. `wrapGenerate`: Wraps the `doGenerate` method of the \[language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).

&nbsp; You can modify the parameters, call the language model, and modify the result.

3\. `wrapStream`: Wraps the `doStream` method of the \[language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).

&nbsp; You can modify the parameters, call the language model, and modify the result.

Here are some examples of how to implement language model middleware:

\## Examples

<Note>

&nbsp; These examples are not meant to be used in production. They are just to show

&nbsp; how you can use middleware to enhance the behavior of language models.

</Note>

\### Logging

This example shows how to log the parameters and generated text of a language model call.

```ts

import type {

&nbsp; LanguageModelV3Middleware,

&nbsp; LanguageModelV3StreamPart,

} from '@ai-sdk/provider';



export const yourLogMiddleware: LanguageModelV3Middleware = {

&nbsp; wrapGenerate: async ({ doGenerate, params }) => {

&nbsp;   console.log('doGenerate called');

&nbsp;   console.log(`params: ${JSON.stringify(params, null, 2)}`);



&nbsp;   const result = await doGenerate();



&nbsp;   console.log('doGenerate finished');

&nbsp;   console.log(`generated text: ${result.text}`);



&nbsp;   return result;

&nbsp; },



&nbsp; wrapStream: async ({ doStream, params }) => {

&nbsp;   console.log('doStream called');

&nbsp;   console.log(`params: ${JSON.stringify(params, null, 2)}`);



&nbsp;   const { stream, ...rest } = await doStream();



&nbsp;   let generatedText = '';

&nbsp;   const textBlocks = new Map<string, string>();



&nbsp;   const transformStream = new TransformStream<

&nbsp;     LanguageModelV3StreamPart,

&nbsp;     LanguageModelV3StreamPart

&nbsp;   >({

&nbsp;     transform(chunk, controller) {

&nbsp;       switch (chunk.type) {

&nbsp;         case 'text-start': {

&nbsp;           textBlocks.set(chunk.id, '');

&nbsp;           break;

&nbsp;         }

&nbsp;         case 'text-delta': {

&nbsp;           const existing = textBlocks.get(chunk.id) || '';

&nbsp;           textBlocks.set(chunk.id, existing + chunk.delta);

&nbsp;           generatedText += chunk.delta;

&nbsp;           break;

&nbsp;         }

&nbsp;         case 'text-end': {

&nbsp;           console.log(

&nbsp;             `Text block ${chunk.id} completed:`,

&nbsp;             textBlocks.get(chunk.id),

&nbsp;           );

&nbsp;           break;

&nbsp;         }

&nbsp;       }



&nbsp;       controller.enqueue(chunk);

&nbsp;     },



&nbsp;     flush() {

&nbsp;       console.log('doStream finished');

&nbsp;       console.log(`generated text: ${generatedText}`);

&nbsp;     },

&nbsp;   });



&nbsp;   return {

&nbsp;     stream: stream.pipeThrough(transformStream),

&nbsp;     ...rest,

&nbsp;   };

&nbsp; },

};

```

\### Caching

This example shows how to build a simple cache for the generated text of a language model call.

```ts

import type { LanguageModelV3Middleware } from '@ai-sdk/provider';



const cache = new Map<string, any>();



export const yourCacheMiddleware: LanguageModelV3Middleware = {

&nbsp; wrapGenerate: async ({ doGenerate, params }) => {

&nbsp;   const cacheKey = JSON.stringify(params);



&nbsp;   if (cache.has(cacheKey)) {

&nbsp;     return cache.get(cacheKey);

&nbsp;   }



&nbsp;   const result = await doGenerate();



&nbsp;   cache.set(cacheKey, result);



&nbsp;   return result;

&nbsp; },



&nbsp; // here you would implement the caching logic for streaming

};

```

\### Retrieval Augmented Generation (RAG)

This example shows how to use RAG as middleware.

<Note>

&nbsp; Helper functions like `getLastUserMessageText` and `findSources` are not part

&nbsp; of the AI SDK. They are just used in this example to illustrate the concept of

&nbsp; RAG.

</Note>

```ts

import type { LanguageModelV3Middleware } from '@ai-sdk/provider';



export const yourRagMiddleware: LanguageModelV3Middleware = {

&nbsp; transformParams: async ({ params }) => {

&nbsp;   const lastUserMessageText = getLastUserMessageText({

&nbsp;     prompt: params.prompt,

&nbsp;   });



&nbsp;   if (lastUserMessageText == null) {

&nbsp;     return params; // do not use RAG (send unmodified parameters)

&nbsp;   }



&nbsp;   const instruction =

&nbsp;     'Use the following information to answer the question:\\n' +

&nbsp;     findSources({ text: lastUserMessageText })

&nbsp;       .map(chunk => JSON.stringify(chunk))

&nbsp;       .join('\\n');



&nbsp;   return addToLastUserMessage({ params, text: instruction });

&nbsp; },

};

```

\### Guardrails

Guard rails are a way to ensure that the generated text of a language model call

is safe and appropriate. This example shows how to use guardrails as middleware.

```ts

import type { LanguageModelV3Middleware } from '@ai-sdk/provider';



export const yourGuardrailMiddleware: LanguageModelV3Middleware = {

&nbsp; wrapGenerate: async ({ doGenerate }) => {

&nbsp;   const { text, ...rest } = await doGenerate();



&nbsp;   // filtering approach, e.g. for PII or other sensitive information:

&nbsp;   const cleanedText = text?.replace(/badword/g, '<REDACTED>');



&nbsp;   return { text: cleanedText, ...rest };

&nbsp; },



&nbsp; // here you would implement the guardrail logic for streaming

&nbsp; // Note: streaming guardrails are difficult to implement, because

&nbsp; // you do not know the full content of the stream until it's finished.

};

```

\## Configuring Per Request Custom Metadata

To send and access custom metadata in Middleware, you can use `providerOptions`. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.

```ts

import { generateText, wrapLanguageModel } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import type { LanguageModelV3Middleware } from '@ai-sdk/provider';



export const yourLogMiddleware: LanguageModelV3Middleware = {

&nbsp; wrapGenerate: async ({ doGenerate, params }) => {

&nbsp;   console.log('METADATA', params?.providerMetadata?.yourLogMiddleware);

&nbsp;   const result = await doGenerate();

&nbsp;   return result;

&nbsp; },

};



const { text } = await generateText({

&nbsp; model: wrapLanguageModel({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   middleware: yourLogMiddleware,

&nbsp; }),

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; providerOptions: {

&nbsp;   yourLogMiddleware: {

&nbsp;     hello: 'world',

&nbsp;   },

&nbsp; },

});



console.log(text);

```

---

title: Provider \& Model Management

description: Learn how to work with multiple providers and models

---

\# Provider \& Model Management

When you work with multiple providers and models, it is often desirable to manage them in a central place

and access the models through simple string ids.

The AI SDK offers \[custom providers](/docs/reference/ai-sdk-core/custom-provider) and

a \[provider registry](/docs/reference/ai-sdk-core/provider-registry) for this purpose:

\- With \*\*custom providers\*\*, you can pre-configure model settings, provide model name aliases,

&nbsp; and limit the available models.

\- The \*\*provider registry\*\* lets you mix multiple providers and access them through simple string ids.

You can mix and match custom providers, the provider registry, and \[middleware](/docs/ai-sdk-core/middleware) in your application.

\## Custom Providers

You can create a \[custom provider](/docs/reference/ai-sdk-core/custom-provider) using `customProvider`.

\### Example: custom model settings

You might want to override the default model settings for a provider or provide model name aliases

with pre-configured settings.

```ts

import {

&nbsp; gateway,

&nbsp; customProvider,

&nbsp; defaultSettingsMiddleware,

&nbsp; wrapLanguageModel,

} from 'ai';



// custom provider with different provider options:

export const openai = customProvider({

&nbsp; languageModels: {

&nbsp;   // replacement model with custom provider options:

&nbsp;   'gpt-5.1': wrapLanguageModel({

&nbsp;     model: gateway('openai/gpt-5.1'),

&nbsp;     middleware: defaultSettingsMiddleware({

&nbsp;       settings: {

&nbsp;         providerOptions: {

&nbsp;           openai: {

&nbsp;             reasoningEffort: 'high',

&nbsp;           },

&nbsp;         },

&nbsp;       },

&nbsp;     }),

&nbsp;   }),

&nbsp;   // alias model with custom provider options:

&nbsp;   'gpt-5.1-high-reasoning': wrapLanguageModel({

&nbsp;     model: gateway('openai/gpt-5.1'),

&nbsp;     middleware: defaultSettingsMiddleware({

&nbsp;       settings: {

&nbsp;         providerOptions: {

&nbsp;           openai: {

&nbsp;             reasoningEffort: 'high',

&nbsp;           },

&nbsp;         },

&nbsp;       },

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; fallbackProvider: gateway,

});

```

\### Example: model name alias

You can also provide model name aliases, so you can update the model version in one place in the future:

```ts

import { customProvider, gateway } from 'ai';



// custom provider with alias names:

export const anthropic = customProvider({

&nbsp; languageModels: {

&nbsp;   opus: gateway('anthropic/claude-opus-4.1'),

&nbsp;   sonnet: gateway('anthropic/claude-sonnet-4.5'),

&nbsp;   haiku: gateway('anthropic/claude-haiku-4.5'),

&nbsp; },

&nbsp; fallbackProvider: gateway,

});

```

\### Example: limit available models

You can limit the available models in the system, even if you have multiple providers.

```ts

import {

&nbsp; customProvider,

&nbsp; defaultSettingsMiddleware,

&nbsp; wrapLanguageModel,

&nbsp; gateway,

} from 'ai';



export const myProvider = customProvider({

&nbsp; languageModels: {

&nbsp;   'text-medium': gateway('anthropic/claude-3-5-sonnet-20240620'),

&nbsp;   'text-small': gateway('openai/gpt-5-mini'),

&nbsp;   'reasoning-medium': wrapLanguageModel({

&nbsp;     model: gateway('openai/gpt-5.1'),

&nbsp;     middleware: defaultSettingsMiddleware({

&nbsp;       settings: {

&nbsp;         providerOptions: {

&nbsp;           openai: {

&nbsp;             reasoningEffort: 'high',

&nbsp;           },

&nbsp;         },

&nbsp;       },

&nbsp;     }),

&nbsp;   }),

&nbsp;   'reasoning-fast': wrapLanguageModel({

&nbsp;     model: gateway('openai/gpt-5.1'),

&nbsp;     middleware: defaultSettingsMiddleware({

&nbsp;       settings: {

&nbsp;         providerOptions: {

&nbsp;           openai: {

&nbsp;             reasoningEffort: 'low',

&nbsp;           },

&nbsp;         },

&nbsp;       },

&nbsp;     }),

&nbsp;   }),

&nbsp; },

&nbsp; embeddingModels: {

&nbsp;   embedding: gateway.embeddingModel('openai/text-embedding-3-small'),

&nbsp; },

&nbsp; // no fallback provider

});

```

\## Provider Registry

You can create a \[provider registry](/docs/reference/ai-sdk-core/provider-registry) with multiple providers and models using `createProviderRegistry`.

\### Setup

```ts filename={"registry.ts"}

import { anthropic } from '@ai-sdk/anthropic';

import { openai } from '@ai-sdk/openai';

import { createProviderRegistry, gateway } from 'ai';



export const registry = createProviderRegistry({

&nbsp; // register provider with prefix and default setup using gateway:

&nbsp; gateway,



&nbsp; // register provider with prefix and direct provider import:

&nbsp; anthropic,

&nbsp; openai,

});

```

\### Setup with Custom Separator

By default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator:

```ts filename={"registry.ts"}

import { anthropic } from '@ai-sdk/anthropic';

import { openai } from '@ai-sdk/openai';

import { createProviderRegistry, gateway } from 'ai';



export const customSeparatorRegistry = createProviderRegistry(

&nbsp; {

&nbsp;   gateway,

&nbsp;   anthropic,

&nbsp;   openai,

&nbsp; },

&nbsp; { separator: ' > ' },

);

```

\### Example: Use language models

You can access language models by using the `languageModel` method on the registry.

The provider id will become the prefix of the model id: `providerId:modelId`.

```ts highlight={"5"}

import { generateText } from 'ai';

import { registry } from './registry';



const { text } = await generateText({

&nbsp; model: registry.languageModel('openai:gpt-5.1'), // default separator

&nbsp; // or with custom separator:

&nbsp; // model: customSeparatorRegistry.languageModel('openai > gpt-5.1'),

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

});

```

\### Example: Use text embedding models

You can access text embedding models by using the `.embeddingModel` method on the registry.

The provider id will become the prefix of the model id: `providerId:modelId`.

```ts highlight={"5"}

import { embed } from 'ai';

import { registry } from './registry';



const { embedding } = await embed({

&nbsp; model: registry.embeddingModel('openai:text-embedding-3-small'),

&nbsp; value: 'sunny day at the beach',

});

```

\### Example: Use image models

You can access image models by using the `imageModel` method on the registry.

The provider id will become the prefix of the model id: `providerId:modelId`.

```ts highlight={"5"}

import { generateImage } from 'ai';

import { registry } from './registry';



const { image } = await generateImage({

&nbsp; model: registry.imageModel('openai:dall-e-3'),

&nbsp; prompt: 'A beautiful sunset over a calm ocean',

});

```

\## Combining Custom Providers, Provider Registry, and Middleware

The central idea of provider management is to set up a file that contains all the providers and models you want to use.

You may want to pre-configure model settings, provide model name aliases, limit the available models, and more.

Here is an example that implements the following concepts:

\- pass through gateway with a namespace prefix (here: `gateway > \*`)

\- pass through a full provider with a namespace prefix (here: `xai > \*`)

\- setup an OpenAI-compatible provider with custom api key and base URL (here: `custom > \*`)

\- setup model name aliases (here: `anthropic > fast`, `anthropic > writing`, `anthropic > reasoning`)

\- pre-configure model settings (here: `anthropic > reasoning`)

\- validate the provider-specific options (here: `AnthropicProviderOptions`)

\- use a fallback provider (here: `anthropic > \*`)

\- limit a provider to certain models without a fallback (here: `groq > gemma2-9b-it`, `groq > qwen-qwq-32b`)

\- define a custom separator for the provider registry (here: `>`)

```ts

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { xai } from '@ai-sdk/xai';

import { groq } from '@ai-sdk/groq';

import {

&nbsp; createProviderRegistry,

&nbsp; customProvider,

&nbsp; defaultSettingsMiddleware,

&nbsp; gateway,

&nbsp; wrapLanguageModel,

} from 'ai';



export const registry = createProviderRegistry(

&nbsp; {

&nbsp;   // pass through gateway with a namespace prefix

&nbsp;   gateway,



&nbsp;   // pass through full providers with namespace prefixes

&nbsp;   xai,



&nbsp;   // access an OpenAI-compatible provider with custom setup

&nbsp;   custom: createOpenAICompatible({

&nbsp;     name: 'provider-name',

&nbsp;     apiKey: process.env.CUSTOM\_API\_KEY,

&nbsp;     baseURL: 'https://api.custom.com/v1',

&nbsp;   }),



&nbsp;   // setup model name aliases

&nbsp;   anthropic: customProvider({

&nbsp;     languageModels: {

&nbsp;       fast: anthropic('claude-haiku-4-5'),



&nbsp;       // simple model

&nbsp;       writing: anthropic('claude-sonnet-4-5'),



&nbsp;       // extended reasoning model configuration:

&nbsp;       reasoning: wrapLanguageModel({

&nbsp;         model: anthropic('claude-sonnet-4-5'),

&nbsp;         middleware: defaultSettingsMiddleware({

&nbsp;           settings: {

&nbsp;             maxOutputTokens: 100000, // example default setting

&nbsp;             providerOptions: {

&nbsp;               anthropic: {

&nbsp;                 thinking: {

&nbsp;                   type: 'enabled',

&nbsp;                   budgetTokens: 32000,

&nbsp;                 },

&nbsp;               } satisfies AnthropicProviderOptions,

&nbsp;             },

&nbsp;           },

&nbsp;         }),

&nbsp;       }),

&nbsp;     },

&nbsp;     fallbackProvider: anthropic,

&nbsp;   }),



&nbsp;   // limit a provider to certain models without a fallback

&nbsp;   groq: customProvider({

&nbsp;     languageModels: {

&nbsp;       'gemma2-9b-it': groq('gemma2-9b-it'),

&nbsp;       'qwen-qwq-32b': groq('qwen-qwq-32b'),

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; { separator: ' > ' },

);



// usage:

const model = registry.languageModel('anthropic > reasoning');

```

\## Global Provider Configuration

The AI SDK 5 includes a global provider feature that allows you to specify a model using just a plain model ID string:

```ts

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const result = await streamText({

&nbsp; model: \_\_MODEL\_\_, // Uses the global provider (defaults to gateway)

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

});

```

By default, the global provider is set to the Vercel AI Gateway.

\### Customizing the Global Provider

You can set your own preferred global provider:

```ts filename="setup.ts"

import { openai } from '@ai-sdk/openai';



// Initialize once during startup:

globalThis.AI\_SDK\_DEFAULT\_PROVIDER = openai;

```

```ts filename="app.ts"

import { streamText } from 'ai';



const result = await streamText({

&nbsp; model: 'gpt-5.1', // Uses OpenAI provider without prefix

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

});

```

This simplifies provider usage and makes it easier to switch between providers without changing your model references throughout your codebase.

---

title: Error Handling

description: Learn how to handle errors in the AI SDK Core

---

\# Error Handling

\## Handling regular errors

Regular errors are thrown and can be handled using the `try/catch` block.

```ts highlight="3,8-10"

import { generateText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



try {

&nbsp; const { text } = await generateText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; });

} catch (error) {

&nbsp; // handle error

}

```

See \[Error Types](/docs/reference/ai-sdk-errors) for more information on the different types of errors that may be thrown.

\## Handling streaming errors (simple streams)

When errors occur during streams that do not support error chunks,

the error is thrown as a regular error.

You can handle these errors using the `try/catch` block.

```ts highlight="3,12-14"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



try {

&nbsp; const { textStream } = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; });



&nbsp; for await (const textPart of textStream) {

&nbsp;   process.stdout.write(textPart);

&nbsp; }

} catch (error) {

&nbsp; // handle error

}

```

\## Handling streaming errors (streaming with `error` support)

Full streams support error parts.

You can handle those parts similar to other parts.

It is recommended to also add a try-catch block for errors that

happen outside of the streaming.

```ts highlight="13-21"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



try {

&nbsp; const { fullStream } = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; });



&nbsp; for await (const part of fullStream) {

&nbsp;   switch (part.type) {

&nbsp;     // ... handle other part types



&nbsp;     case 'error': {

&nbsp;       const error = part.error;

&nbsp;       // handle error

&nbsp;       break;

&nbsp;     }



&nbsp;     case 'abort': {

&nbsp;       // handle stream abort

&nbsp;       break;

&nbsp;     }



&nbsp;     case 'tool-error': {

&nbsp;       const error = part.error;

&nbsp;       // handle error

&nbsp;       break;

&nbsp;     }

&nbsp;   }

&nbsp; }

} catch (error) {

&nbsp; // handle error

}

```

\## Handling stream aborts

When streams are aborted (e.g., via chat stop button), you may want to perform cleanup operations like updating stored messages in your UI. Use the `onAbort` callback to handle these cases.

The `onAbort` callback is called when a stream is aborted via `AbortSignal`, but `onFinish` is not called. This ensures you can still update your UI state appropriately.

```ts highlight="5-9"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { textStream } = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; onAbort: ({ steps }) => {

&nbsp;   // Update stored messages or perform cleanup

&nbsp;   console.log('Stream aborted after', steps.length, 'steps');

&nbsp; },

&nbsp; onFinish: ({ steps, totalUsage }) => {

&nbsp;   // This is called on normal completion

&nbsp;   console.log('Stream completed normally');

&nbsp; },

});



for await (const textPart of textStream) {

&nbsp; process.stdout.write(textPart);

}

```

The `onAbort` callback receives:

\- `steps`: An array of all completed steps before the abort

You can also handle abort events directly in the stream:

```ts highlight="10-13"

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const { fullStream } = streamText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});



for await (const chunk of fullStream) {

&nbsp; switch (chunk.type) {

&nbsp;   case 'abort': {

&nbsp;     // Handle abort directly in stream

&nbsp;     console.log('Stream was aborted');

&nbsp;     break;

&nbsp;   }

&nbsp;   // ... handle other part types

&nbsp; }

}

```

---

title: Testing

description: Learn how to use AI SDK Core mock providers for testing.

---

\# Testing

Testing language models can be challenging, because they are non-deterministic

and calling them is slow and expensive.

To enable you to unit test your code that uses the AI SDK, the AI SDK Core

includes mock providers and test helpers. You can import the following helpers from `ai/test`:

\- `MockEmbeddingModelV3`: A mock embedding model using the \[embedding model v3 specification](https://github.com/vercel/ai/blob/main/packages/provider/src/embedding-model/v3/embedding-model-v3.ts).

\- `MockLanguageModelV3`: A mock language model using the \[language model v3 specification](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v3/language-model-v3.ts).

\- `mockId`: Provides an incrementing integer ID.

\- `mockValues`: Iterates over an array of values with each call. Returns the last value when the array is exhausted.

\- \[`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream): Simulates a readable stream with delays.

With mock providers and test helpers, you can control the output of the AI SDK

and test your code in a repeatable and deterministic way without actually calling

a language model provider.

\## Examples

You can use the test helpers with the AI Core functions in your unit tests:

\### generateText

```ts

import { generateText } from 'ai';

import { MockLanguageModelV3 } from 'ai/test';



const result = await generateText({

&nbsp; model: new MockLanguageModelV3({

&nbsp;   doGenerate: async () => ({

&nbsp;     content: \[{ type: 'text', text: `Hello, world!` }],

&nbsp;     finishReason: { unified: 'stop', raw: undefined },

&nbsp;     usage: {

&nbsp;       inputTokens: {

&nbsp;         total: 10,

&nbsp;         noCache: 10,

&nbsp;         cacheRead: undefined,

&nbsp;         cacheWrite: undefined,

&nbsp;       },

&nbsp;       outputTokens: {

&nbsp;         total: 20,

&nbsp;         text: 20,

&nbsp;         reasoning: undefined,

&nbsp;       },

&nbsp;     },

&nbsp;     warnings: \[],

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Hello, test!',

});

```

\### streamText

```ts

import { streamText, simulateReadableStream } from 'ai';

import { MockLanguageModelV3 } from 'ai/test';



const result = streamText({

&nbsp; model: new MockLanguageModelV3({

&nbsp;   doStream: async () => ({

&nbsp;     stream: simulateReadableStream({

&nbsp;       chunks: \[

&nbsp;         { type: 'text-start', id: 'text-1' },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: 'Hello' },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: ', ' },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: 'world!' },

&nbsp;         { type: 'text-end', id: 'text-1' },

&nbsp;         {

&nbsp;           type: 'finish',

&nbsp;           finishReason: { unified: 'stop', raw: undefined },

&nbsp;           logprobs: undefined,

&nbsp;           usage: {

&nbsp;             inputTokens: {

&nbsp;               total: 3,

&nbsp;               noCache: 3,

&nbsp;               cacheRead: undefined,

&nbsp;               cacheWrite: undefined,

&nbsp;             },

&nbsp;             outputTokens: {

&nbsp;               total: 10,

&nbsp;               text: 10,

&nbsp;               reasoning: undefined,

&nbsp;             },

&nbsp;           },

&nbsp;         },

&nbsp;       ],

&nbsp;     }),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Hello, test!',

});

```

\### generateObject

```ts

import { generateObject } from 'ai';

import { MockLanguageModelV3 } from 'ai/test';

import { z } from 'zod';



const result = await generateObject({

&nbsp; model: new MockLanguageModelV3({

&nbsp;   doGenerate: async () => ({

&nbsp;     content: \[{ type: 'text', text: `{"content":"Hello, world!"}` }],

&nbsp;     finishReason: { unified: 'stop', raw: undefined },

&nbsp;     usage: {

&nbsp;       inputTokens: {

&nbsp;         total: 10,

&nbsp;         noCache: 10,

&nbsp;         cacheRead: undefined,

&nbsp;         cacheWrite: undefined,

&nbsp;       },

&nbsp;       outputTokens: {

&nbsp;         total: 20,

&nbsp;         text: 20,

&nbsp;         reasoning: undefined,

&nbsp;       },

&nbsp;     },

&nbsp;     warnings: \[],

&nbsp;   }),

&nbsp; }),

&nbsp; schema: z.object({ content: z.string() }),

&nbsp; prompt: 'Hello, test!',

});

```

\### streamObject

```ts

import { streamObject, simulateReadableStream } from 'ai';

import { MockLanguageModelV3 } from 'ai/test';

import { z } from 'zod';



const result = streamObject({

&nbsp; model: new MockLanguageModelV3({

&nbsp;   doStream: async () => ({

&nbsp;     stream: simulateReadableStream({

&nbsp;       chunks: \[

&nbsp;         { type: 'text-start', id: 'text-1' },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: '{ ' },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: '"content": ' },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: `"Hello, ` },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: `world` },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: `!"` },

&nbsp;         { type: 'text-delta', id: 'text-1', delta: ' }' },

&nbsp;         { type: 'text-end', id: 'text-1' },

&nbsp;         {

&nbsp;           type: 'finish',

&nbsp;           finishReason: { unified: 'stop', raw: undefined },

&nbsp;           logprobs: undefined,

&nbsp;           usage: {

&nbsp;             inputTokens: {

&nbsp;               total: 3,

&nbsp;               noCache: 3,

&nbsp;               cacheRead: undefined,

&nbsp;               cacheWrite: undefined,

&nbsp;             },

&nbsp;             outputTokens: {

&nbsp;               total: 10,

&nbsp;               text: 10,

&nbsp;               reasoning: undefined,

&nbsp;             },

&nbsp;           },

&nbsp;         },

&nbsp;       ],

&nbsp;     }),

&nbsp;   }),

&nbsp; }),

&nbsp; schema: z.object({ content: z.string() }),

&nbsp; prompt: 'Hello, test!',

});

```

\### Simulate UI Message Stream Responses

You can also simulate \[UI Message Stream](/docs/ai-sdk-ui/stream-protocol#ui-message-stream) responses for testing,

debugging, or demonstration purposes.

Here is a Next example:

```ts filename="route.ts"

import { simulateReadableStream } from 'ai';



export async function POST(req: Request) {

&nbsp; return new Response(

&nbsp;   simulateReadableStream({

&nbsp;     initialDelayInMs: 1000, // Delay before the first chunk

&nbsp;     chunkDelayInMs: 300, // Delay between chunks

&nbsp;     chunks: \[

&nbsp;       `data: {"type":"start","messageId":"msg-123"}\\n\\n`,

&nbsp;       `data: {"type":"text-start","id":"text-1"}\\n\\n`,

&nbsp;       `data: {"type":"text-delta","id":"text-1","delta":"This"}\\n\\n`,

&nbsp;       `data: {"type":"text-delta","id":"text-1","delta":" is an"}\\n\\n`,

&nbsp;       `data: {"type":"text-delta","id":"text-1","delta":" example."}\\n\\n`,

&nbsp;       `data: {"type":"text-end","id":"text-1"}\\n\\n`,

&nbsp;       `data: {"type":"finish"}\\n\\n`,

&nbsp;       `data: \[DONE]\\n\\n`,

&nbsp;     ],

&nbsp;   }).pipeThrough(new TextEncoderStream()),

&nbsp;   {

&nbsp;     status: 200,

&nbsp;     headers: {

&nbsp;       'Content-Type': 'text/event-stream',

&nbsp;       'Cache-Control': 'no-cache',

&nbsp;       Connection: 'keep-alive',

&nbsp;       'x-vercel-ai-ui-message-stream': 'v1',

&nbsp;     },

&nbsp;   },

&nbsp; );

}

```

---

title: Telemetry

description: Using OpenTelemetry with AI SDK Core

---

\# Telemetry

<Note type="warning">

&nbsp; AI SDK Telemetry is experimental and may change in the future.

</Note>

The AI SDK uses \[OpenTelemetry](https://opentelemetry.io/) to collect telemetry data.

OpenTelemetry is an open-source observability framework designed to provide

standardized instrumentation for collecting telemetry data.

Check out the \[AI SDK Observability Integrations](/providers/observability)

to see providers that offer monitoring and tracing for AI SDK applications.

\## Enabling telemetry

For Next.js applications, please follow the \[Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry) to enable telemetry first.

You can then use the `experimental\_telemetry` option to enable telemetry on specific function calls while the feature is experimental:

```ts highlight="4"

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Write a short story about a cat.',

&nbsp; experimental\_telemetry: { isEnabled: true },

});

```

When telemetry is enabled, you can also control if you want to record the input values and the output values for the function.

By default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.

Disabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons.

You might for example want to disable recording inputs if they contain sensitive information.

\## Telemetry Metadata

You can provide a `functionId` to identify the function that the telemetry data is for,

and `metadata` to include additional information in the telemetry data.

```ts highlight="6-10"

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Write a short story about a cat.',

&nbsp; experimental\_telemetry: {

&nbsp;   isEnabled: true,

&nbsp;   functionId: 'my-awesome-function',

&nbsp;   metadata: {

&nbsp;     something: 'custom',

&nbsp;     someOtherThing: 'other-value',

&nbsp;   },

&nbsp; },

});

```

\## Custom Tracer

You may provide a `tracer` which must return an OpenTelemetry `Tracer`. This is useful in situations where

you want your traces to use a `TracerProvider` other than the one provided by the `@opentelemetry/api` singleton.

```ts highlight="7"

const tracerProvider = new NodeTracerProvider();

const result = await generateText({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; prompt: 'Write a short story about a cat.',

&nbsp; experimental\_telemetry: {

&nbsp;   isEnabled: true,

&nbsp;   tracer: tracerProvider.getTracer('ai'),

&nbsp; },

});

```

\## Collected Data

\### generateText function

`generateText` records 3 types of spans:

\- `ai.generateText` (span): the full length of the generateText call. It contains 1 or more `ai.generateText.doGenerate` spans.

&nbsp; It contains the \[basic LLM span information](#basic-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.generateText` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.generateText"`

&nbsp; - `ai.prompt`: the prompt that was used when calling `generateText`

&nbsp; - `ai.response.text`: the text that was generated

&nbsp; - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)

&nbsp; - `ai.response.finishReason`: the reason why the generation finished

&nbsp; - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set

\- `ai.generateText.doGenerate` (span): a provider doGenerate call. It can contain `ai.toolCall` spans.

&nbsp; It contains the \[call LLM span information](#call-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.generateText.doGenerate` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.generateText.doGenerate"`

&nbsp; - `ai.prompt.messages`: the messages that were passed into the provider

&nbsp; - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.

&nbsp; Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).

&nbsp; Provider-defined-client tools have a `name`, `id`, and `input` (Record).

&nbsp; - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property

&nbsp; (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.

&nbsp; - `ai.response.text`: the text that was generated

&nbsp; - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)

&nbsp; - `ai.response.finishReason`: the reason why the generation finished

\- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See \[Tool call spans](#tool-call-spans) for more details.

\### streamText function

`streamText` records 3 types of spans and 2 types of events:

\- `ai.streamText` (span): the full length of the streamText call. It contains a `ai.streamText.doStream` span.

&nbsp; It contains the \[basic LLM span information](#basic-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.streamText` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.streamText"`

&nbsp; - `ai.prompt`: the prompt that was used when calling `streamText`

&nbsp; - `ai.response.text`: the text that was generated

&nbsp; - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)

&nbsp; - `ai.response.finishReason`: the reason why the generation finished

&nbsp; - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set

\- `ai.streamText.doStream` (span): a provider doStream call.

&nbsp; This span contains an `ai.stream.firstChunk` event and `ai.toolCall` spans.

&nbsp; It contains the \[call LLM span information](#call-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.streamText.doStream` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.streamText.doStream"`

&nbsp; - `ai.prompt.messages`: the messages that were passed into the provider

&nbsp; - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.

&nbsp; Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).

&nbsp; Provider-defined-client tools have a `name`, `id`, and `input` (Record).

&nbsp; - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property

&nbsp; (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.

&nbsp; - `ai.response.text`: the text that was generated

&nbsp; - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)

&nbsp; - `ai.response.msToFirstChunk`: the time it took to receive the first chunk in milliseconds

&nbsp; - `ai.response.msToFinish`: the time it took to receive the finish part of the LLM stream in milliseconds

&nbsp; - `ai.response.avgCompletionTokensPerSecond`: the average number of completion tokens per second

&nbsp; - `ai.response.finishReason`: the reason why the generation finished

\- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See \[Tool call spans](#tool-call-spans) for more details.

\- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.

&nbsp; - `ai.response.msToFirstChunk`: the time it took to receive the first chunk

\- `ai.stream.finish` (event): an event that is emitted when the finish part of the LLM stream is received.

It also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.

\### generateObject function

`generateObject` records 2 types of spans:

\- `ai.generateObject` (span): the full length of the generateObject call. It contains 1 or more `ai.generateObject.doGenerate` spans.

&nbsp; It contains the \[basic LLM span information](#basic-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.generateObject` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.generateObject"`

&nbsp; - `ai.prompt`: the prompt that was used when calling `generateObject`

&nbsp; - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `generateObject` function

&nbsp; - `ai.schema.name`: the name of the schema that was passed into the `generateObject` function

&nbsp; - `ai.schema.description`: the description of the schema that was passed into the `generateObject` function

&nbsp; - `ai.response.object`: the object that was generated (stringified JSON)

&nbsp; - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`

\- `ai.generateObject.doGenerate` (span): a provider doGenerate call.

&nbsp; It contains the \[call LLM span information](#call-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.generateObject.doGenerate` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.generateObject.doGenerate"`

&nbsp; - `ai.prompt.messages`: the messages that were passed into the provider

&nbsp; - `ai.response.object`: the object that was generated (stringified JSON)

&nbsp; - `ai.response.finishReason`: the reason why the generation finished

\### streamObject function

`streamObject` records 2 types of spans and 1 type of event:

\- `ai.streamObject` (span): the full length of the streamObject call. It contains 1 or more `ai.streamObject.doStream` spans.

&nbsp; It contains the \[basic LLM span information](#basic-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.streamObject` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.streamObject"`

&nbsp; - `ai.prompt`: the prompt that was used when calling `streamObject`

&nbsp; - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `streamObject` function

&nbsp; - `ai.schema.name`: the name of the schema that was passed into the `streamObject` function

&nbsp; - `ai.schema.description`: the description of the schema that was passed into the `streamObject` function

&nbsp; - `ai.response.object`: the object that was generated (stringified JSON)

&nbsp; - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`

\- `ai.streamObject.doStream` (span): a provider doStream call.

&nbsp; This span contains an `ai.stream.firstChunk` event.

&nbsp; It contains the \[call LLM span information](#call-llm-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.streamObject.doStream` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.streamObject.doStream"`

&nbsp; - `ai.prompt.messages`: the messages that were passed into the provider

&nbsp; - `ai.response.object`: the object that was generated (stringified JSON)

&nbsp; - `ai.response.msToFirstChunk`: the time it took to receive the first chunk

&nbsp; - `ai.response.finishReason`: the reason why the generation finished

\- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.

&nbsp; - `ai.response.msToFirstChunk`: the time it took to receive the first chunk

\### embed function

`embed` records 2 types of spans:

\- `ai.embed` (span): the full length of the embed call. It contains 1 `ai.embed.doEmbed` spans.

&nbsp; It contains the \[basic embedding span information](#basic-embedding-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.embed` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.embed"`

&nbsp; - `ai.value`: the value that was passed into the `embed` function

&nbsp; - `ai.embedding`: a JSON-stringified embedding

\- `ai.embed.doEmbed` (span): a provider doEmbed call.

&nbsp; It contains the \[basic embedding span information](#basic-embedding-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.embed.doEmbed` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.embed.doEmbed"`

&nbsp; - `ai.values`: the values that were passed into the provider (array)

&nbsp; - `ai.embeddings`: an array of JSON-stringified embeddings

\### embedMany function

`embedMany` records 2 types of spans:

\- `ai.embedMany` (span): the full length of the embedMany call. It contains 1 or more `ai.embedMany.doEmbed` spans.

&nbsp; It contains the \[basic embedding span information](#basic-embedding-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.embedMany` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.embedMany"`

&nbsp; - `ai.values`: the values that were passed into the `embedMany` function

&nbsp; - `ai.embeddings`: an array of JSON-stringified embedding

\- `ai.embedMany.doEmbed` (span): a provider doEmbed call.

&nbsp; It contains the \[basic embedding span information](#basic-embedding-span-information) and the following attributes:

&nbsp; - `operation.name`: `ai.embedMany.doEmbed` and the functionId that was set through `telemetry.functionId`

&nbsp; - `ai.operationId`: `"ai.embedMany.doEmbed"`

&nbsp; - `ai.values`: the values that were sent to the provider

&nbsp; - `ai.embeddings`: an array of JSON-stringified embeddings for each value

\## Span Details

\### Basic LLM span information

Many spans that use LLMs (`ai.generateText`, `ai.generateText.doGenerate`, `ai.streamText`, `ai.streamText.doStream`,

`ai.generateObject`, `ai.generateObject.doGenerate`, `ai.streamObject`, `ai.streamObject.doStream`) contain the following attributes:

\- `resource.name`: the functionId that was set through `telemetry.functionId`

\- `ai.model.id`: the id of the model

\- `ai.model.provider`: the provider of the model

\- `ai.request.headers.\*`: the request headers that were passed in through `headers`

\- `ai.response.providerMetadata`: provider specific metadata returned with the generation response

\- `ai.settings.maxRetries`: the maximum number of retries that were set

\- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`

\- `ai.telemetry.metadata.\*`: the metadata that was passed in through `telemetry.metadata`

\- `ai.usage.completionTokens`: the number of completion tokens that were used

\- `ai.usage.promptTokens`: the number of prompt tokens that were used

\### Call LLM span information

Spans that correspond to individual LLM calls (`ai.generateText.doGenerate`, `ai.streamText.doStream`, `ai.generateObject.doGenerate`, `ai.streamObject.doStream`) contain

\[basic LLM span information](#basic-llm-span-information) and the following attributes:

\- `ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.

\- `ai.response.id`: the id of the response. Uses the ID from the provider when available.

\- `ai.response.timestamp`: the timestamp of the response. Uses the timestamp from the provider when available.

\- \[Semantic Conventions for GenAI operations](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/)

&nbsp; - `gen\_ai.system`: the provider that was used

&nbsp; - `gen\_ai.request.model`: the model that was requested

&nbsp; - `gen\_ai.request.temperature`: the temperature that was set

&nbsp; - `gen\_ai.request.max\_tokens`: the maximum number of tokens that were set

&nbsp; - `gen\_ai.request.frequency\_penalty`: the frequency penalty that was set

&nbsp; - `gen\_ai.request.presence\_penalty`: the presence penalty that was set

&nbsp; - `gen\_ai.request.top\_k`: the topK parameter value that was set

&nbsp; - `gen\_ai.request.top\_p`: the topP parameter value that was set

&nbsp; - `gen\_ai.request.stop\_sequences`: the stop sequences

&nbsp; - `gen\_ai.response.finish\_reasons`: the finish reasons that were returned by the provider

&nbsp; - `gen\_ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.

&nbsp; - `gen\_ai.response.id`: the id of the response. Uses the ID from the provider when available.

&nbsp; - `gen\_ai.usage.input\_tokens`: the number of prompt tokens that were used

&nbsp; - `gen\_ai.usage.output\_tokens`: the number of completion tokens that were used

\### Basic embedding span information

Many spans that use embedding models (`ai.embed`, `ai.embed.doEmbed`, `ai.embedMany`, `ai.embedMany.doEmbed`) contain the following attributes:

\- `ai.model.id`: the id of the model

\- `ai.model.provider`: the provider of the model

\- `ai.request.headers.\*`: the request headers that were passed in through `headers`

\- `ai.settings.maxRetries`: the maximum number of retries that were set

\- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`

\- `ai.telemetry.metadata.\*`: the metadata that was passed in through `telemetry.metadata`

\- `ai.usage.tokens`: the number of tokens that were used

\- `resource.name`: the functionId that was set through `telemetry.functionId`

\### Tool call spans

Tool call spans (`ai.toolCall`) contain the following attributes:

\- `operation.name`: `"ai.toolCall"`

\- `ai.operationId`: `"ai.toolCall"`

\- `ai.toolCall.name`: the name of the tool

\- `ai.toolCall.id`: the id of the tool call

\- `ai.toolCall.args`: the input parameters of the tool call

\- `ai.toolCall.result`: the output result of the tool call. Only available if the tool call is successful and the result is serializable.

---

title: DevTools

description: Debug and inspect AI SDK applications with DevTools

---

\# DevTools

<Note type="warning">

&nbsp; AI SDK DevTools is experimental and intended for local development only. Do

&nbsp; not use in production environments.

</Note>

AI SDK DevTools gives you full visibility over your AI SDK calls with \[`generateText`](/docs/reference/ai-sdk-core/generate-text), \[`streamText`](/docs/reference/ai-sdk-core/stream-text), and \[`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent). It helps you debug and inspect LLM requests, responses, tool calls, and multi-step interactions through a web-based UI.

DevTools is composed of two parts:

1\. \*\*Middleware\*\*: Captures runs and steps from your AI SDK calls

2\. \*\*Viewer\*\*: A web UI to inspect the captured data

\## Installation

Install the DevTools package:

```bash

pnpm add @ai-sdk/devtools

```

\## Requirements

\- AI SDK v6 beta (`ai@^6.0.0-beta.0`)

\- Node.js compatible runtime

\## Using DevTools

\### Add the middleware

Wrap your language model with the DevTools middleware using \[`wrapLanguageModel`](/docs/ai-sdk-core/middleware):

```ts

import { wrapLanguageModel, gateway } from 'ai';

import { devToolsMiddleware } from '@ai-sdk/devtools';



const model = wrapLanguageModel({

&nbsp; model: gateway('anthropic/claude-sonnet-4.5'),

&nbsp; middleware: devToolsMiddleware(),

});

```

The wrapped model can be used with any AI SDK Core function:

```ts highlight="4"

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model, // wrapped model with DevTools

&nbsp; prompt: 'What cities are in the United States?',

});

```

\### Launch the viewer

Start the DevTools viewer:

```bash

npx @ai-sdk/devtools

```

Open \[http://localhost:4983](http://localhost:4983) to view your AI SDK interactions.

\## Captured data

The DevTools middleware captures the following information from your AI SDK calls:

\- \*\*Input parameters and prompts\*\*: View the complete input sent to your LLM

\- \*\*Output content and tool calls\*\*: Inspect generated text and tool invocations

\- \*\*Token usage and timing\*\*: Monitor resource consumption and performance

\- \*\*Raw provider data\*\*: Access complete request and response payloads

\### Runs and steps

DevTools organizes captured data into runs and steps:

\- \*\*Run\*\*: A complete multi-step AI interaction, grouped by the initial prompt

\- \*\*Step\*\*: A single LLM call within a run (e.g., one `generateText` or `streamText` call)

Multi-step interactions, such as those created by tool calling or agent loops, are grouped together as a single run with multiple steps.

\## How it works

The DevTools middleware intercepts all `generateText` and `streamText` calls through the \[language model middleware](/docs/ai-sdk-core/middleware) system. Captured data is stored locally in a JSON file (`.devtools/generations.json`) and served through a web UI built with Hono and React.

<Note type="warning">

&nbsp; The middleware automatically adds `.devtools` to your `.gitignore` file.

&nbsp; Verify that `.devtools` is in your `.gitignore` to ensure you don't commit

&nbsp; sensitive AI interaction data to your repository.

</Note>

\## Security considerations

DevTools stores all AI interactions locally in plain text files, including:

\- User prompts and messages

\- LLM responses

\- Tool call arguments and results

\- API request and response data

\*\*Only use DevTools in local development environments.\*\* Do not enable DevTools in production or when handling sensitive data.

---

title: Overview

description: An overview of AI SDK UI.

---

\# AI SDK UI

AI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a \*\*framework-agnostic toolkit\*\*, streamlining the integration of advanced AI functionalities into your applications.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With three main hooks — \*\*`useChat`\*\*, \*\*`useCompletion`\*\*, and \*\*`useObject`\*\* — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

\- \*\*\[`useChat`](/docs/ai-sdk-ui/chatbot)\*\* offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.

\- \*\*\[`useCompletion`](/docs/ai-sdk-ui/completion)\*\* enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.

\- \*\*\[`useObject`](/docs/ai-sdk-ui/object-generation)\*\* is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.

These hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.

\## UI Framework Support

AI SDK UI supports the following frameworks: \[React](https://react.dev/), \[Svelte](https://svelte.dev/), \[Vue.js](https://vuejs.org/),

\[Angular](https://angular.dev/), and \[SolidJS](https://www.solidjs.com/).

Here is a comparison of the supported functions across these frameworks:

| | \[useChat](/docs/reference/ai-sdk-ui/use-chat) | \[useCompletion](/docs/reference/ai-sdk-ui/use-completion) | \[useObject](/docs/reference/ai-sdk-ui/use-object) |

| --------------------------------------------------------------- | --------------------------------------------- | --------------------------------------------------------- | ------------------------------------------------- |

| React `@ai-sdk/react` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| Vue.js `@ai-sdk/vue` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| Svelte `@ai-sdk/svelte` | <Check size={18} /> Chat | <Check size={18} /> Completion | <Check size={18} /> StructuredObject |

| Angular `@ai-sdk/angular` | <Check size={18} /> Chat | <Check size={18} /> Completion | <Check size={18} /> StructuredObject |

| \[SolidJS](https://github.com/kodehort/ai-sdk-solid) (community) | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

\## Framework Examples

Explore these example implementations for different frameworks:

\- \[\*\*Next.js\*\*](https://github.com/vercel/ai/tree/main/examples/next-openai)

\- \[\*\*Nuxt\*\*](https://github.com/vercel/ai/tree/main/examples/nuxt-openai)

\- \[\*\*SvelteKit\*\*](https://github.com/vercel/ai/tree/main/examples/sveltekit-openai)

\- \[\*\*Angular\*\*](https://github.com/vercel/ai/tree/main/examples/angular)

\## API Reference

Please check out the \[AI SDK UI API Reference](/docs/reference/ai-sdk-ui) for more details on each function.

---

title: Chatbot

description: Learn how to use the useChat hook.

---

\# Chatbot

The `useChat` hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.

To summarize, the `useChat` hook provides the following features:

\- \*\*Message Streaming\*\*: All the messages from the AI provider are streamed to the chat UI in real-time.

\- \*\*Managed States\*\*: The hook manages the states for input, messages, status, error and more for you.

\- \*\*Seamless Integration\*\*: Easily integrate your chat AI into any design or layout with minimal effort.

In this guide, you will learn how to use the `useChat` hook to create a chatbot application with real-time message streaming.

Check out our \[chatbot with tools guide](/docs/ai-sdk-ui/chatbot-with-tool-calling) to learn how to use tools in your chatbot.

Let's start with the following example first.

\## Example

```tsx filename='app/page.tsx'

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Page() {

&nbsp; const { messages, sendMessage, status } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });

&nbsp; const \[input, setInput] = useState('');



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) =>

&nbsp;           part.type === 'text' ? <span key={index}>{part.text}</span> : null,

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({ text: input });

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         disabled={status !== 'ready'}

&nbsp;         placeholder="Say something..."

&nbsp;       />

&nbsp;       <button type="submit" disabled={status !== 'ready'}>

&nbsp;         Submit

&nbsp;       </button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

```ts filename='app/api/chat/route.ts'

import { convertToModelMessages, streamText, UIMessage } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   system: 'You are a helpful assistant.',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

<Note>

&nbsp; The UI messages have a new `parts` property that contains the message parts.

&nbsp; We recommend rendering the messages using the `parts` property instead of the

&nbsp; `content` property. The parts property supports different message types,

&nbsp; including text, tool invocation, and tool result, and allows for more flexible

&nbsp; and complex chat UIs.

</Note>

In the `Page` component, the `useChat` hook will request to your AI provider endpoint whenever the user sends a message using `sendMessage`.

The messages are then streamed back in real-time and displayed in the chat UI.

This enables a seamless chat experience where the user can see the AI response as soon as it is available,

without having to wait for the entire response to be received.

\## Customized UI

`useChat` also provides ways to manage the chat message states via code, show status, and update messages without being triggered by user interactions.

\### Status

The `useChat` hook returns a `status`. It has the following possible values:

\- `submitted`: The message has been sent to the API and we're awaiting the start of the response stream.

\- `streaming`: The response is actively streaming in from the API, receiving chunks of data.

\- `ready`: The full response has been received and processed; a new user message can be submitted.

\- `error`: An error occurred during the API request, preventing successful completion.

You can use `status` for e.g. the following purposes:

\- To show a loading spinner while the chatbot is processing the user's message.

\- To show a "Stop" button to abort the current message.

\- To disable the submit button.

```tsx filename='app/page.tsx' highlight="6,22-29,36"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Page() {

&nbsp; const { messages, sendMessage, status, stop } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });

&nbsp; const \[input, setInput] = useState('');



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) =>

&nbsp;           part.type === 'text' ? <span key={index}>{part.text}</span> : null,

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     {(status === 'submitted' || status === 'streaming') \&\& (

&nbsp;       <div>

&nbsp;         {status === 'submitted' \&\& <Spinner />}

&nbsp;         <button type="button" onClick={() => stop()}>

&nbsp;           Stop

&nbsp;         </button>

&nbsp;       </div>

&nbsp;     )}



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({ text: input });

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         disabled={status !== 'ready'}

&nbsp;         placeholder="Say something..."

&nbsp;       />

&nbsp;       <button type="submit" disabled={status !== 'ready'}>

&nbsp;         Submit

&nbsp;       </button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

\### Error State

Similarly, the `error` state reflects the error object thrown during the fetch request.

It can be used to display an error message, disable the submit button, or show a retry button:

<Note>

&nbsp; We recommend showing a generic error message to the user, such as "Something

&nbsp; went wrong." This is a good practice to avoid leaking information from the

&nbsp; server.

</Note>

```tsx file="app/page.tsx" highlight="6,20-27,33"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Chat() {

&nbsp; const { messages, sendMessage, error, reload } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });

&nbsp; const \[input, setInput] = useState('');



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id}>

&nbsp;         {m.role}:{' '}

&nbsp;         {m.parts.map((part, index) =>

&nbsp;           part.type === 'text' ? <span key={index}>{part.text}</span> : null,

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     {error \&\& (

&nbsp;       <>

&nbsp;         <div>An error occurred.</div>

&nbsp;         <button type="button" onClick={() => reload()}>

&nbsp;           Retry

&nbsp;         </button>

&nbsp;       </>

&nbsp;     )}



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({ text: input });

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         disabled={error != null}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

Please also see the \[error handling](/docs/ai-sdk-ui/error-handling) guide for more information.

\### Modify messages

Sometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.

The `setMessages` function can help you achieve these tasks:

```tsx

const { messages, setMessages } = useChat()



const handleDelete = (id) => {

&nbsp; setMessages(messages.filter(message => message.id !== id))

}



return <>

&nbsp; {messages.map(message => (

&nbsp;   <div key={message.id}>

&nbsp;     {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;     {message.parts.map((part, index) => (

&nbsp;       part.type === 'text' ? (

&nbsp;         <span key={index}>{part.text}</span>

&nbsp;       ) : null

&nbsp;     ))}

&nbsp;     <button onClick={() => handleDelete(message.id)}>Delete</button>

&nbsp;   </div>

&nbsp; ))}

&nbsp; ...

```

You can think of `messages` and `setMessages` as a pair of `state` and `setState` in React.

\### Cancellation and regeneration

It's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useChat` hook.

```tsx

const { stop, status } = useChat()



return <>

&nbsp; <button onClick={stop} disabled={!(status === 'streaming' || status === 'submitted')}>Stop</button>

&nbsp; ...

```

When the user clicks the "Stop" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.

Similarly, you can also request the AI provider to reprocess the last message by calling the `regenerate` function returned by the `useChat` hook:

```tsx

const { regenerate, status } = useChat();



return (

&nbsp; <>

&nbsp;   <button

&nbsp;     onClick={regenerate}

&nbsp;     disabled={!(status === 'ready' || status === 'error')}

&nbsp;   >

&nbsp;     Regenerate

&nbsp;   </button>

&nbsp;   ...

&nbsp; </>

);

```

When the user clicks the "Regenerate" button, the AI provider will regenerate the last message and replace the current one correspondingly.

\### Throttling UI Updates

<Note>This feature is currently only available for React.</Note>

By default, the `useChat` hook will trigger a render every time a new chunk is received.

You can throttle the UI updates with the `experimental\_throttle` option.

```tsx filename="page.tsx" highlight="2-3"

const { messages, ... } = useChat({

&nbsp; // Throttle the messages and data updates to 50ms:

&nbsp; experimental\_throttle: 50

})

```

\## Event Callbacks

`useChat` provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:

\- `onFinish`: Called when the assistant response is completed. The event includes the response message, all messages, and flags for abort, disconnect, and errors.

\- `onError`: Called when an error occurs during the fetch request.

\- `onData`: Called whenever a data part is received.

These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.

```tsx

import { UIMessage } from 'ai';



const {

&nbsp; /\* ... \*/

} = useChat({

&nbsp; onFinish: ({ message, messages, isAbort, isDisconnect, isError }) => {

&nbsp;   // use information to e.g. update other UI states

&nbsp; },

&nbsp; onError: error => {

&nbsp;   console.error('An error occurred:', error);

&nbsp; },

&nbsp; onData: data => {

&nbsp;   console.log('Received data part from server:', data);

&nbsp; },

});

```

It's worth noting that you can abort the processing by throwing an error in the `onData` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.

\## Request Configuration

\### Custom headers, body, and credentials

By default, the `useChat` hook sends a HTTP POST request to the `/api/chat` endpoint with the message list as the request body. You can customize the request in two ways:

\#### Hook-Level Configuration (Applied to all requests)

You can configure transport-level options that will be applied to all requests made by the hook:

```tsx

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



const { messages, sendMessage } = useChat({

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/custom-chat',

&nbsp;   headers: {

&nbsp;     Authorization: 'your\_token',

&nbsp;   },

&nbsp;   body: {

&nbsp;     user\_id: '123',

&nbsp;   },

&nbsp;   credentials: 'same-origin',

&nbsp; }),

});

```

\#### Dynamic Hook-Level Configuration

You can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:

```tsx

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



const { messages, sendMessage } = useChat({

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/custom-chat',

&nbsp;   headers: () => ({

&nbsp;     Authorization: `Bearer ${getAuthToken()}`,

&nbsp;     'X-User-ID': getCurrentUserId(),

&nbsp;   }),

&nbsp;   body: () => ({

&nbsp;     sessionId: getCurrentSessionId(),

&nbsp;     preferences: getUserPreferences(),

&nbsp;   }),

&nbsp;   credentials: () => 'include',

&nbsp; }),

});

```

<Note>

&nbsp; For component state that changes over time, use `useRef` to store the current

&nbsp; value and reference `ref.current` in your configuration function, or prefer

&nbsp; request-level options (see next section) for better reliability.

</Note>

\#### Request-Level Configuration (Recommended)

<Note>

&nbsp; \*\*Recommended\*\*: Use request-level options for better flexibility and control.

&nbsp; Request-level options take precedence over hook-level options and allow you to

&nbsp; customize each request individually.

</Note>

```tsx

// Pass options as the second parameter to sendMessage

sendMessage(

&nbsp; { text: input },

&nbsp; {

&nbsp;   headers: {

&nbsp;     Authorization: 'Bearer token123',

&nbsp;     'X-Custom-Header': 'custom-value',

&nbsp;   },

&nbsp;   body: {

&nbsp;     temperature: 0.7,

&nbsp;     max\_tokens: 100,

&nbsp;     user\_id: '123',

&nbsp;   },

&nbsp;   metadata: {

&nbsp;     userId: 'user123',

&nbsp;     sessionId: 'session456',

&nbsp;   },

&nbsp; },

);

```

The request-level options are merged with hook-level options, with request-level options taking precedence. On your server side, you can handle the request with this additional information.

\### Setting custom body fields per request

You can configure custom `body` fields on a per-request basis using the second parameter of the `sendMessage` function.

This is useful if you want to pass in additional information to your backend that is not part of the message list.

```tsx filename="app/page.tsx" highlight="20-25"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Chat() {

&nbsp; const { messages, sendMessage } = useChat();

&nbsp; const \[input, setInput] = useState('');



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id}>

&nbsp;         {m.role}:{' '}

&nbsp;         {m.parts.map((part, index) =>

&nbsp;           part.type === 'text' ? <span key={index}>{part.text}</span> : null,

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form

&nbsp;       onSubmit={event => {

&nbsp;         event.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage(

&nbsp;             { text: input },

&nbsp;             {

&nbsp;               body: {

&nbsp;                 customKey: 'customValue',

&nbsp;               },

&nbsp;             },

&nbsp;           );

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input value={input} onChange={e => setInput(e.target.value)} />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

You can retrieve these custom fields on your server side by destructuring the request body:

```ts filename="app/api/chat/route.ts" highlight="3,4"

export async function POST(req: Request) {

&nbsp; // Extract additional information ("customKey") from the body of the request:

&nbsp; const { messages, customKey }: { messages: UIMessage\[]; customKey: string } =

&nbsp;   await req.json();

&nbsp; //...

}

```

\## Message Metadata

You can attach custom metadata to messages for tracking information like timestamps, model details, and token usage.

```ts

// Server: Send metadata about the message

return result.toUIMessageStreamResponse({

&nbsp; messageMetadata: ({ part }) => {

&nbsp;   if (part.type === 'start') {

&nbsp;     return {

&nbsp;       createdAt: Date.now(),

&nbsp;       model: 'gpt-5.1',

&nbsp;     };

&nbsp;   }



&nbsp;   if (part.type === 'finish') {

&nbsp;     return {

&nbsp;       totalTokens: part.totalUsage.totalTokens,

&nbsp;     };

&nbsp;   }

&nbsp; },

});

```

```tsx

// Client: Access metadata via message.metadata

{

&nbsp; messages.map(message => (

&nbsp;   <div key={message.id}>

&nbsp;     {message.role}:{' '}

&nbsp;     {message.metadata?.createdAt \&\&

&nbsp;       new Date(message.metadata.createdAt).toLocaleTimeString()}

&nbsp;     {/\* Render message content \*/}

&nbsp;     {message.parts.map((part, index) =>

&nbsp;       part.type === 'text' ? <span key={index}>{part.text}</span> : null,

&nbsp;     )}

&nbsp;     {/\* Show token count if available \*/}

&nbsp;     {message.metadata?.totalTokens \&\& (

&nbsp;       <span>{message.metadata.totalTokens} tokens</span>

&nbsp;     )}

&nbsp;   </div>

&nbsp; ));

}

```

For complete examples with type safety and advanced use cases, see the \[Message Metadata documentation](/docs/ai-sdk-ui/message-metadata).

\## Transport Configuration

You can configure custom transport behavior using the `transport` option to customize how messages are sent to your API:

```tsx filename="app/page.tsx"

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



export default function Chat() {

&nbsp; const { messages, sendMessage } = useChat({

&nbsp;   id: 'my-chat',

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     prepareSendMessagesRequest: ({ id, messages }) => {

&nbsp;       return {

&nbsp;         body: {

&nbsp;           id,

&nbsp;           message: messages\[messages.length - 1],

&nbsp;         },

&nbsp;       };

&nbsp;     },

&nbsp;   }),

&nbsp; });



&nbsp; // ... rest of your component

}

```

The corresponding API route receives the custom request format:

```ts filename="app/api/chat/route.ts"

export async function POST(req: Request) {

&nbsp; const { id, message } = await req.json();



&nbsp; // Load existing messages and add the new one

&nbsp; const messages = await loadMessages(id);

&nbsp; messages.push(message);



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

\### Advanced: Trigger-based routing

For more complex scenarios like message regeneration, you can use trigger-based routing:

```tsx filename="app/page.tsx"

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



export default function Chat() {

&nbsp; const { messages, sendMessage, regenerate } = useChat({

&nbsp;   id: 'my-chat',

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {

&nbsp;       if (trigger === 'submit-user-message') {

&nbsp;         return {

&nbsp;           body: {

&nbsp;             trigger: 'submit-user-message',

&nbsp;             id,

&nbsp;             message: messages\[messages.length - 1],

&nbsp;             messageId,

&nbsp;           },

&nbsp;         };

&nbsp;       } else if (trigger === 'regenerate-assistant-message') {

&nbsp;         return {

&nbsp;           body: {

&nbsp;             trigger: 'regenerate-assistant-message',

&nbsp;             id,

&nbsp;             messageId,

&nbsp;           },

&nbsp;         };

&nbsp;       }

&nbsp;       throw new Error(`Unsupported trigger: ${trigger}`);

&nbsp;     },

&nbsp;   }),

&nbsp; });



&nbsp; // ... rest of your component

}

```

The corresponding API route would handle different triggers:

```ts filename="app/api/chat/route.ts"

export async function POST(req: Request) {

&nbsp; const { trigger, id, message, messageId } = await req.json();



&nbsp; const chat = await readChat(id);

&nbsp; let messages = chat.messages;



&nbsp; if (trigger === 'submit-user-message') {

&nbsp;   // Handle new user message

&nbsp;   messages = \[...messages, message];

&nbsp; } else if (trigger === 'regenerate-assistant-message') {

&nbsp;   // Handle message regeneration - remove messages after messageId

&nbsp;   const messageIndex = messages.findIndex(m => m.id === messageId);

&nbsp;   if (messageIndex !== -1) {

&nbsp;     messages = messages.slice(0, messageIndex);

&nbsp;   }

&nbsp; }



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

To learn more about building custom transports, refer to the \[Transport API documentation](/docs/ai-sdk-ui/transport).

\### Direct Agent Transport

For scenarios where you want to communicate directly with an Agent without going through HTTP, you can use `DirectChatTransport`. This is useful for:

\- Server-side rendering scenarios

\- Testing without network

\- Single-process applications

```tsx filename="app/page.tsx"

import { useChat } from '@ai-sdk/react';

import { DirectChatTransport, ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: 'You are a helpful assistant.',

});



export default function Chat() {

&nbsp; const { messages, sendMessage, status } = useChat({

&nbsp;   transport: new DirectChatTransport({ agent }),

&nbsp; });



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {message.parts.map((part, index) =>

&nbsp;           part.type === 'text' ? <span key={index}>{part.text}</span> : null,

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <button

&nbsp;       onClick={() => sendMessage({ text: 'Hello!' })}

&nbsp;       disabled={status !== 'ready'}

&nbsp;     >

&nbsp;       Send

&nbsp;     </button>

&nbsp;   </>

&nbsp; );

}

```

The `DirectChatTransport` invokes the agent's `stream()` method directly, converting UI messages to model messages and streaming the response back as UI message chunks.

For more details, see the \[DirectChatTransport reference](/docs/reference/ai-sdk-ui/direct-chat-transport).

\## Controlling the response stream

With `streamText`, you can control how error messages and usage information are sent back to the client.

\### Error Messages

By default, the error message is masked for security reasons.

The default error message is "An error occurred."

You can forward error messages or send your own error message by providing a `getErrorMessage` function:

```ts filename="app/api/chat/route.ts" highlight="13-27"

import { convertToModelMessages, streamText, UIMessage } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   onError: error => {

&nbsp;     if (error == null) {

&nbsp;       return 'unknown error';

&nbsp;     }



&nbsp;     if (typeof error === 'string') {

&nbsp;       return error;

&nbsp;     }



&nbsp;     if (error instanceof Error) {

&nbsp;       return error.message;

&nbsp;     }



&nbsp;     return JSON.stringify(error);

&nbsp;   },

&nbsp; });

}

```

\### Usage Information

Track token consumption and resource usage with \[message metadata](/docs/ai-sdk-ui/message-metadata):

1\. Define a custom metadata type with usage fields (optional, for type safety)

2\. Attach usage data using `messageMetadata` in your response

3\. Display usage metrics in your UI components

Usage data is attached as metadata to messages and becomes available once the model completes its response generation.

```ts

import { openai } from '@ai-sdk/openai';

import {

&nbsp; convertToModelMessages,

&nbsp; streamText,

&nbsp; UIMessage,

&nbsp; type LanguageModelUsage,

} from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



// Create a new metadata type (optional for type-safety)

type MyMetadata = {

&nbsp; totalUsage: LanguageModelUsage;

};



// Create a new custom message type with your own metadata

export type MyUIMessage = UIMessage<MyMetadata>;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: MyUIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages,

&nbsp;   messageMetadata: ({ part }) => {

&nbsp;     // Send total usage when generation is finished

&nbsp;     if (part.type === 'finish') {

&nbsp;       return { totalUsage: part.totalUsage };

&nbsp;     }

&nbsp;   },

&nbsp; });

}

```

Then, on the client, you can access the message-level metadata.

```tsx

'use client';



import { useChat } from '@ai-sdk/react';

import type { MyUIMessage } from './api/chat/route';

import { DefaultChatTransport } from 'ai';



export default function Chat() {

&nbsp; // Use custom message type defined on the server (optional for type-safety)

&nbsp; const { messages } = useChat<MyUIMessage>({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });



&nbsp; return (

&nbsp;   <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id} className="whitespace-pre-wrap">

&nbsp;         {m.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {m.parts.map(part => {

&nbsp;           if (part.type === 'text') {

&nbsp;             return part.text;

&nbsp;           }

&nbsp;         })}

&nbsp;         {/\* Render usage via metadata \*/}

&nbsp;         {m.metadata?.totalUsage \&\& (

&nbsp;           <div>Total usage: {m.metadata?.totalUsage.totalTokens} tokens</div>

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </div>

&nbsp; );

}

```

You can also access your metadata from the `onFinish` callback of `useChat`:

```tsx

'use client';



import { useChat } from '@ai-sdk/react';

import type { MyUIMessage } from './api/chat/route';

import { DefaultChatTransport } from 'ai';



export default function Chat() {

&nbsp; // Use custom message type defined on the server (optional for type-safety)

&nbsp; const { messages } = useChat<MyUIMessage>({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp;   onFinish: ({ message }) => {

&nbsp;     // Access message metadata via onFinish callback

&nbsp;     console.log(message.metadata?.totalUsage);

&nbsp;   },

&nbsp; });

}

```

\### Text Streams

`useChat` can handle plain text streams by setting the `streamProtocol` option to `text`:

```tsx filename="app/page.tsx" highlight="7"

'use client';



import { useChat } from '@ai-sdk/react';

import { TextStreamChatTransport } from 'ai';



export default function Chat() {

&nbsp; const { messages } = useChat({

&nbsp;   transport: new TextStreamChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });



&nbsp; return <>...</>;

}

```

This configuration also works with other backend servers that stream plain text.

Check out the \[stream protocol guide](/docs/ai-sdk-ui/stream-protocol) for more information.

<Note>

&nbsp; When using `TextStreamChatTransport`, tool calls, usage information and finish

&nbsp; reasons are not available.

</Note>

\## Reasoning

Some models such as as DeepSeek `deepseek-r1`

and Anthropic `claude-3-7-sonnet-20250219` support reasoning tokens.

These tokens are typically sent before the message content.

You can forward them to the client with the `sendReasoning` option:

```ts filename="app/api/chat/route.ts" highlight="13"

import { convertToModelMessages, streamText, UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'deepseek/deepseek-r1',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   sendReasoning: true,

&nbsp; });

}

```

On the client side, you can access the reasoning parts of the message object.

Reasoning parts have a `text` property that contains the reasoning content.

```tsx filename="app/page.tsx"

messages.map(message => (

&nbsp; <div key={message.id}>

&nbsp;   {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;   {message.parts.map((part, index) => {

&nbsp;     // text parts:

&nbsp;     if (part.type === 'text') {

&nbsp;       return <div key={index}>{part.text}</div>;

&nbsp;     }



&nbsp;     // reasoning parts:

&nbsp;     if (part.type === 'reasoning') {

&nbsp;       return <pre key={index}>{part.text}</pre>;

&nbsp;     }

&nbsp;   })}

&nbsp; </div>

));

```

\## Sources

Some providers such as \[Perplexity](/providers/ai-sdk-providers/perplexity#sources) and

\[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.

Currently sources are limited to web pages that ground the response.

You can forward them to the client with the `sendSources` option:

```ts filename="app/api/chat/route.ts" highlight="13"

import { convertToModelMessages, streamText, UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'perplexity/sonar-pro',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   sendSources: true,

&nbsp; });

}

```

On the client side, you can access source parts of the message object.

There are two types of sources: `source-url` for web pages and `source-document` for documents.

Here is an example that renders both types of sources:

```tsx filename="app/page.tsx"

messages.map(message => (

&nbsp; <div key={message.id}>

&nbsp;   {message.role === 'user' ? 'User: ' : 'AI: '}



&nbsp;   {/\* Render URL sources \*/}

&nbsp;   {message.parts

&nbsp;     .filter(part => part.type === 'source-url')

&nbsp;     .map(part => (

&nbsp;       <span key={`source-${part.id}`}>

&nbsp;         \[

&nbsp;         <a href={part.url} target="\_blank">

&nbsp;           {part.title ?? new URL(part.url).hostname}

&nbsp;         </a>

&nbsp;         ]

&nbsp;       </span>

&nbsp;     ))}



&nbsp;   {/\* Render document sources \*/}

&nbsp;   {message.parts

&nbsp;     .filter(part => part.type === 'source-document')

&nbsp;     .map(part => (

&nbsp;       <span key={`source-${part.id}`}>

&nbsp;         \[<span>{part.title ?? `Document ${part.id}`}</span>]

&nbsp;       </span>

&nbsp;     ))}

&nbsp; </div>

));

```

\## Image Generation

Some models such as Google `gemini-2.5-flash-image-preview` support image generation.

When images are generated, they are exposed as files to the client.

On the client side, you can access file parts of the message object

and render them as images.

```tsx filename="app/page.tsx"

messages.map(message => (

&nbsp; <div key={message.id}>

&nbsp;   {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;   {message.parts.map((part, index) => {

&nbsp;     if (part.type === 'text') {

&nbsp;       return <div key={index}>{part.text}</div>;

&nbsp;     } else if (part.type === 'file' \&\& part.mediaType.startsWith('image/')) {

&nbsp;       return <img key={index} src={part.url} alt="Generated image" />;

&nbsp;     }

&nbsp;   })}

&nbsp; </div>

));

```

\## Attachments

The `useChat` hook supports sending file attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.

There are two ways to send files with a message: using a `FileList` object from file inputs or using an array of file objects.

\### FileList

By using `FileList`, you can send multiple files as attachments along with a message using the file input element. The `useChat` hook will automatically convert them into data URLs and send them to the AI provider.

<Note>

&nbsp; Currently, only `image/\*` and `text/\*` content types get automatically

&nbsp; converted into \[multi-modal content

&nbsp; parts](/docs/foundations/prompts#multi-modal-messages). You will need to

&nbsp; handle other content types manually.

</Note>

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useRef, useState } from 'react';



export default function Page() {

&nbsp; const { messages, sendMessage, status } = useChat();



&nbsp; const \[input, setInput] = useState('');

&nbsp; const \[files, setFiles] = useState<FileList | undefined>(undefined);

&nbsp; const fileInputRef = useRef<HTMLInputElement>(null);



&nbsp; return (

&nbsp;   <div>

&nbsp;     <div>

&nbsp;       {messages.map(message => (

&nbsp;         <div key={message.id}>

&nbsp;           <div>{`${message.role}: `}</div>



&nbsp;           <div>

&nbsp;             {message.parts.map((part, index) => {

&nbsp;               if (part.type === 'text') {

&nbsp;                 return <span key={index}>{part.text}</span>;

&nbsp;               }



&nbsp;               if (

&nbsp;                 part.type === 'file' \&\&

&nbsp;                 part.mediaType?.startsWith('image/')

&nbsp;               ) {

&nbsp;                 return <img key={index} src={part.url} alt={part.filename} />;

&nbsp;               }



&nbsp;               return null;

&nbsp;             })}

&nbsp;           </div>

&nbsp;         </div>

&nbsp;       ))}

&nbsp;     </div>



&nbsp;     <form

&nbsp;       onSubmit={event => {

&nbsp;         event.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({

&nbsp;             text: input,

&nbsp;             files,

&nbsp;           });

&nbsp;           setInput('');

&nbsp;           setFiles(undefined);



&nbsp;           if (fileInputRef.current) {

&nbsp;             fileInputRef.current.value = '';

&nbsp;           }

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         type="file"

&nbsp;         onChange={event => {

&nbsp;           if (event.target.files) {

&nbsp;             setFiles(event.target.files);

&nbsp;           }

&nbsp;         }}

&nbsp;         multiple

&nbsp;         ref={fileInputRef}

&nbsp;       />

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         placeholder="Send message..."

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         disabled={status !== 'ready'}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

\### File Objects

You can also send files as objects along with a message. This can be useful for sending pre-uploaded files or data URLs.

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';

import { FileUIPart } from 'ai';



export default function Page() {

&nbsp; const { messages, sendMessage, status } = useChat();



&nbsp; const \[input, setInput] = useState('');

&nbsp; const \[files] = useState<FileUIPart\[]>(\[

&nbsp;   {

&nbsp;     type: 'file',

&nbsp;     filename: 'earth.png',

&nbsp;     mediaType: 'image/png',

&nbsp;     url: 'https://example.com/earth.png',

&nbsp;   },

&nbsp;   {

&nbsp;     type: 'file',

&nbsp;     filename: 'moon.png',

&nbsp;     mediaType: 'image/png',

&nbsp;     url: 'data:image/png;base64,iVBORw0KGgo...',

&nbsp;   },

&nbsp; ]);



&nbsp; return (

&nbsp;   <div>

&nbsp;     <div>

&nbsp;       {messages.map(message => (

&nbsp;         <div key={message.id}>

&nbsp;           <div>{`${message.role}: `}</div>



&nbsp;           <div>

&nbsp;             {message.parts.map((part, index) => {

&nbsp;               if (part.type === 'text') {

&nbsp;                 return <span key={index}>{part.text}</span>;

&nbsp;               }



&nbsp;               if (

&nbsp;                 part.type === 'file' \&\&

&nbsp;                 part.mediaType?.startsWith('image/')

&nbsp;               ) {

&nbsp;                 return <img key={index} src={part.url} alt={part.filename} />;

&nbsp;               }



&nbsp;               return null;

&nbsp;             })}

&nbsp;           </div>

&nbsp;         </div>

&nbsp;       ))}

&nbsp;     </div>



&nbsp;     <form

&nbsp;       onSubmit={event => {

&nbsp;         event.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({

&nbsp;             text: input,

&nbsp;             files,

&nbsp;           });

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         placeholder="Send message..."

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         disabled={status !== 'ready'}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

\## Type Inference for Tools

When working with tools in TypeScript, AI SDK UI provides type inference helpers to ensure type safety for your tool inputs and outputs.

\### InferUITool

The `InferUITool` type helper infers the input and output types of a single tool for use in UI messages:

```tsx

import { InferUITool } from 'ai';

import { z } from 'zod';



const weatherTool = {

&nbsp; description: 'Get the current weather',

&nbsp; inputSchema: z.object({

&nbsp;   location: z.string().describe('The city and state'),

&nbsp; }),

&nbsp; execute: async ({ location }) => {

&nbsp;   return `The weather in ${location} is sunny.`;

&nbsp; },

};



// Infer the types from the tool

type WeatherUITool = InferUITool<typeof weatherTool>;

// This creates a type with:

// {

//   input: { location: string };

//   output: string;

// }

```

\### InferUITools

The `InferUITools` type helper infers the input and output types of a `ToolSet`:

```tsx

import { InferUITools, ToolSet } from 'ai';

import { z } from 'zod';



const tools = {

&nbsp; weather: {

&nbsp;   description: 'Get the current weather',

&nbsp;   inputSchema: z.object({

&nbsp;     location: z.string().describe('The city and state'),

&nbsp;   }),

&nbsp;   execute: async ({ location }) => {

&nbsp;     return `The weather in ${location} is sunny.`;

&nbsp;   },

&nbsp; },

&nbsp; calculator: {

&nbsp;   description: 'Perform basic arithmetic',

&nbsp;   inputSchema: z.object({

&nbsp;     operation: z.enum(\['add', 'subtract', 'multiply', 'divide']),

&nbsp;     a: z.number(),

&nbsp;     b: z.number(),

&nbsp;   }),

&nbsp;   execute: async ({ operation, a, b }) => {

&nbsp;     switch (operation) {

&nbsp;       case 'add':

&nbsp;         return a + b;

&nbsp;       case 'subtract':

&nbsp;         return a - b;

&nbsp;       case 'multiply':

&nbsp;         return a \* b;

&nbsp;       case 'divide':

&nbsp;         return a / b;

&nbsp;     }

&nbsp;   },

&nbsp; },

} satisfies ToolSet;



// Infer the types from the tool set

type MyUITools = InferUITools<typeof tools>;

// This creates a type with:

// {

//   weather: { input: { location: string }; output: string };

//   calculator: { input: { operation: 'add' | 'subtract' | 'multiply' | 'divide'; a: number; b: number }; output: number };

// }

```

\### Using Inferred Types

You can use these inferred types to create a custom UIMessage type and pass it to various AI SDK UI functions:

```tsx
import { InferUITools, UIMessage, UIDataTypes } from "ai";

type MyUITools = InferUITools<typeof tools>;

type MyUIMessage = UIMessage<never, UIDataTypes, MyUITools>;
```

Pass the custom type to `useChat` or `createUIMessageStream`:

```tsx
import { useChat } from "@ai-sdk/react";

import { createUIMessageStream } from "ai";

import type { MyUIMessage } from "./types";

// With useChat

const { messages } = useChat<MyUIMessage>();

// With createUIMessageStream

const stream = createUIMessageStream<MyUIMessage>(/\* ... \*/);
```

This provides full type safety for tool inputs and outputs on the client and server.

---

title: Chatbot Message Persistence

description: Learn how to store and load chat messages in a chatbot.

---

\# Chatbot Message Persistence

Being able to store and load chat messages is crucial for most AI chatbots.

In this guide, we'll show how to implement message persistence with `useChat` and `streamText`.

<Note>

&nbsp; This guide does not cover authorization, error handling, or other real-world

&nbsp; considerations. It is intended to be a simple example of how to implement

&nbsp; message persistence.

</Note>

\## Starting a new chat

When the user navigates to the chat page without providing a chat ID,

we need to create a new chat and redirect to the chat page with the new chat ID.

```tsx filename="app/chat/page.tsx"

import { redirect } from 'next/navigation';

import { createChat } from '@util/chat-store';



export default async function Page() {

&nbsp; const id = await createChat(); // create a new chat

&nbsp; redirect(`/chat/${id}`); // redirect to chat page, see below

}

```

Our example chat store implementation uses files to store the chat messages.

In a real-world application, you would use a database or a cloud storage service,

and get the chat ID from the database.

That being said, the function interfaces are designed to be easily replaced with other implementations.

```tsx filename="util/chat-store.ts"

import { generateId } from 'ai';

import { existsSync, mkdirSync } from 'fs';

import { writeFile } from 'fs/promises';

import path from 'path';



export async function createChat(): Promise<string> {

&nbsp; const id = generateId(); // generate a unique chat ID

&nbsp; await writeFile(getChatFile(id), '\[]'); // create an empty chat file

&nbsp; return id;

}



function getChatFile(id: string): string {

&nbsp; const chatDir = path.join(process.cwd(), '.chats');

&nbsp; if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });

&nbsp; return path.join(chatDir, `${id}.json`);

}

```

\## Loading an existing chat

When the user navigates to the chat page with a chat ID, we need to load the chat messages from storage.

The `loadChat` function in our file-based chat store is implemented as follows:

```tsx filename="util/chat-store.ts"

import { UIMessage } from 'ai';

import { readFile } from 'fs/promises';



export async function loadChat(id: string): Promise<UIMessage\[]> {

&nbsp; return JSON.parse(await readFile(getChatFile(id), 'utf8'));

}



// ... rest of the file

```

\## Validating messages on the server

When processing messages on the server that contain tool calls, custom metadata, or data parts, you should validate them using `validateUIMessages` before sending them to the model.

\### Validation with tools

When your messages include tool calls, validate them against your tool definitions:

```tsx filename="app/api/chat/route.ts" highlight="7-25,32-37"

import {

&nbsp; convertToModelMessages,

&nbsp; streamText,

&nbsp; UIMessage,

&nbsp; validateUIMessages,

&nbsp; tool,

} from 'ai';

import { z } from 'zod';

import { loadChat, saveChat } from '@util/chat-store';

import { openai } from '@ai-sdk/openai';

import { dataPartsSchema, metadataSchema } from '@util/schemas';



// Define your tools

const tools = {

&nbsp; weather: tool({

&nbsp;   description: 'Get weather information',

&nbsp;   parameters: z.object({

&nbsp;     location: z.string(),

&nbsp;     units: z.enum(\['celsius', 'fahrenheit']),

&nbsp;   }),

&nbsp;   execute: async ({ location, units }) => {

&nbsp;     /\* tool implementation \*/

&nbsp;   },

&nbsp; }),

&nbsp; // other tools

};



export async function POST(req: Request) {

&nbsp; const { message, id } = await req.json();



&nbsp; // Load previous messages from database

&nbsp; const previousMessages = await loadChat(id);



&nbsp; // Append new message to previousMessages messages

&nbsp; const messages = \[...previousMessages, message];



&nbsp; // Validate loaded messages against

&nbsp; // tools, data parts schema, and metadata schema

&nbsp; const validatedMessages = await validateUIMessages({

&nbsp;   messages,

&nbsp;   tools, // Ensures tool calls in messages match current schemas

&nbsp;   dataPartsSchema,

&nbsp;   metadataSchema,

&nbsp; });



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-5-mini',

&nbsp;   messages: convertToModelMessages(validatedMessages),

&nbsp;   tools,

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages,

&nbsp;   onFinish: ({ messages }) => {

&nbsp;     saveChat({ chatId: id, messages });

&nbsp;   },

&nbsp; });

}

```

\### Handling validation errors

Handle validation errors gracefully when messages from the database don't match current schemas:

```tsx filename="app/api/chat/route.ts" highlight="3,10-24"

import {

&nbsp; convertToModelMessages,

&nbsp; streamText,

&nbsp; validateUIMessages,

&nbsp; TypeValidationError,

} from 'ai';

import { type MyUIMessage } from '@/types';



export async function POST(req: Request) {

&nbsp; const { message, id } = await req.json();



&nbsp; // Load and validate messages from database

&nbsp; let validatedMessages: MyUIMessage\[];



&nbsp; try {

&nbsp;   const previousMessages = await loadMessagesFromDB(id);

&nbsp;   validatedMessages = await validateUIMessages({

&nbsp;     // append the new message to the previous messages:

&nbsp;     messages: \[...previousMessages, message],

&nbsp;     tools,

&nbsp;     metadataSchema,

&nbsp;   });

&nbsp; } catch (error) {

&nbsp;   if (error instanceof TypeValidationError) {

&nbsp;     // Log validation error for monitoring

&nbsp;     console.error('Database messages validation failed:', error);

&nbsp;     // Could implement message migration or filtering here

&nbsp;     // For now, start with empty history

&nbsp;     validatedMessages = \[];

&nbsp;   } else {

&nbsp;     throw error;

&nbsp;   }

&nbsp; }



&nbsp; // Continue with validated messages...

}

```

\## Displaying the chat

Once messages are loaded from storage, you can display them in your chat UI. Here's how to set up the page component and the chat display:

```tsx filename="app/chat/[id]/page.tsx"

import { loadChat } from '@util/chat-store';

import Chat from '@ui/chat';



export default async function Page(props: { params: Promise<{ id: string }> }) {

&nbsp; const { id } = await props.params;

&nbsp; const messages = await loadChat(id);

&nbsp; return <Chat id={id} initialMessages={messages} />;

}

```

The chat component uses the `useChat` hook to manage the conversation:

```tsx filename="ui/chat.tsx" highlight="10-16"

'use client';



import { UIMessage, useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import { useState } from 'react';



export default function Chat({

&nbsp; id,

&nbsp; initialMessages,

}: { id?: string | undefined; initialMessages?: UIMessage\[] } = {}) {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { sendMessage, messages } = useChat({

&nbsp;   id, // use the provided chat ID

&nbsp;   messages: initialMessages, // load initial messages

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   if (input.trim()) {

&nbsp;     sendMessage({ text: input });

&nbsp;     setInput('');

&nbsp;   }

&nbsp; };



&nbsp; // simplified rendering code, extend as needed:

&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id}>

&nbsp;         {m.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;         {m.parts

&nbsp;           .map(part => (part.type === 'text' ? part.text : ''))

&nbsp;           .join('')}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         placeholder="Type a message..."

&nbsp;       />

&nbsp;       <button type="submit">Send</button>

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

\## Storing messages

`useChat` sends the chat id and the messages to the backend.

<Note>

&nbsp; The `useChat` message format is different from the `ModelMessage` format. The

&nbsp; `useChat` message format is designed for frontend display, and contains

&nbsp; additional fields such as `id` and `createdAt`. We recommend storing the

&nbsp; messages in the `useChat` message format.

When loading messages from storage that contain tools, metadata, or custom data

parts, validate them using `validateUIMessages` before processing (see the

\[validation section](#validating-messages-from-database) above).

</Note>

Storing messages is done in the `onFinish` callback of the `toUIMessageStreamResponse` function.

`onFinish` receives the complete messages including the new AI response as `UIMessage\[]`.

```tsx filename="app/api/chat/route.ts" highlight="6,11-17"

import { openai } from '@ai-sdk/openai';

import { saveChat } from '@util/chat-store';

import { convertToModelMessages, streamText, UIMessage } from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages, chatId }: { messages: UIMessage\[]; chatId: string } =

&nbsp;   await req.json();



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-5-mini',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages,

&nbsp;   onFinish: ({ messages }) => {

&nbsp;     saveChat({ chatId, messages });

&nbsp;   },

&nbsp; });

}

```

The actual storage of the messages is done in the `saveChat` function, which in

our file-based chat store is implemented as follows:

```tsx filename="util/chat-store.ts"

import { UIMessage } from 'ai';

import { writeFile } from 'fs/promises';



export async function saveChat({

&nbsp; chatId,

&nbsp; messages,

}: {

&nbsp; chatId: string;

&nbsp; messages: UIMessage\[];

}): Promise<void> {

&nbsp; const content = JSON.stringify(messages, null, 2);

&nbsp; await writeFile(getChatFile(chatId), content);

}



// ... rest of the file

```

\## Message IDs

In addition to a chat ID, each message has an ID.

You can use this message ID to e.g. manipulate individual messages.

\### Client-side vs Server-side ID Generation

By default, message IDs are generated client-side:

\- User message IDs are generated by the `useChat` hook on the client

\- AI response message IDs are generated by `streamText` on the server

For applications without persistence, client-side ID generation works perfectly.

However, \*\*for persistence, you need server-side generated IDs\*\* to ensure consistency across sessions and prevent ID conflicts when messages are stored and retrieved.

\### Setting Up Server-side ID Generation

When implementing persistence, you have two options for generating server-side IDs:

1\. \*\*Using `generateMessageId` in `toUIMessageStreamResponse`\*\*

2\. \*\*Setting IDs in your start message part with `createUIMessageStream`\*\*

\#### Option 1: Using `generateMessageId` in `toUIMessageStreamResponse`

You can control the ID format by providing ID generators using \[`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator):

```tsx filename="app/api/chat/route.ts" highlight="7-11"

import { createIdGenerator, streamText } from 'ai';



export async function POST(req: Request) {

&nbsp; // ...

&nbsp; const result = streamText({

&nbsp;   // ...

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages,

&nbsp;   // Generate consistent server-side IDs for persistence:

&nbsp;   generateMessageId: createIdGenerator({

&nbsp;     prefix: 'msg',

&nbsp;     size: 16,

&nbsp;   }),

&nbsp;   onFinish: ({ messages }) => {

&nbsp;     saveChat({ chatId, messages });

&nbsp;   },

&nbsp; });

}

```

\#### Option 2: Setting IDs with `createUIMessageStream`

Alternatively, you can use `createUIMessageStream` to control the message ID by writing a start message part:

```tsx filename="app/api/chat/route.ts" highlight="8-18"

import {

&nbsp; generateId,

&nbsp; streamText,

&nbsp; createUIMessageStream,

&nbsp; createUIMessageStreamResponse,

} from 'ai';



export async function POST(req: Request) {

&nbsp; const { messages, chatId } = await req.json();



&nbsp; const stream = createUIMessageStream({

&nbsp;   execute: ({ writer }) => {

&nbsp;     // Write start message part with custom ID

&nbsp;     writer.write({

&nbsp;       type: 'start',

&nbsp;       messageId: generateId(), // Generate server-side ID for persistence

&nbsp;     });



&nbsp;     const result = streamText({

&nbsp;       model: 'openai/gpt-5-mini',

&nbsp;       messages: await convertToModelMessages(messages),

&nbsp;     });



&nbsp;     writer.merge(result.toUIMessageStream({ sendStart: false })); // omit start message part

&nbsp;   },

&nbsp;   originalMessages: messages,

&nbsp;   onFinish: ({ responseMessage }) => {

&nbsp;     // save your chat here

&nbsp;   },

&nbsp; });



&nbsp; return createUIMessageStreamResponse({ stream });

}

```

<Note>

&nbsp; For client-side applications that don't require persistence, you can still customize client-side ID generation:

```tsx filename="ui/chat.tsx"

import { createIdGenerator } from 'ai';

import { useChat } from '@ai-sdk/react';



const { ... } = useChat({

&nbsp; generateId: createIdGenerator({

&nbsp;   prefix: 'msgc',

&nbsp;   size: 16,

&nbsp; }),

&nbsp; // ...

});

```

</Note>

\## Sending only the last message

Once you have implemented message persistence, you might want to send only the last message to the server.

This reduces the amount of data sent to the server on each request and can improve performance.

To achieve this, you can provide a `prepareSendMessagesRequest` function to the transport.

This function receives the messages and the chat ID, and returns the request body to be sent to the server.

```tsx filename="ui/chat.tsx" highlight="7-12"

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



const {

&nbsp; // ...

} = useChat({

&nbsp; // ...

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/chat',

&nbsp;   // only send the last message to the server:

&nbsp;   prepareSendMessagesRequest({ messages, id }) {

&nbsp;     return { body: { message: messages\[messages.length - 1], id } };

&nbsp;   },

&nbsp; }),

});

```

On the server, you can then load the previous messages and append the new message to the previous messages. If your messages contain tools, metadata, or custom data parts, you should validate them:

```tsx filename="app/api/chat/route.ts" highlight="2-11,14-18"

import { convertToModelMessages, UIMessage, validateUIMessages } from 'ai';

// import your tools and schemas



export async function POST(req: Request) {

&nbsp; // get the last message from the client:

&nbsp; const { message, id } = await req.json();



&nbsp; // load the previous messages from the server:

&nbsp; const previousMessages = await loadChat(id);



&nbsp; // validate messages if they contain tools, metadata, or data parts:

&nbsp; const validatedMessages = await validateUIMessages({

&nbsp;   // append the new message to the previous messages:

&nbsp;   messages: \[...previousMessages, message],

&nbsp;   tools, // if using tools

&nbsp;   metadataSchema, // if using custom metadata

&nbsp;   dataSchemas, // if using custom data parts

&nbsp; });



&nbsp; const result = streamText({

&nbsp;   // ...

&nbsp;   messages: convertToModelMessages(validatedMessages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: validatedMessages,

&nbsp;   onFinish: ({ messages }) => {

&nbsp;     saveChat({ chatId: id, messages });

&nbsp;   },

&nbsp; });

}

```

\## Handling client disconnects

By default, the AI SDK `streamText` function uses backpressure to the language model provider to prevent

the consumption of tokens that are not yet requested.

However, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue,

the stream from the LLM will be aborted and the conversation may end up in a broken state.

Assuming that you have a \[storage solution](#storing-messages) in place, you can use the `consumeStream` method to consume the stream on the backend,

and then save the result as usual.

`consumeStream` effectively removes the backpressure,

meaning that the result is stored even when the client has already disconnected.

```tsx filename="app/api/chat/route.ts" highlight="19-21"

import { convertToModelMessages, streamText, UIMessage } from 'ai';

import { saveChat } from '@util/chat-store';



export async function POST(req: Request) {

&nbsp; const { messages, chatId }: { messages: UIMessage\[]; chatId: string } =

&nbsp;   await req.json();



&nbsp; const result = streamText({

&nbsp;   model,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; // consume the stream to ensure it runs to completion \& triggers onFinish

&nbsp; // even when the client response is aborted:

&nbsp; result.consumeStream(); // no await



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages,

&nbsp;   onFinish: ({ messages }) => {

&nbsp;     saveChat({ chatId, messages });

&nbsp;   },

&nbsp; });

}

```

When the client reloads the page after a disconnect, the chat will be restored from the storage solution.

<Note>

&nbsp; In production applications, you would also track the state of the request (in

&nbsp; progress, complete) in your stored messages and use it on the client to cover

&nbsp; the case where the client reloads the page after a disconnection, but the

&nbsp; streaming is not yet complete.

</Note>

For more robust handling of disconnects, you may want to add resumability on disconnects. Check out the \[Chatbot Resume Streams](/docs/ai-sdk-ui/chatbot-resume-streams) documentation to learn more.

---

title: Chatbot Resume Streams

description: Learn how to resume chatbot streams after client disconnects.

---

\# Chatbot Resume Streams

`useChat` supports resuming ongoing streams after page reloads. Use this feature to build applications with long-running generations.

<Note type="warning">

&nbsp; Stream resumption is not compatible with abort functionality. Closing a tab or

&nbsp; refreshing the page triggers an abort signal that will break the resumption

&nbsp; mechanism. Do not use `resume: true` if you need abort functionality in your

&nbsp; application. See

&nbsp; \[troubleshooting](/docs/troubleshooting/abort-breaks-resumable-streams) for

&nbsp; more details.

</Note>

\## How stream resumption works

Stream resumption requires persistence for messages and active streams in your application. The AI SDK provides tools to connect to storage, but you need to set up the storage yourself.

\*\*The AI SDK provides:\*\*

\- A `resume` option in `useChat` that automatically reconnects to active streams

\- Access to the outgoing stream through the `consumeSseStream` callback

\- Automatic HTTP requests to your resume endpoints

\*\*You build:\*\*

\- Storage to track which stream belongs to each chat

\- Redis to store the UIMessage stream

\- Two API endpoints: POST to create streams, GET to resume them

\- Integration with \[`resumable-stream`](https://www.npmjs.com/package/resumable-stream) to manage Redis storage

\## Prerequisites

To implement resumable streams in your chat application, you need:

1\. \*\*The `resumable-stream` package\*\* - Handles the publisher/subscriber mechanism for streams

2\. \*\*A Redis instance\*\* - Stores stream data (e.g. \[Redis through Vercel](https://vercel.com/marketplace/redis))

3\. \*\*A persistence layer\*\* - Tracks which stream ID is active for each chat (e.g. database)

\## Implementation

\### 1. Client-side: Enable stream resumption

Use the `resume` option in the `useChat` hook to enable stream resumption. When `resume` is true, the hook automatically attempts to reconnect to any active stream for the chat on mount:

```tsx filename="app/chat/[chatId]/chat.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport, type UIMessage } from 'ai';



export function Chat({

&nbsp; chatData,

&nbsp; resume = false,

}: {

&nbsp; chatData: { id: string; messages: UIMessage\[] };

&nbsp; resume?: boolean;

}) {

&nbsp; const { messages, sendMessage, status } = useChat({

&nbsp;   id: chatData.id,

&nbsp;   messages: chatData.messages,

&nbsp;   resume, // Enable automatic stream resumption

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     // You must send the id of the chat

&nbsp;     prepareSendMessagesRequest: ({ id, messages }) => {

&nbsp;       return {

&nbsp;         body: {

&nbsp;           id,

&nbsp;           message: messages\[messages.length - 1],

&nbsp;         },

&nbsp;       };

&nbsp;     },

&nbsp;   }),

&nbsp; });



&nbsp; return <div>{/\* Your chat UI \*/}</div>;

}

```

<Note>

&nbsp; You must send the chat ID with each request (see

&nbsp; `prepareSendMessagesRequest`).

</Note>

When you enable `resume`, the `useChat` hook makes a `GET` request to `/api/chat/\[id]/stream` on mount to check for and resume any active streams.

Let's start by creating the POST handler to create the resumable stream.

\### 2. Create the POST handler

The POST handler creates resumable streams using the `consumeSseStream` callback:

```ts filename="app/api/chat/route.ts"

import { openai } from '@ai-sdk/openai';

import { readChat, saveChat } from '@util/chat-store';

import {

&nbsp; convertToModelMessages,

&nbsp; generateId,

&nbsp; streamText,

&nbsp; type UIMessage,

} from 'ai';

import { after } from 'next/server';

import { createResumableStreamContext } from 'resumable-stream';



export async function POST(req: Request) {

&nbsp; const {

&nbsp;   message,

&nbsp;   id,

&nbsp; }: {

&nbsp;   message: UIMessage | undefined;

&nbsp;   id: string;

&nbsp; } = await req.json();



&nbsp; const chat = await readChat(id);

&nbsp; let messages = chat.messages;



&nbsp; messages = \[...messages, message!];



&nbsp; // Clear any previous active stream and save the user message

&nbsp; saveChat({ id, messages, activeStreamId: null });



&nbsp; const result = streamText({

&nbsp;   model: 'openai/gpt-5-mini',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages,

&nbsp;   generateMessageId: generateId,

&nbsp;   onFinish: ({ messages }) => {

&nbsp;     // Clear the active stream when finished

&nbsp;     saveChat({ id, messages, activeStreamId: null });

&nbsp;   },

&nbsp;   async consumeSseStream({ stream }) {

&nbsp;     const streamId = generateId();



&nbsp;     // Create a resumable stream from the SSE stream

&nbsp;     const streamContext = createResumableStreamContext({ waitUntil: after });

&nbsp;     await streamContext.createNewResumableStream(streamId, () => stream);



&nbsp;     // Update the chat with the active stream ID

&nbsp;     saveChat({ id, activeStreamId: streamId });

&nbsp;   },

&nbsp; });

}

```

\### 3. Implement the GET handler

Create a GET handler at `/api/chat/\[id]/stream` that:

1\. Reads the chat ID from the route params

2\. Loads the chat data to check for an active stream

3\. Returns 204 (No Content) if no stream is active

4\. Resumes the existing stream if one is found

```ts filename="app/api/chat/[id]/stream/route.ts"

import { readChat } from '@util/chat-store';

import { UI\_MESSAGE\_STREAM\_HEADERS } from 'ai';

import { after } from 'next/server';

import { createResumableStreamContext } from 'resumable-stream';



export async function GET(

&nbsp; \_: Request,

&nbsp; { params }: { params: Promise<{ id: string }> },

) {

&nbsp; const { id } = await params;



&nbsp; const chat = await readChat(id);



&nbsp; if (chat.activeStreamId == null) {

&nbsp;   // no content response when there is no active stream

&nbsp;   return new Response(null, { status: 204 });

&nbsp; }



&nbsp; const streamContext = createResumableStreamContext({

&nbsp;   waitUntil: after,

&nbsp; });



&nbsp; return new Response(

&nbsp;   await streamContext.resumeExistingStream(chat.activeStreamId),

&nbsp;   { headers: UI\_MESSAGE\_STREAM\_HEADERS },

&nbsp; );

}

```

<Note>

&nbsp; The `after` function from Next.js allows work to continue after the response

&nbsp; has been sent. This ensures that the resumable stream persists in Redis even

&nbsp; after the initial response is returned to the client, enabling reconnection

&nbsp; later.

</Note>

\## How it works

\### Request lifecycle

!\[Diagram showing the architecture and lifecycle of resumable stream requests](https://e742qlubrjnjqpp0.public.blob.vercel-storage.com/resume-stream-diagram.png)

The diagram above shows the complete lifecycle of a resumable stream:

1\. \*\*Stream creation\*\*: When you send a new message, the POST handler uses `streamText` to generate the response. The `consumeSseStream` callback creates a resumable stream with a unique ID and stores it in Redis through the `resumable-stream` package

2\. \*\*Stream tracking\*\*: Your persistence layer saves the `activeStreamId` in the chat data

3\. \*\*Client reconnection\*\*: When the client reconnects (page reload), the `resume` option triggers a GET request to `/api/chat/\[id]/stream`

4\. \*\*Stream recovery\*\*: The GET handler checks for an `activeStreamId` and uses `resumeExistingStream` to reconnect. If no active stream exists, it returns a 204 (No Content) response

5\. \*\*Completion cleanup\*\*: When the stream finishes, the `onFinish` callback clears the `activeStreamId` by setting it to `null`

\## Customize the resume endpoint

By default, the `useChat` hook makes a GET request to `/api/chat/\[id]/stream` when resuming. Customize this endpoint, credentials, and headers, using the `prepareReconnectToStreamRequest` option in `DefaultChatTransport`:

```tsx filename="app/chat/[chatId]/chat.tsx"

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



export function Chat({ chatData, resume }) {

&nbsp; const { messages, sendMessage } = useChat({

&nbsp;   id: chatData.id,

&nbsp;   messages: chatData.messages,

&nbsp;   resume,

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     // Customize reconnect settings (optional)

&nbsp;     prepareReconnectToStreamRequest: ({ id }) => {

&nbsp;       return {

&nbsp;         api: `/api/chat/${id}/stream`, // Default pattern

&nbsp;         // Or use a different pattern:

&nbsp;         // api: `/api/streams/${id}/resume`,

&nbsp;         // api: `/api/resume-chat?id=${id}`,

&nbsp;         credentials: 'include', // Include cookies/auth

&nbsp;         headers: {

&nbsp;           Authorization: 'Bearer token',

&nbsp;           'X-Custom-Header': 'value',

&nbsp;         },

&nbsp;       };

&nbsp;     },

&nbsp;   }),

&nbsp; });



&nbsp; return <div>{/\* Your chat UI \*/}</div>;

}

```

This lets you:

\- Match your existing API route structure

\- Add query parameters or custom paths

\- Integrate with different backend architectures

\## Important considerations

\- \*\*Incompatibility with abort\*\*: Stream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use `resume: true` if you need abort functionality in your application

\- \*\*Stream expiration\*\*: Streams in Redis expire after a set time (configurable in the `resumable-stream` package)

\- \*\*Multiple clients\*\*: Multiple clients can connect to the same stream simultaneously

\- \*\*Error handling\*\*: When no active stream exists, the GET handler returns a 204 (No Content) status code

\- \*\*Security\*\*: Ensure proper authentication and authorization for both creating and resuming streams

\- \*\*Race conditions\*\*: Clear the `activeStreamId` when starting a new stream to prevent resuming outdated streams

<br />

<GithubLink link="https://github.com/vercel/ai/blob/main/examples/next" />

---

title: Chatbot Tool Usage

description: Learn how to use tools with the useChat hook.

---

\# Chatbot Tool Usage

With \[`useChat`](/docs/reference/ai-sdk-ui/use-chat) and \[`streamText`](/docs/reference/ai-sdk-core/stream-text), you can use tools in your chatbot application.

The AI SDK supports three types of tools in this context:

1\. Automatically executed server-side tools

2\. Automatically executed client-side tools

3\. Tools that require user interaction, such as confirmation dialogs

The flow is as follows:

1\. The user enters a message in the chat UI.

1\. The message is sent to the API route.

1\. In your server side route, the language model generates tool calls during the `streamText` call.

1\. All tool calls are forwarded to the client.

1\. Server-side tools are executed using their `execute` method and their results are forwarded to the client.

1\. Client-side tools that should be automatically executed are handled with the `onToolCall` callback.

&nbsp; You must call `addToolOutput` to provide the tool result.

1\. Client-side tool that require user interactions can be displayed in the UI.

&nbsp; The tool calls and results are available as tool invocation parts in the `parts` property of the last assistant message.

1\. When the user interaction is done, `addToolOutput` can be used to add the tool result to the chat.

1\. The chat can be configured to automatically submit when all tool results are available using `sendAutomaticallyWhen`.

&nbsp; This triggers another iteration of this flow.

The tool calls and tool executions are integrated into the assistant message as typed tool parts.

A tool part is at first a tool call, and then it becomes a tool result when the tool is executed.

The tool result contains all information about the tool call as well as the result of the tool execution.

<Note>

&nbsp; Tool result submission can be configured using the `sendAutomaticallyWhen`

&nbsp; option. You can use the `lastAssistantMessageIsCompleteWithToolCalls` helper

&nbsp; to automatically submit when all tool results are available. This simplifies

&nbsp; the client-side code while still allowing full control when needed.

</Note>

\## Example

In this example, we'll use three tools:

\- `getWeatherInformation`: An automatically executed server-side tool that returns the weather in a given city.

\- `askForConfirmation`: A user-interaction client-side tool that asks the user for confirmation.

\- `getLocation`: An automatically executed client-side tool that returns a random city.

\### API route

```tsx filename='app/api/chat/route.ts'

import { convertToModelMessages, streamText, UIMessage } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   tools: {

&nbsp;     // server-side tool with execute function:

&nbsp;     getWeatherInformation: {

&nbsp;       description: 'show the weather in a given city to the user',

&nbsp;       inputSchema: z.object({ city: z.string() }),

&nbsp;       execute: async ({}: { city: string }) => {

&nbsp;         const weatherOptions = \['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];

&nbsp;         return weatherOptions\[

&nbsp;           Math.floor(Math.random() \* weatherOptions.length)

&nbsp;         ];

&nbsp;       },

&nbsp;     },

&nbsp;     // client-side tool that starts user interaction:

&nbsp;     askForConfirmation: {

&nbsp;       description: 'Ask the user for confirmation.',

&nbsp;       inputSchema: z.object({

&nbsp;         message: z.string().describe('The message to ask for confirmation.'),

&nbsp;       }),

&nbsp;     },

&nbsp;     // client-side tool that is automatically executed on the client:

&nbsp;     getLocation: {

&nbsp;       description:

&nbsp;         'Get the user location. Always ask for confirmation before using this tool.',

&nbsp;       inputSchema: z.object({}),

&nbsp;     },

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

\### Client-side page

The client-side page uses the `useChat` hook to create a chatbot application with real-time message streaming.

Tool calls are displayed in the chat UI as typed tool parts.

Please make sure to render the messages using the `parts` property of the message.

There are three things worth mentioning:

1\. The \[`onToolCall`](/docs/reference/ai-sdk-ui/use-chat#on-tool-call) callback is used to handle client-side tools that should be automatically executed.

&nbsp; In this example, the `getLocation` tool is a client-side tool that returns a random city.

&nbsp; You call `addToolOutput` to provide the result (without `await` to avoid potential deadlocks).

&nbsp; <Note>

&nbsp; Always check `if (toolCall.dynamic)` first in your `onToolCall` handler.

&nbsp; Without this check, TypeScript will throw an error like: `Type 'string' is

&nbsp; not assignable to type '"toolName1" | "toolName2"'` when you try to use

&nbsp; `toolCall.toolName` in `addToolOutput`.

&nbsp; </Note>

2\. The \[`sendAutomaticallyWhen`](/docs/reference/ai-sdk-ui/use-chat#send-automatically-when) option with `lastAssistantMessageIsCompleteWithToolCalls` helper automatically submits when all tool results are available.

3\. The `parts` array of assistant messages contains tool parts with typed names like `tool-askForConfirmation`.

&nbsp; The client-side tool `askForConfirmation` is displayed in the UI.

&nbsp; It asks the user for confirmation and displays the result once the user confirms or denies the execution.

&nbsp; The result is added to the chat using `addToolOutput` with the `tool` parameter for type safety.

```tsx filename='app/page.tsx' highlight="2,6,10,14-20"

'use client';



import { useChat } from '@ai-sdk/react';

import {

&nbsp; DefaultChatTransport,

&nbsp; lastAssistantMessageIsCompleteWithToolCalls,

} from 'ai';

import { useState } from 'react';



export default function Chat() {

&nbsp; const { messages, sendMessage, addToolOutput } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),



&nbsp;   sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,



&nbsp;   // run client-side tools that are automatically executed:

&nbsp;   async onToolCall({ toolCall }) {

&nbsp;     // Check if it's a dynamic tool first for proper type narrowing

&nbsp;     if (toolCall.dynamic) {

&nbsp;       return;

&nbsp;     }



&nbsp;     if (toolCall.toolName === 'getLocation') {

&nbsp;       const cities = \['New York', 'Los Angeles', 'Chicago', 'San Francisco'];



&nbsp;       // No await - avoids potential deadlocks

&nbsp;       addToolOutput({

&nbsp;         tool: 'getLocation',

&nbsp;         toolCallId: toolCall.toolCallId,

&nbsp;         output: cities\[Math.floor(Math.random() \* cities.length)],

&nbsp;       });

&nbsp;     }

&nbsp;   },

&nbsp; });

&nbsp; const \[input, setInput] = useState('');



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages?.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         <strong>{`${message.role}: `}</strong>

&nbsp;         {message.parts.map(part => {

&nbsp;           switch (part.type) {

&nbsp;             // render text parts as simple text:

&nbsp;             case 'text':

&nbsp;               return part.text;



&nbsp;             // for tool parts, use the typed tool part names:

&nbsp;             case 'tool-askForConfirmation': {

&nbsp;               const callId = part.toolCallId;



&nbsp;               switch (part.state) {

&nbsp;                 case 'input-streaming':

&nbsp;                   return (

&nbsp;                     <div key={callId}>Loading confirmation request...</div>

&nbsp;                   );

&nbsp;                 case 'input-available':

&nbsp;                   return (

&nbsp;                     <div key={callId}>

&nbsp;                       {part.input.message}

&nbsp;                       <div>

&nbsp;                         <button

&nbsp;                           onClick={() =>

&nbsp;                             addToolOutput({

&nbsp;                               tool: 'askForConfirmation',

&nbsp;                               toolCallId: callId,

&nbsp;                               output: 'Yes, confirmed.',

&nbsp;                             })

&nbsp;                           }

&nbsp;                         >

&nbsp;                           Yes

&nbsp;                         </button>

&nbsp;                         <button

&nbsp;                           onClick={() =>

&nbsp;                             addToolOutput({

&nbsp;                               tool: 'askForConfirmation',

&nbsp;                               toolCallId: callId,

&nbsp;                               output: 'No, denied',

&nbsp;                             })

&nbsp;                           }

&nbsp;                         >

&nbsp;                           No

&nbsp;                         </button>

&nbsp;                       </div>

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-available':

&nbsp;                   return (

&nbsp;                     <div key={callId}>

&nbsp;                       Location access allowed: {part.output}

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-error':

&nbsp;                   return <div key={callId}>Error: {part.errorText}</div>;

&nbsp;               }

&nbsp;               break;

&nbsp;             }



&nbsp;             case 'tool-getLocation': {

&nbsp;               const callId = part.toolCallId;



&nbsp;               switch (part.state) {

&nbsp;                 case 'input-streaming':

&nbsp;                   return (

&nbsp;                     <div key={callId}>Preparing location request...</div>

&nbsp;                   );

&nbsp;                 case 'input-available':

&nbsp;                   return <div key={callId}>Getting location...</div>;

&nbsp;                 case 'output-available':

&nbsp;                   return <div key={callId}>Location: {part.output}</div>;

&nbsp;                 case 'output-error':

&nbsp;                   return (

&nbsp;                     <div key={callId}>

&nbsp;                       Error getting location: {part.errorText}

&nbsp;                     </div>

&nbsp;                   );

&nbsp;               }

&nbsp;               break;

&nbsp;             }



&nbsp;             case 'tool-getWeatherInformation': {

&nbsp;               const callId = part.toolCallId;



&nbsp;               switch (part.state) {

&nbsp;                 // example of pre-rendering streaming tool inputs:

&nbsp;                 case 'input-streaming':

&nbsp;                   return (

&nbsp;                     <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>

&nbsp;                   );

&nbsp;                 case 'input-available':

&nbsp;                   return (

&nbsp;                     <div key={callId}>

&nbsp;                       Getting weather information for {part.input.city}...

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-available':

&nbsp;                   return (

&nbsp;                     <div key={callId}>

&nbsp;                       Weather in {part.input.city}: {part.output}

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-error':

&nbsp;                   return (

&nbsp;                     <div key={callId}>

&nbsp;                       Error getting weather for {part.input.city}:{' '}

&nbsp;                       {part.errorText}

&nbsp;                     </div>

&nbsp;                   );

&nbsp;               }

&nbsp;               break;

&nbsp;             }

&nbsp;           }

&nbsp;         })}

&nbsp;         <br />

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form

&nbsp;       onSubmit={e => {

&nbsp;         e.preventDefault();

&nbsp;         if (input.trim()) {

&nbsp;           sendMessage({ text: input });

&nbsp;           setInput('');

&nbsp;         }

&nbsp;       }}

&nbsp;     >

&nbsp;       <input value={input} onChange={e => setInput(e.target.value)} />

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

\### Error handling

Sometimes an error may occur during client-side tool execution. Use the `addToolOutput` method with a `state` of `output-error` and `errorText` value instead of `output` record the error.

```tsx filename='app/page.tsx' highlight="19,36-41"

'use client';



import { useChat } from '@ai-sdk/react';

import {

&nbsp; DefaultChatTransport,

&nbsp; lastAssistantMessageIsCompleteWithToolCalls,

} from 'ai';

import { useState } from 'react';



export default function Chat() {

&nbsp; const { messages, sendMessage, addToolOutput } = useChat({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),



&nbsp;   sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,



&nbsp;   // run client-side tools that are automatically executed:

&nbsp;   async onToolCall({ toolCall }) {

&nbsp;     // Check if it's a dynamic tool first for proper type narrowing

&nbsp;     if (toolCall.dynamic) {

&nbsp;       return;

&nbsp;     }



&nbsp;     if (toolCall.toolName === 'getWeatherInformation') {

&nbsp;       try {

&nbsp;         const weather = await getWeatherInformation(toolCall.input);



&nbsp;         // No await - avoids potential deadlocks

&nbsp;         addToolOutput({

&nbsp;           tool: 'getWeatherInformation',

&nbsp;           toolCallId: toolCall.toolCallId,

&nbsp;           output: weather,

&nbsp;         });

&nbsp;       } catch (err) {

&nbsp;         addToolOutput({

&nbsp;           tool: 'getWeatherInformation',

&nbsp;           toolCallId: toolCall.toolCallId,

&nbsp;           state: 'output-error',

&nbsp;           errorText: 'Unable to get the weather information',

&nbsp;         });

&nbsp;       }

&nbsp;     }

&nbsp;   },

&nbsp; });

}

```

\## Tool Execution Approval

Tool execution approval lets you require user confirmation before a server-side tool runs. Unlike \[client-side tools](#example) that execute in the browser, tools with approval still execute on the server—but only after the user approves.

Use tool execution approval when you want to:

\- Confirm sensitive operations (payments, deletions, external API calls)

\- Let users review tool inputs before execution

\- Add human oversight to automated workflows

For tools that need to run in the browser (updating UI state, accessing browser APIs), use client-side tools instead.

\### Server Setup

Enable approval by setting `needsApproval` on your tool. See \[Tool Execution Approval](/docs/ai-sdk-core/tools-and-tool-calling#tool-execution-approval) for configuration options including dynamic approval based on input.

```tsx filename='app/api/chat/route.ts'

import { streamText, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



export async function POST(req: Request) {

&nbsp; const { messages } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages,

&nbsp;   tools: {

&nbsp;     getWeather: tool({

&nbsp;       description: 'Get the weather in a location',

&nbsp;       inputSchema: z.object({

&nbsp;         city: z.string(),

&nbsp;       }),

&nbsp;       needsApproval: true,

&nbsp;       execute: async ({ city }) => {

&nbsp;         const weather = await fetchWeather(city);

&nbsp;         return weather;

&nbsp;       },

&nbsp;     }),

&nbsp;   },

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

\### Client-Side Approval UI

When a tool requires approval, the tool part state is `approval-requested`. Use `addToolApprovalResponse` to approve or deny:

```tsx filename='app/page.tsx'

'use client';



import { useChat } from '@ai-sdk/react';



export default function Chat() {

&nbsp; const { messages, addToolApprovalResponse } = useChat();



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.parts.map(part => {

&nbsp;           if (part.type === 'tool-getWeather') {

&nbsp;             switch (part.state) {

&nbsp;               case 'approval-requested':

&nbsp;                 return (

&nbsp;                   <div key={part.toolCallId}>

&nbsp;                     <p>Get weather for {part.input.city}?</p>

&nbsp;                     <button

&nbsp;                       onClick={() =>

&nbsp;                         addToolApprovalResponse({

&nbsp;                           id: part.approval.id,

&nbsp;                           approved: true,

&nbsp;                         })

&nbsp;                       }

&nbsp;                     >

&nbsp;                       Approve

&nbsp;                     </button>

&nbsp;                     <button

&nbsp;                       onClick={() =>

&nbsp;                         addToolApprovalResponse({

&nbsp;                           id: part.approval.id,

&nbsp;                           approved: false,

&nbsp;                         })

&nbsp;                       }

&nbsp;                     >

&nbsp;                       Deny

&nbsp;                     </button>

&nbsp;                   </div>

&nbsp;                 );

&nbsp;               case 'output-available':

&nbsp;                 return (

&nbsp;                   <div key={part.toolCallId}>

&nbsp;                     Weather in {part.input.city}: {part.output}

&nbsp;                   </div>

&nbsp;                 );

&nbsp;             }

&nbsp;           }

&nbsp;           // Handle other part types...

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </>

&nbsp; );

}

```

\### Auto-Submit After Approval

<Note>

&nbsp; If nothing happens after you approve a tool execution, make sure you either

&nbsp; call `sendMessage` manually or configure `sendAutomaticallyWhen` on the

&nbsp; `useChat` hook.

</Note>

Use `lastAssistantMessageIsCompleteWithApprovalResponses` to automatically continue the conversation after approvals:

```tsx

import { useChat } from '@ai-sdk/react';

import { lastAssistantMessageIsCompleteWithApprovalResponses } from 'ai';



const { messages, addToolApprovalResponse } = useChat({

&nbsp; sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithApprovalResponses,

});

```

\## Dynamic Tools

When using dynamic tools (tools with unknown types at compile time), the UI parts use a generic `dynamic-tool` type instead of specific tool types:

```tsx filename='app/page.tsx'

{

&nbsp; message.parts.map((part, index) => {

&nbsp;   switch (part.type) {

&nbsp;     // Static tools with specific (`tool-${toolName}`) types

&nbsp;     case 'tool-getWeatherInformation':

&nbsp;       return <WeatherDisplay part={part} />;



&nbsp;     // Dynamic tools use generic `dynamic-tool` type

&nbsp;     case 'dynamic-tool':

&nbsp;       return (

&nbsp;         <div key={index}>

&nbsp;           <h4>Tool: {part.toolName}</h4>

&nbsp;           {part.state === 'input-streaming' \&\& (

&nbsp;             <pre>{JSON.stringify(part.input, null, 2)}</pre>

&nbsp;           )}

&nbsp;           {part.state === 'output-available' \&\& (

&nbsp;             <pre>{JSON.stringify(part.output, null, 2)}</pre>

&nbsp;           )}

&nbsp;           {part.state === 'output-error' \&\& (

&nbsp;             <div>Error: {part.errorText}</div>

&nbsp;           )}

&nbsp;         </div>

&nbsp;       );

&nbsp;   }

&nbsp; });

}

```

Dynamic tools are useful when integrating with:

\- MCP (Model Context Protocol) tools without schemas

\- User-defined functions loaded at runtime

\- External tool providers

\## Tool call streaming

Tool call streaming is \*\*enabled by default\*\* in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.

```tsx filename='app/api/chat/route.ts'

export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   // toolCallStreaming is enabled by default in v5

&nbsp;   // ...

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

With tool call streaming enabled, partial tool calls are streamed as part of the data stream.

They are available through the `useChat` hook.

The typed tool parts of assistant messages will also contain partial tool calls.

You can use the `state` property of the tool part to render the correct UI.

```tsx filename='app/page.tsx' highlight="9,10"

export default function Chat() {

&nbsp; // ...

&nbsp; return (

&nbsp;   <>

&nbsp;     {messages?.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.parts.map(part => {

&nbsp;           switch (part.type) {

&nbsp;             case 'tool-askForConfirmation':

&nbsp;             case 'tool-getLocation':

&nbsp;             case 'tool-getWeatherInformation':

&nbsp;               switch (part.state) {

&nbsp;                 case 'input-streaming':

&nbsp;                   return <pre>{JSON.stringify(part.input, null, 2)}</pre>;

&nbsp;                 case 'input-available':

&nbsp;                   return <pre>{JSON.stringify(part.input, null, 2)}</pre>;

&nbsp;                 case 'output-available':

&nbsp;                   return <pre>{JSON.stringify(part.output, null, 2)}</pre>;

&nbsp;                 case 'output-error':

&nbsp;                   return <div>Error: {part.errorText}</div>;

&nbsp;               }

&nbsp;           }

&nbsp;         })}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </>

&nbsp; );

}

```

\## Step start parts

When you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages.

If you want to display boundaries between tool calls, you can use the `step-start` parts as follows:

```tsx filename='app/page.tsx'

// ...

// where you render the message parts:

message.parts.map((part, index) => {

&nbsp; switch (part.type) {

&nbsp;   case 'step-start':

&nbsp;     // show step boundaries as horizontal lines:

&nbsp;     return index > 0 ? (

&nbsp;       <div key={index} className="text-gray-500">

&nbsp;         <hr className="my-2 border-gray-300" />

&nbsp;       </div>

&nbsp;     ) : null;

&nbsp;   case 'text':

&nbsp;   // ...

&nbsp;   case 'tool-askForConfirmation':

&nbsp;   case 'tool-getLocation':

&nbsp;   case 'tool-getWeatherInformation':

&nbsp;   // ...

&nbsp; }

});

// ...

```

\## Server-side Multi-Step Calls

You can also use multi-step calls on the server-side with `streamText`.

This works when all invoked tools have an `execute` function on the server side.

```tsx filename='app/api/chat/route.ts' highlight="15-21,24"

import { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   tools: {

&nbsp;     getWeatherInformation: {

&nbsp;       description: 'show the weather in a given city to the user',

&nbsp;       inputSchema: z.object({ city: z.string() }),

&nbsp;       // tool has execute function:

&nbsp;       execute: async ({}: { city: string }) => {

&nbsp;         const weatherOptions = \['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];

&nbsp;         return weatherOptions\[

&nbsp;           Math.floor(Math.random() \* weatherOptions.length)

&nbsp;         ];

&nbsp;       },

&nbsp;     },

&nbsp;   },

&nbsp;   stopWhen: stepCountIs(5),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

\## Errors

Language models can make errors when calling tools.

By default, these errors are masked for security reasons, and show up as "An error occurred" in the UI.

To surface the errors, you can use the `onError` function when calling `toUIMessageResponse`.

```tsx

export function errorHandler(error: unknown) {

&nbsp; if (error == null) {

&nbsp;   return 'unknown error';

&nbsp; }



&nbsp; if (typeof error === 'string') {

&nbsp;   return error;

&nbsp; }



&nbsp; if (error instanceof Error) {

&nbsp;   return error.message;

&nbsp; }



&nbsp; return JSON.stringify(error);

}

```

```tsx

const result = streamText({

&nbsp; // ...

});



return result.toUIMessageStreamResponse({

&nbsp; onError: errorHandler,

});

```

In case you are using `createUIMessageResponse`, you can use the `onError` function when calling `toUIMessageResponse`:

```tsx

const response = createUIMessageResponse({

&nbsp; // ...

&nbsp; async execute(dataStream) {

&nbsp;   // ...

&nbsp; },

&nbsp; onError: error => `Custom error: ${error.message}`,

});

```

---

title: Generative User Interfaces

description: Learn how to build Generative UI with AI SDK UI.

---

\# Generative User Interfaces

Generative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and "generate UI". This creates a more engaging and AI-native experience for users.

<WeatherSearch />

At the core of generative UI are \[ tools ](/docs/ai-sdk-core/tools-and-tool-calling), which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.

Generative UI is the process of connecting the results of a tool call to a React component. Here's how it works:

1\. You provide the model with a prompt or conversation history, along with a set of tools.

2\. Based on the context, the model may decide to call a tool.

3\. If a tool is called, it will execute and return data.

4\. This data can then be passed to a React component for rendering.

By passing the tool results to React components, you can create a generative UI experience that's more engaging and adaptive to your needs.

\## Build a Generative UI Chat Interface

Let's create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.

\### Basic Chat Implementation

Start with a basic chat implementation using the `useChat` hook:

```tsx filename="app/page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   sendMessage({ text: input });

&nbsp;   setInput('');

&nbsp; };



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>

&nbsp;         <div>

&nbsp;           {message.parts.map((part, index) => {

&nbsp;             if (part.type === 'text') {

&nbsp;               return <span key={index}>{part.text}</span>;

&nbsp;             }

&nbsp;             return null;

&nbsp;           })}

&nbsp;         </div>

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         placeholder="Type a message..."

&nbsp;       />

&nbsp;       <button type="submit">Send</button>

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

To handle the chat requests and model responses, set up an API route:

```ts filename="app/api/chat/route.ts"

import { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



export async function POST(request: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await request.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   system: 'You are a friendly assistant!',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   stopWhen: stepCountIs(5),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

This API route uses the `streamText` function to process chat messages and stream the model's responses back to the client.

\### Create a Tool

Before enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.

Create a new file called `ai/tools.ts` with the following content:

```ts filename="ai/tools.ts"

import { tool as createTool } from 'ai';

import { z } from 'zod';



export const weatherTool = createTool({

&nbsp; description: 'Display the weather for a location',

&nbsp; inputSchema: z.object({

&nbsp;   location: z.string().describe('The location to get the weather for'),

&nbsp; }),

&nbsp; execute: async function ({ location }) {

&nbsp;   await new Promise(resolve => setTimeout(resolve, 2000));

&nbsp;   return { weather: 'Sunny', temperature: 75, location };

&nbsp; },

});



export const tools = {

&nbsp; displayWeather: weatherTool,

};

```

In this file, you've created a tool called `weatherTool`. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.

\### Update the API Route

Update the API route to include the tool you've defined:

```ts filename="app/api/chat/route.ts" highlight="3,8,14"

import { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { tools } from '@/ai/tools';



export async function POST(request: Request) {

&nbsp; const { messages }: { messages: UIMessage\[] } = await request.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   system: 'You are a friendly assistant!',

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp;   stopWhen: stepCountIs(5),

&nbsp;   tools,

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

Now that you've defined the tool and added it to your `streamText` call, let's build a React component to display the weather information it returns.

\### Create UI Components

Create a new file called `components/weather.tsx`:

```tsx filename="components/weather.tsx"

type WeatherProps = {

&nbsp; temperature: number;

&nbsp; weather: string;

&nbsp; location: string;

};



export const Weather = ({ temperature, weather, location }: WeatherProps) => {

&nbsp; return (

&nbsp;   <div>

&nbsp;     <h2>Current Weather for {location}</h2>

&nbsp;     <p>Condition: {weather}</p>

&nbsp;     <p>Temperature: {temperature}°C</p>

&nbsp;   </div>

&nbsp; );

};

```

This component will display the weather information for a given location. It takes three props: `temperature`, `weather`, and `location` (exactly what the `weatherTool` returns).

\### Render the Weather Component

Now that you have your tool and corresponding React component, let's integrate them into your chat interface. You'll render the Weather component when the model calls the weather tool.

To check if the model has called a tool, you can check the `parts` array of the UIMessage object for tool-specific parts. In AI SDK 5.0, tool parts use typed naming: `tool-${toolName}` instead of generic types.

Update your `page.tsx` file:

```tsx filename="app/page.tsx" highlight="4,9,14-15,19-46"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';

import { Weather } from '@/components/weather';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   sendMessage({ text: input });

&nbsp;   setInput('');

&nbsp; };



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>

&nbsp;         <div>

&nbsp;           {message.parts.map((part, index) => {

&nbsp;             if (part.type === 'text') {

&nbsp;               return <span key={index}>{part.text}</span>;

&nbsp;             }



&nbsp;             if (part.type === 'tool-displayWeather') {

&nbsp;               switch (part.state) {

&nbsp;                 case 'input-available':

&nbsp;                   return <div key={index}>Loading weather...</div>;

&nbsp;                 case 'output-available':

&nbsp;                   return (

&nbsp;                     <div key={index}>

&nbsp;                       <Weather {...part.output} />

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-error':

&nbsp;                   return <div key={index}>Error: {part.errorText}</div>;

&nbsp;                 default:

&nbsp;                   return null;

&nbsp;               }

&nbsp;             }



&nbsp;             return null;

&nbsp;           })}

&nbsp;         </div>

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         placeholder="Type a message..."

&nbsp;       />

&nbsp;       <button type="submit">Send</button>

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

In this updated code snippet, you:

1\. Use manual input state management with `useState` instead of the built-in `input` and `handleInputChange`.

2\. Use `sendMessage` instead of `handleSubmit` to send messages.

3\. Check the `parts` array of each message for different content types.

4\. Handle tool parts with type `tool-displayWeather` and their different states (`input-available`, `output-available`, `output-error`).

This approach allows you to dynamically render UI components based on the model's responses, creating a more interactive and context-aware chat experience.

\## Expanding Your Generative UI Application

You can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here's how you can expand your application:

\### Adding More Tools

To add more tools, simply define them in your `ai/tools.ts` file:

```ts

// Add a new stock tool

export const stockTool = createTool({

&nbsp; description: 'Get price for a stock',

&nbsp; inputSchema: z.object({

&nbsp;   symbol: z.string().describe('The stock symbol to get the price for'),

&nbsp; }),

&nbsp; execute: async function ({ symbol }) {

&nbsp;   // Simulated API call

&nbsp;   await new Promise(resolve => setTimeout(resolve, 2000));

&nbsp;   return { symbol, price: 100 };

&nbsp; },

});



// Update the tools object

export const tools = {

&nbsp; displayWeather: weatherTool,

&nbsp; getStockPrice: stockTool,

};

```

Now, create a new file called `components/stock.tsx`:

```tsx

type StockProps = {

&nbsp; price: number;

&nbsp; symbol: string;

};



export const Stock = ({ price, symbol }: StockProps) => {

&nbsp; return (

&nbsp;   <div>

&nbsp;     <h2>Stock Information</h2>

&nbsp;     <p>Symbol: {symbol}</p>

&nbsp;     <p>Price: ${price}</p>

&nbsp;   </div>

&nbsp; );

};

```

Finally, update your `page.tsx` file to include the new Stock component:

```tsx

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';

import { Weather } from '@/components/weather';

import { Stock } from '@/components/stock';



export default function Page() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage } = useChat();



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   sendMessage({ text: input });

&nbsp;   setInput('');

&nbsp; };



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         <div>{message.role}</div>

&nbsp;         <div>

&nbsp;           {message.parts.map((part, index) => {

&nbsp;             if (part.type === 'text') {

&nbsp;               return <span key={index}>{part.text}</span>;

&nbsp;             }



&nbsp;             if (part.type === 'tool-displayWeather') {

&nbsp;               switch (part.state) {

&nbsp;                 case 'input-available':

&nbsp;                   return <div key={index}>Loading weather...</div>;

&nbsp;                 case 'output-available':

&nbsp;                   return (

&nbsp;                     <div key={index}>

&nbsp;                       <Weather {...part.output} />

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-error':

&nbsp;                   return <div key={index}>Error: {part.errorText}</div>;

&nbsp;                 default:

&nbsp;                   return null;

&nbsp;               }

&nbsp;             }



&nbsp;             if (part.type === 'tool-getStockPrice') {

&nbsp;               switch (part.state) {

&nbsp;                 case 'input-available':

&nbsp;                   return <div key={index}>Loading stock price...</div>;

&nbsp;                 case 'output-available':

&nbsp;                   return (

&nbsp;                     <div key={index}>

&nbsp;                       <Stock {...part.output} />

&nbsp;                     </div>

&nbsp;                   );

&nbsp;                 case 'output-error':

&nbsp;                   return <div key={index}>Error: {part.errorText}</div>;

&nbsp;                 default:

&nbsp;                   return null;

&nbsp;               }

&nbsp;             }



&nbsp;             return null;

&nbsp;           })}

&nbsp;         </div>

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         type="text"

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;       />

&nbsp;       <button type="submit">Send</button>

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

By following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.

---

title: Completion

description: Learn how to use the useCompletion hook.

---

\# Completion

The `useCompletion` hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.

<Note>

&nbsp; The `useCompletion` hook is now part of the `@ai-sdk/react` package.

</Note>

In this guide, you will learn how to use the `useCompletion` hook in your application to generate text completions and stream them in real-time to your users.

\## Example

```tsx filename='app/page.tsx'

'use client';



import { useCompletion } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { completion, input, handleInputChange, handleSubmit } = useCompletion({

&nbsp;   api: '/api/completion',

&nbsp; });



&nbsp; return (

&nbsp;   <form onSubmit={handleSubmit}>

&nbsp;     <input

&nbsp;       name="prompt"

&nbsp;       value={input}

&nbsp;       onChange={handleInputChange}

&nbsp;       id="input"

&nbsp;     />

&nbsp;     <button type="submit">Submit</button>

&nbsp;     <div>{completion}</div>

&nbsp;   </form>

&nbsp; );

}

```

```ts filename='app/api/completion/route.ts'

import { streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const { prompt }: { prompt: string } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   prompt,

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse();

}

```

In the `Page` component, the `useCompletion` hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.

This enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.

\## Customized UI

`useCompletion` also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.

\### Loading and error states

To show a loading spinner while the chatbot is processing the user's message, you can use the `isLoading` state returned by the `useCompletion` hook:

```tsx

const { isLoading, ... } = useCompletion()



return(

&nbsp; <>

&nbsp;   {isLoading ? <Spinner /> : null}

&nbsp; </>

)

```

Similarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:

```tsx

const { error, ... } = useCompletion()



useEffect(() => {

&nbsp; if (error) {

&nbsp;   toast.error(error.message)

&nbsp; }

}, \[error])



// Or display the error message in the UI:

return (

&nbsp; <>

&nbsp;   {error ? <div>{error.message}</div> : null}

&nbsp; </>

)

```

\### Controlled input

In the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.

The following example demonstrates how to use more granular APIs like `setInput` with your custom input and submit button components:

```tsx

const { input, setInput } = useCompletion();



return (

&nbsp; <>

&nbsp;   <MyCustomInput value={input} onChange={value => setInput(value)} />

&nbsp; </>

);

```

\### Cancelation

It's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useCompletion` hook.

```tsx

const { stop, isLoading, ... } = useCompletion()



return (

&nbsp; <>

&nbsp;   <button onClick={stop} disabled={!isLoading}>Stop</button>

&nbsp; </>

)

```

When the user clicks the "Stop" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.

\### Throttling UI Updates

<Note>This feature is currently only available for React.</Note>

By default, the `useCompletion` hook will trigger a render every time a new chunk is received.

You can throttle the UI updates with the `experimental\_throttle` option.

```tsx filename="page.tsx" highlight="2-3"

const { completion, ... } = useCompletion({

&nbsp; // Throttle the completion and data updates to 50ms:

&nbsp; experimental\_throttle: 50

})

```

\## Event Callbacks

`useCompletion` also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.

```tsx

const { ... } = useCompletion({

&nbsp; onResponse: (response: Response) => {

&nbsp;   console.log('Received response from server:', response)

&nbsp; },

&nbsp; onFinish: (prompt: string, completion: string) => {

&nbsp;   console.log('Finished streaming completion:', completion)

&nbsp; },

&nbsp; onError: (error: Error) => {

&nbsp;   console.error('An error occurred:', error)

&nbsp; },

})

```

It's worth noting that you can abort the processing by throwing an error in the `onResponse` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.

\## Configure Request Options

By default, the `useCompletion` hook sends a HTTP POST request to the `/api/completion` endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the `useCompletion` hook:

```tsx

const { messages, input, handleInputChange, handleSubmit } = useCompletion({

&nbsp; api: '/api/custom-completion',

&nbsp; headers: {

&nbsp;   Authorization: 'your\_token',

&nbsp; },

&nbsp; body: {

&nbsp;   user\_id: '123',

&nbsp; },

&nbsp; credentials: 'same-origin',

});

```

In this example, the `useCompletion` hook sends a POST request to the `/api/completion` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.

---

title: Object Generation

description: Learn how to use the useObject hook.

---

\# Object Generation

<Note>

&nbsp; `useObject` is an experimental feature and only available in React, Svelte,

&nbsp; and Vue.

</Note>

The \[`useObject`](/docs/reference/ai-sdk-ui/use-object) hook allows you to create interfaces that represent a structured JSON object that is being streamed.

In this guide, you will learn how to use the `useObject` hook in your application to generate UIs for structured data on the fly.

\## Example

The example shows a small notifications demo app that generates fake notifications in real-time.

\### Schema

It is helpful to set up the schema in a separate file that is imported on both the client and server.

```ts filename='app/api/notifications/schema.ts'

import { z } from 'zod';



// define a schema for the notifications

export const notificationSchema = z.object({

&nbsp; notifications: z.array(

&nbsp;   z.object({

&nbsp;     name: z.string().describe('Name of a fictional person.'),

&nbsp;     message: z.string().describe('Message. Do not use emojis or links.'),

&nbsp;   }),

&nbsp; ),

});

```

\### Client

The client uses \[`useObject`](/docs/reference/ai-sdk-ui/use-object) to stream the object generation process.

The results are partial and are displayed as they are received.

Please note the code for handling `undefined` values in the JSX.

```tsx filename='app/page.tsx'

'use client';



import { experimental\_useObject as useObject } from '@ai-sdk/react';

import { notificationSchema } from './api/notifications/schema';



export default function Page() {

&nbsp; const { object, submit } = useObject({

&nbsp;   api: '/api/notifications',

&nbsp;   schema: notificationSchema,

&nbsp; });



&nbsp; return (

&nbsp;   <>

&nbsp;     <button onClick={() => submit('Messages during finals week.')}>

&nbsp;       Generate notifications

&nbsp;     </button>



&nbsp;     {object?.notifications?.map((notification, index) => (

&nbsp;       <div key={index}>

&nbsp;         <p>{notification?.name}</p>

&nbsp;         <p>{notification?.message}</p>

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </>

&nbsp; );

}

```

\### Server

On the server, we use \[`streamText`](/docs/reference/ai-sdk-core/stream-text) with \[`Output.object()`](/docs/reference/ai-sdk-core/output#output-object) to stream the object generation process.

```typescript filename='app/api/notifications/route.ts'

import { streamText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { notificationSchema } from './schema';



// Allow streaming responses up to 30 seconds

export const maxDuration = 30;



export async function POST(req: Request) {

&nbsp; const context = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   output: Output.object({ schema: notificationSchema }),

&nbsp;   prompt:

&nbsp;     `Generate 3 notifications for a messages app in this context:` + context,

&nbsp; });



&nbsp; return result.toTextStreamResponse();

}

```

\## Enum Output Mode

When you need to classify or categorize input into predefined options, you can use the `enum` output mode with `useObject`. This requires a specific schema structure where the object has `enum` as a key with `z.enum` containing your possible values.

\### Example: Text Classification

This example shows how to build a simple text classifier that categorizes statements as true or false.

\#### Client

When using `useObject` with enum output mode, your schema must be an object with `enum` as the key:

```tsx filename='app/classify/page.tsx'

'use client';



import { experimental\_useObject as useObject } from '@ai-sdk/react';

import { z } from 'zod';



export default function ClassifyPage() {

&nbsp; const { object, submit, isLoading } = useObject({

&nbsp;   api: '/api/classify',

&nbsp;   schema: z.object({ enum: z.enum(\['true', 'false']) }),

&nbsp; });



&nbsp; return (

&nbsp;   <>

&nbsp;     <button onClick={() => submit('The earth is flat')} disabled={isLoading}>

&nbsp;       Classify statement

&nbsp;     </button>



&nbsp;     {object \&\& <div>Classification: {object.enum}</div>}

&nbsp;   </>

&nbsp; );

}

```

\#### Server

On the server, use `streamText` with `Output.choice()` to stream the classification result:

```typescript filename='app/api/classify/route.ts'

import { streamText, Output } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



export async function POST(req: Request) {

&nbsp; const context = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   output: Output.choice({ options: \['true', 'false'] }),

&nbsp;   prompt: `Classify this statement as true or false: ${context}`,

&nbsp; });



&nbsp; return result.toTextStreamResponse();

}

```

\## Customized UI

`useObject` also provides ways to show loading and error states:

\### Loading State

The `isLoading` state returned by the `useObject` hook can be used for several

purposes:

\- To show a loading spinner while the object is generated.

\- To disable the submit button.

```tsx filename='app/page.tsx' highlight="6,13-20,24"

'use client';



import { useObject } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { isLoading, object, submit } = useObject({

&nbsp;   api: '/api/notifications',

&nbsp;   schema: notificationSchema,

&nbsp; });



&nbsp; return (

&nbsp;   <>

&nbsp;     {isLoading \&\& <Spinner />}



&nbsp;     <button

&nbsp;       onClick={() => submit('Messages during finals week.')}

&nbsp;       disabled={isLoading}

&nbsp;     >

&nbsp;       Generate notifications

&nbsp;     </button>



&nbsp;     {object?.notifications?.map((notification, index) => (

&nbsp;       <div key={index}>

&nbsp;         <p>{notification?.name}</p>

&nbsp;         <p>{notification?.message}</p>

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </>

&nbsp; );

}

```

\### Stop Handler

The `stop` function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.

```tsx filename='app/page.tsx' highlight="6,14-16"

'use client';



import { useObject } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { isLoading, stop, object, submit } = useObject({

&nbsp;   api: '/api/notifications',

&nbsp;   schema: notificationSchema,

&nbsp; });



&nbsp; return (

&nbsp;   <>

&nbsp;     {isLoading \&\& (

&nbsp;       <button type="button" onClick={() => stop()}>

&nbsp;         Stop

&nbsp;       </button>

&nbsp;     )}



&nbsp;     <button onClick={() => submit('Messages during finals week.')}>

&nbsp;       Generate notifications

&nbsp;     </button>



&nbsp;     {object?.notifications?.map((notification, index) => (

&nbsp;       <div key={index}>

&nbsp;         <p>{notification?.name}</p>

&nbsp;         <p>{notification?.message}</p>

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </>

&nbsp; );

}

```

\### Error State

Similarly, the `error` state reflects the error object thrown during the fetch request.

It can be used to display an error message, or to disable the submit button:

<Note>

&nbsp; We recommend showing a generic error message to the user, such as "Something

&nbsp; went wrong." This is a good practice to avoid leaking information from the

&nbsp; server.

</Note>

```tsx file="app/page.tsx" highlight="6,13"

'use client';



import { useObject } from '@ai-sdk/react';



export default function Page() {

&nbsp; const { error, object, submit } = useObject({

&nbsp;   api: '/api/notifications',

&nbsp;   schema: notificationSchema,

&nbsp; });



&nbsp; return (

&nbsp;   <>

&nbsp;     {error \&\& <div>An error occurred.</div>}



&nbsp;     <button onClick={() => submit('Messages during finals week.')}>

&nbsp;       Generate notifications

&nbsp;     </button>



&nbsp;     {object?.notifications?.map((notification, index) => (

&nbsp;       <div key={index}>

&nbsp;         <p>{notification?.name}</p>

&nbsp;         <p>{notification?.message}</p>

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </>

&nbsp; );

}

```

\## Event Callbacks

`useObject` provides optional event callbacks that you can use to handle life-cycle events.

\- `onFinish`: Called when the object generation is completed.

\- `onError`: Called when an error occurs during the fetch request.

These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.

```tsx filename='app/page.tsx' highlight="10-20"

'use client';



import { experimental\_useObject as useObject } from '@ai-sdk/react';

import { notificationSchema } from './api/notifications/schema';



export default function Page() {

&nbsp; const { object, submit } = useObject({

&nbsp;   api: '/api/notifications',

&nbsp;   schema: notificationSchema,

&nbsp;   onFinish({ object, error }) {

&nbsp;     // typed object, undefined if schema validation fails:

&nbsp;     console.log('Object generation completed:', object);



&nbsp;     // error, undefined if schema validation succeeds:

&nbsp;     console.log('Schema validation error:', error);

&nbsp;   },

&nbsp;   onError(error) {

&nbsp;     // error during fetch request:

&nbsp;     console.error('An error occurred:', error);

&nbsp;   },

&nbsp; });



&nbsp; return (

&nbsp;   <div>

&nbsp;     <button onClick={() => submit('Messages during finals week.')}>

&nbsp;       Generate notifications

&nbsp;     </button>



&nbsp;     {object?.notifications?.map((notification, index) => (

&nbsp;       <div key={index}>

&nbsp;         <p>{notification?.name}</p>

&nbsp;         <p>{notification?.message}</p>

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </div>

&nbsp; );

}

```

\## Configure Request Options

You can configure the API endpoint, optional headers and credentials using the `api`, `headers` and `credentials` settings.

```tsx highlight="2-5"

const { submit, object } = useObject({

&nbsp; api: '/api/use-object',

&nbsp; headers: {

&nbsp;   'X-Custom-Header': 'CustomValue',

&nbsp; },

&nbsp; credentials: 'include',

&nbsp; schema: yourSchema,

});

```

---

title: Streaming Custom Data

description: Learn how to stream custom data from the server to the client.

---

\# Streaming Custom Data

It is often useful to send additional data alongside the model's response.

For example, you may want to send status information, the message ids after storing them,

or references to content that the language model is referring to.

The AI SDK provides several helpers that allows you to stream additional data to the client

and attach it to the `UIMessage` parts array:

\- `createUIMessageStream`: creates a data stream

\- `createUIMessageStreamResponse`: creates a response object that streams data

\- `pipeUIMessageStreamToResponse`: pipes a data stream to a server response object

The data is streamed as part of the response stream using Server-Sent Events.

\## Setting Up Type-Safe Data Streaming

First, define your custom message type with data part schemas for type safety:

```tsx filename="ai/types.ts"

import { UIMessage } from 'ai';



// Define your custom message type with data part schemas

export type MyUIMessage = UIMessage<

&nbsp; never, // metadata type

&nbsp; {

&nbsp;   weather: {

&nbsp;     city: string;

&nbsp;     weather?: string;

&nbsp;     status: 'loading' | 'success';

&nbsp;   };

&nbsp;   notification: {

&nbsp;     message: string;

&nbsp;     level: 'info' | 'warning' | 'error';

&nbsp;   };

&nbsp; } // data parts type

>;

```

\## Streaming Data from the Server

In your server-side route handler, you can create a `UIMessageStream` and then pass it to `createUIMessageStreamResponse`:

```tsx filename="route.ts"

import { openai } from '@ai-sdk/openai';

import {

&nbsp; createUIMessageStream,

&nbsp; createUIMessageStreamResponse,

&nbsp; streamText,

&nbsp; convertToModelMessages,

} from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import type { MyUIMessage } from '@/ai/types';



export async function POST(req: Request) {

&nbsp; const { messages } = await req.json();



&nbsp; const stream = createUIMessageStream<MyUIMessage>({

&nbsp;   execute: ({ writer }) => {

&nbsp;     // 1. Send initial status (transient - won't be added to message history)

&nbsp;     writer.write({

&nbsp;       type: 'data-notification',

&nbsp;       data: { message: 'Processing your request...', level: 'info' },

&nbsp;       transient: true, // This part won't be added to message history

&nbsp;     });



&nbsp;     // 2. Send sources (useful for RAG use cases)

&nbsp;     writer.write({

&nbsp;       type: 'source',

&nbsp;       value: {

&nbsp;         type: 'source',

&nbsp;         sourceType: 'url',

&nbsp;         id: 'source-1',

&nbsp;         url: 'https://weather.com',

&nbsp;         title: 'Weather Data Source',

&nbsp;       },

&nbsp;     });



&nbsp;     // 3. Send data parts with loading state

&nbsp;     writer.write({

&nbsp;       type: 'data-weather',

&nbsp;       id: 'weather-1',

&nbsp;       data: { city: 'San Francisco', status: 'loading' },

&nbsp;     });



&nbsp;     const result = streamText({

&nbsp;       model: \_\_MODEL\_\_,

&nbsp;       messages: await convertToModelMessages(messages),

&nbsp;       onFinish() {

&nbsp;         // 4. Update the same data part (reconciliation)

&nbsp;         writer.write({

&nbsp;           type: 'data-weather',

&nbsp;           id: 'weather-1', // Same ID = update existing part

&nbsp;           data: {

&nbsp;             city: 'San Francisco',

&nbsp;             weather: 'sunny',

&nbsp;             status: 'success',

&nbsp;           },

&nbsp;         });



&nbsp;         // 5. Send completion notification (transient)

&nbsp;         writer.write({

&nbsp;           type: 'data-notification',

&nbsp;           data: { message: 'Request completed', level: 'info' },

&nbsp;           transient: true, // Won't be added to message history

&nbsp;         });

&nbsp;       },

&nbsp;     });



&nbsp;     writer.merge(result.toUIMessageStream());

&nbsp;   },

&nbsp; });



&nbsp; return createUIMessageStreamResponse({ stream });

}

```

<Note>

&nbsp; You can also send stream data from custom backends, e.g. Python / FastAPI,

&nbsp; using the \[UI Message Stream

&nbsp; Protocol](/docs/ai-sdk-ui/stream-protocol#ui-message-stream-protocol).

</Note>

\## Types of Streamable Data

\### Data Parts (Persistent)

Regular data parts are added to the message history and appear in `message.parts`:

```tsx

writer.write({

&nbsp; type: 'data-weather',

&nbsp; id: 'weather-1', // Optional: enables reconciliation

&nbsp; data: { city: 'San Francisco', status: 'loading' },

});

```

\### Sources

Sources are useful for RAG implementations where you want to show which documents or URLs were referenced:

```tsx

writer.write({

&nbsp; type: 'source',

&nbsp; value: {

&nbsp;   type: 'source',

&nbsp;   sourceType: 'url',

&nbsp;   id: 'source-1',

&nbsp;   url: 'https://example.com',

&nbsp;   title: 'Example Source',

&nbsp; },

});

```

\### Transient Data Parts (Ephemeral)

Transient parts are sent to the client but not added to the message history. They are only accessible via the `onData` useChat handler:

```tsx

// server

writer.write({

&nbsp; type: 'data-notification',

&nbsp; data: { message: 'Processing...', level: 'info' },

&nbsp; transient: true, // Won't be added to message history

});



// client

const \[notification, setNotification] = useState();



const { messages } = useChat({

&nbsp; onData: ({ data, type }) => {

&nbsp;   if (type === 'data-notification') {

&nbsp;     setNotification({ message: data.message, level: data.level });

&nbsp;   }

&nbsp; },

});

```

\## Data Part Reconciliation

When you write to a data part with the same ID, the client automatically reconciles and updates that part. This enables powerful dynamic experiences like:

\- \*\*Collaborative artifacts\*\* - Update code, documents, or designs in real-time

\- \*\*Progressive data loading\*\* - Show loading states that transform into final results

\- \*\*Live status updates\*\* - Update progress bars, counters, or status indicators

\- \*\*Interactive components\*\* - Build UI elements that evolve based on user interaction

The reconciliation happens automatically - simply use the same `id` when writing to the stream.

\## Processing Data on the Client

\### Using the onData Callback

The `onData` callback is essential for handling streaming data, especially transient parts:

```tsx filename="page.tsx"

import { useChat } from '@ai-sdk/react';

import type { MyUIMessage } from '@/ai/types';



const { messages } = useChat<MyUIMessage>({

&nbsp; api: '/api/chat',

&nbsp; onData: dataPart => {

&nbsp;   // Handle all data parts as they arrive (including transient parts)

&nbsp;   console.log('Received data part:', dataPart);



&nbsp;   // Handle different data part types

&nbsp;   if (dataPart.type === 'data-weather') {

&nbsp;     console.log('Weather update:', dataPart.data);

&nbsp;   }



&nbsp;   // Handle transient notifications (ONLY available here, not in message.parts)

&nbsp;   if (dataPart.type === 'data-notification') {

&nbsp;     showToast(dataPart.data.message, dataPart.data.level);

&nbsp;   }

&nbsp; },

});

```

\*\*Important:\*\* Transient data parts are \*\*only\*\* available through the `onData` callback. They will not appear in the `message.parts` array since they're not added to message history.

\### Rendering Persistent Data Parts

You can filter and render data parts from the message parts array:

```tsx filename="page.tsx"

const result = (

&nbsp; <>

&nbsp;   {messages?.map(message => (

&nbsp;     <div key={message.id}>

&nbsp;       {/\* Render weather data parts \*/}

&nbsp;       {message.parts

&nbsp;         .filter(part => part.type === 'data-weather')

&nbsp;         .map((part, index) => (

&nbsp;           <div key={index} className="weather-widget">

&nbsp;             {part.data.status === 'loading' ? (

&nbsp;               <>Getting weather for {part.data.city}...</>

&nbsp;             ) : (

&nbsp;               <>

&nbsp;                 Weather in {part.data.city}: {part.data.weather}

&nbsp;               </>

&nbsp;             )}

&nbsp;           </div>

&nbsp;         ))}



&nbsp;       {/\* Render text content \*/}

&nbsp;       {message.parts

&nbsp;         .filter(part => part.type === 'text')

&nbsp;         .map((part, index) => (

&nbsp;           <div key={index}>{part.text}</div>

&nbsp;         ))}



&nbsp;       {/\* Render sources \*/}

&nbsp;       {message.parts

&nbsp;         .filter(part => part.type === 'source')

&nbsp;         .map((part, index) => (

&nbsp;           <div key={index} className="source">

&nbsp;             Source: <a href={part.url}>{part.title}</a>

&nbsp;           </div>

&nbsp;         ))}

&nbsp;     </div>

&nbsp;   ))}

&nbsp; </>

);

```

\### Complete Example

```tsx filename="page.tsx"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';

import type { MyUIMessage } from '@/ai/types';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');



&nbsp; const { messages, sendMessage } = useChat<MyUIMessage>({

&nbsp;   api: '/api/chat',

&nbsp;   onData: dataPart => {

&nbsp;     // Handle transient notifications

&nbsp;     if (dataPart.type === 'data-notification') {

&nbsp;       console.log('Notification:', dataPart.data.message);

&nbsp;     }

&nbsp;   },

&nbsp; });



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   sendMessage({ text: input });

&nbsp;   setInput('');

&nbsp; };



&nbsp; return (

&nbsp;   <>

&nbsp;     {messages?.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         {message.role === 'user' ? 'User: ' : 'AI: '}



&nbsp;         {/\* Render weather data \*/}

&nbsp;         {message.parts

&nbsp;           .filter(part => part.type === 'data-weather')

&nbsp;           .map((part, index) => (

&nbsp;             <span key={index} className="weather-update">

&nbsp;               {part.data.status === 'loading' ? (

&nbsp;                 <>Getting weather for {part.data.city}...</>

&nbsp;               ) : (

&nbsp;                 <>

&nbsp;                   Weather in {part.data.city}: {part.data.weather}

&nbsp;                 </>

&nbsp;               )}

&nbsp;             </span>

&nbsp;           ))}



&nbsp;         {/\* Render text content \*/}

&nbsp;         {message.parts

&nbsp;           .filter(part => part.type === 'text')

&nbsp;           .map((part, index) => (

&nbsp;             <div key={index}>{part.text}</div>

&nbsp;           ))}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         placeholder="Ask about the weather..."

&nbsp;       />

&nbsp;       <button type="submit">Send</button>

&nbsp;     </form>

&nbsp;   </>

&nbsp; );

}

```

\## Use Cases

\- \*\*RAG Applications\*\* - Stream sources and retrieved documents

\- \*\*Real-time Status\*\* - Show loading states and progress updates

\- \*\*Collaborative Tools\*\* - Stream live updates to shared artifacts

\- \*\*Analytics\*\* - Send usage data without cluttering message history

\- \*\*Notifications\*\* - Display temporary alerts and status messages

\## Message Metadata vs Data Parts

Both \[message metadata](/docs/ai-sdk-ui/message-metadata) and data parts allow you to send additional information alongside messages, but they serve different purposes:

\### Message Metadata

Message metadata is best for \*\*message-level information\*\* that describes the message as a whole:

\- Attached at the message level via `message.metadata`

\- Sent using the `messageMetadata` callback in `toUIMessageStreamResponse`

\- Ideal for: timestamps, model info, token usage, user context

\- Type-safe with custom metadata types

```ts

// Server: Send metadata about the message

return result.toUIMessageStreamResponse({

&nbsp; messageMetadata: ({ part }) => {

&nbsp;   if (part.type === 'finish') {

&nbsp;     return {

&nbsp;       model: part.response.modelId,

&nbsp;       totalTokens: part.totalUsage.totalTokens,

&nbsp;       createdAt: Date.now(),

&nbsp;     };

&nbsp;   }

&nbsp; },

});

```

\### Data Parts

Data parts are best for streaming \*\*dynamic arbitrary data\*\*:

\- Added to the message parts array via `message.parts`

\- Streamed using `createUIMessageStream` and `writer.write()`

\- Can be reconciled/updated using the same ID

\- Support transient parts that don't persist

\- Ideal for: dynamic content, loading states, interactive components

```ts

// Server: Stream data as part of message content

writer.write({

&nbsp; type: 'data-weather',

&nbsp; id: 'weather-1',

&nbsp; data: { city: 'San Francisco', status: 'loading' },

});

```

For more details on message metadata, see the \[Message Metadata documentation](/docs/ai-sdk-ui/message-metadata).

---

title: Error Handling

description: Learn how to handle errors in the AI SDK UI

---

\# Error Handling and warnings

\## Warnings

The AI SDK shows warnings when something might not work as expected.

These warnings help you fix problems before they cause errors.

\### When Warnings Appear

Warnings are shown in the browser console when:

\- \*\*Unsupported features\*\*: You use a feature or setting that is not supported by the AI model (e.g., certain options or parameters).

\- \*\*Compatibility warnings\*\*: A feature is used in a compatibility mode, which might work differently or less optimally than intended.

\- \*\*Other warnings\*\*: The AI model reports another type of issue, such as general problems or advisory messages.

\### Warning Messages

All warnings start with "AI SDK Warning:" so you can easily find them. For example:

```

AI SDK Warning: The feature "temperature" is not supported by this model

```

\### Turning Off Warnings

By default, warnings are shown in the console. You can control this behavior:

\#### Turn Off All Warnings

Set a global variable to turn off warnings completely:

```ts

globalThis.AI\_SDK\_LOG\_WARNINGS = false;

```

\#### Custom Warning Handler

You can also provide your own function to handle warnings.

It receives provider id, model id, and a list of warnings.

```ts

globalThis.AI\_SDK\_LOG\_WARNINGS = ({ warnings, provider, model }) => {

&nbsp; // Handle warnings your own way

};

```

\## Error Handling

\### Error Helper Object

Each AI SDK UI hook also returns an \[error](/docs/reference/ai-sdk-ui/use-chat#error) object that you can use to render the error in your UI.

You can use the error object to show an error message, disable the submit button, or show a retry button.

<Note>

&nbsp; We recommend showing a generic error message to the user, such as "Something

&nbsp; went wrong." This is a good practice to avoid leaking information from the

&nbsp; server.

</Note>

```tsx file="app/page.tsx" highlight="7,18-25,31"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { messages, sendMessage, error, regenerate } = useChat();



&nbsp; const handleSubmit = (e: React.FormEvent) => {

&nbsp;   e.preventDefault();

&nbsp;   sendMessage({ text: input });

&nbsp;   setInput('');

&nbsp; };



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id}>

&nbsp;         {m.role}:{' '}

&nbsp;         {m.parts

&nbsp;           .filter(part => part.type === 'text')

&nbsp;           .map(part => part.text)

&nbsp;           .join('')}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     {error \&\& (

&nbsp;       <>

&nbsp;         <div>An error occurred.</div>

&nbsp;         <button type="button" onClick={() => regenerate()}>

&nbsp;           Retry

&nbsp;         </button>

&nbsp;       </>

&nbsp;     )}



&nbsp;     <form onSubmit={handleSubmit}>

&nbsp;       <input

&nbsp;         value={input}

&nbsp;         onChange={e => setInput(e.target.value)}

&nbsp;         disabled={error != null}

&nbsp;       />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

\#### Alternative: replace last message

Alternatively you can write a custom submit handler that replaces the last message when an error is present.

```tsx file="app/page.tsx" highlight="17-23,35"

'use client';



import { useChat } from '@ai-sdk/react';

import { useState } from 'react';



export default function Chat() {

&nbsp; const \[input, setInput] = useState('');

&nbsp; const { sendMessage, error, messages, setMessages } = useChat();



&nbsp; function customSubmit(event: React.FormEvent<HTMLFormElement>) {

&nbsp;   event.preventDefault();



&nbsp;   if (error != null) {

&nbsp;     setMessages(messages.slice(0, -1)); // remove last message

&nbsp;   }



&nbsp;   sendMessage({ text: input });

&nbsp;   setInput('');

&nbsp; }



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(m => (

&nbsp;       <div key={m.id}>

&nbsp;         {m.role}:{' '}

&nbsp;         {m.parts

&nbsp;           .filter(part => part.type === 'text')

&nbsp;           .map(part => part.text)

&nbsp;           .join('')}

&nbsp;       </div>

&nbsp;     ))}



&nbsp;     {error \&\& <div>An error occurred.</div>}



&nbsp;     <form onSubmit={customSubmit}>

&nbsp;       <input value={input} onChange={e => setInput(e.target.value)} />

&nbsp;     </form>

&nbsp;   </div>

&nbsp; );

}

```

\### Error Handling Callback

Errors can be processed by passing an \[`onError`](/docs/reference/ai-sdk-ui/use-chat#on-error) callback function as an option to the \[`useChat`](/docs/reference/ai-sdk-ui/use-chat) or \[`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) hooks.

The callback function receives an error object as an argument.

```tsx file="app/page.tsx" highlight="6-9"

import { useChat } from '@ai-sdk/react';



export default function Page() {

&nbsp; const {

&nbsp;   /\* ... \*/

&nbsp; } = useChat({

&nbsp;   // handle error:

&nbsp;   onError: error => {

&nbsp;     console.error(error);

&nbsp;   },

&nbsp; });

}

```

\### Injecting Errors for Testing

You might want to create errors for testing.

You can easily do so by throwing an error in your route handler:

```ts file="app/api/chat/route.ts"

export async function POST(req: Request) {

&nbsp; throw new Error('This is a test error');

}

```

---

title: Transport

description: Learn how to use custom transports with useChat.

---

\# Transport

The `useChat` transport system provides fine-grained control over how messages are sent to your API endpoints and how responses are processed. This is particularly useful for alternative communication protocols like WebSockets, custom authentication patterns, or specialized backend integrations.

\## Default Transport

By default, `useChat` uses HTTP POST requests to send messages to `/api/chat`:

```tsx
import { useChat } from "@ai-sdk/react";

// Uses default HTTP transport

const { messages, sendMessage } = useChat();
```

This is equivalent to:

```tsx

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



const { messages, sendMessage } = useChat({

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/chat',

&nbsp; }),

});

```

\## Custom Transport Configuration

Configure the default transport with custom options:

```tsx

import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';



const { messages, sendMessage } = useChat({

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/custom-chat',

&nbsp;   headers: {

&nbsp;     Authorization: 'Bearer your-token',

&nbsp;     'X-API-Version': '2024-01',

&nbsp;   },

&nbsp;   credentials: 'include',

&nbsp; }),

});

```

\### Dynamic Configuration

You can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:

```tsx

const { messages, sendMessage } = useChat({

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/chat',

&nbsp;   headers: () => ({

&nbsp;     Authorization: `Bearer ${getAuthToken()}`,

&nbsp;     'X-User-ID': getCurrentUserId(),

&nbsp;   }),

&nbsp;   body: () => ({

&nbsp;     sessionId: getCurrentSessionId(),

&nbsp;     preferences: getUserPreferences(),

&nbsp;   }),

&nbsp;   credentials: () => 'include',

&nbsp; }),

});

```

\### Request Transformation

Transform requests before sending to your API:

```tsx

const { messages, sendMessage } = useChat({

&nbsp; transport: new DefaultChatTransport({

&nbsp;   api: '/api/chat',

&nbsp;   prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {

&nbsp;     return {

&nbsp;       headers: {

&nbsp;         'X-Session-ID': id,

&nbsp;       },

&nbsp;       body: {

&nbsp;         messages: messages.slice(-10), // Only send last 10 messages

&nbsp;         trigger,

&nbsp;         messageId,

&nbsp;       },

&nbsp;     };

&nbsp;   },

&nbsp; }),

});

```

\## Direct Agent Transport

For scenarios where you want to communicate directly with an \[Agent](/docs/reference/ai-sdk-core/agent) without going through HTTP, you can use `DirectChatTransport`. This transport invokes the agent's `stream()` method directly in-process.

This is useful for:

\- \*\*Server-side rendering\*\*: Run the agent on the server without an API endpoint

\- \*\*Testing\*\*: Test chat functionality without network requests

\- \*\*Single-process applications\*\*: Desktop or CLI apps where client and agent run together

```tsx

import { useChat } from '@ai-sdk/react';

import { DirectChatTransport, ToolLoopAgent } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



const agent = new ToolLoopAgent({

&nbsp; model: \_\_MODEL\_\_,

&nbsp; instructions: 'You are a helpful assistant.',

&nbsp; tools: {

&nbsp;   weather: weatherTool,

&nbsp; },

});



const { messages, sendMessage } = useChat({

&nbsp; transport: new DirectChatTransport({ agent }),

});

```

\### How It Works

Unlike `DefaultChatTransport` which sends HTTP requests:

1\. `DirectChatTransport` validates incoming UI messages

2\. Converts them to model messages using `convertToModelMessages`

3\. Calls the agent's `stream()` method directly

4\. Returns the result as a UI message stream via `toUIMessageStream()`

\### Configuration Options

You can pass additional options to customize the stream output:

```tsx

const transport = new DirectChatTransport({

&nbsp; agent,

&nbsp; // Pass options to the agent

&nbsp; options: { customOption: 'value' },

&nbsp; // Configure what's sent to the client

&nbsp; sendReasoning: true,

&nbsp; sendSources: true,

});

```

<Note>

&nbsp; `DirectChatTransport` does not support stream reconnection since there is no

&nbsp; persistent server-side stream. The `reconnectToStream()` method always returns

&nbsp; `null`.

</Note>

For complete API details, see the \[DirectChatTransport reference](/docs/reference/ai-sdk-ui/direct-chat-transport).

\## Building Custom Transports

To understand how to build your own transport, refer to the source code of the default implementation:

\- \*\*\[DefaultChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/default-chat-transport.ts)\*\* - The complete default HTTP transport implementation

\- \*\*\[HttpChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/http-chat-transport.ts)\*\* - Base HTTP transport with request handling

\- \*\*\[ChatTransport Interface](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/chat-transport.ts)\*\* - The transport interface you need to implement

These implementations show you exactly how to:

\- Handle the `sendMessages` method

\- Process UI message streams

\- Transform requests and responses

\- Handle errors and connection management

The transport system gives you complete control over how your chat application communicates, enabling integration with any backend protocol or service.

---

title: Reading UIMessage Streams

description: Learn how to read UIMessage streams.

---

\# Reading UI Message Streams

`UIMessage` streams are useful outside of traditional chat use cases. You can consume them for terminal UIs, custom stream processing on the client, or React Server Components (RSC).

The `readUIMessageStream` helper transforms a stream of `UIMessageChunk` objects into an `AsyncIterableStream` of `UIMessage` objects, allowing you to process messages as they're being constructed.

\## Basic Usage

```tsx

import { readUIMessageStream, streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



async function main() {

&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   prompt: 'Write a short story about a robot.',

&nbsp; });



&nbsp; for await (const uiMessage of readUIMessageStream({

&nbsp;   stream: result.toUIMessageStream(),

&nbsp; })) {

&nbsp;   console.log('Current message state:', uiMessage);

&nbsp; }

}

```

\## Tool Calls Integration

Handle streaming responses that include tool calls:

```tsx

import { readUIMessageStream, streamText, tool } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import { z } from 'zod';



async function handleToolCalls() {

&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   tools: {

&nbsp;     weather: tool({

&nbsp;       description: 'Get the weather in a location',

&nbsp;       inputSchema: z.object({

&nbsp;         location: z.string().describe('The location to get the weather for'),

&nbsp;       }),

&nbsp;       execute: ({ location }) => ({

&nbsp;         location,

&nbsp;         temperature: 72 + Math.floor(Math.random() \* 21) - 10,

&nbsp;       }),

&nbsp;     }),

&nbsp;   },

&nbsp;   prompt: 'What is the weather in Tokyo?',

&nbsp; });



&nbsp; for await (const uiMessage of readUIMessageStream({

&nbsp;   stream: result.toUIMessageStream(),

&nbsp; })) {

&nbsp;   // Handle different part types

&nbsp;   uiMessage.parts.forEach(part => {

&nbsp;     switch (part.type) {

&nbsp;       case 'text':

&nbsp;         console.log('Text:', part.text);

&nbsp;         break;

&nbsp;       case 'tool-call':

&nbsp;         console.log('Tool called:', part.toolName, 'with args:', part.args);

&nbsp;         break;

&nbsp;       case 'tool-result':

&nbsp;         console.log('Tool result:', part.result);

&nbsp;         break;

&nbsp;     }

&nbsp;   });

&nbsp; }

}

```

\## Resuming Conversations

Resume streaming from a previous message state:

```tsx

import { readUIMessageStream, streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;



async function resumeConversation(lastMessage: UIMessage) {

&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: \[

&nbsp;     { role: 'user', content: 'Continue our previous conversation.' },

&nbsp;   ],

&nbsp; });



&nbsp; // Resume from the last message

&nbsp; for await (const uiMessage of readUIMessageStream({

&nbsp;   stream: result.toUIMessageStream(),

&nbsp;   message: lastMessage, // Resume from this message

&nbsp; })) {

&nbsp;   console.log('Resumed message:', uiMessage);

&nbsp; }

}

```

---

title: Message Metadata

description: Learn how to attach and use metadata with messages in AI SDK UI

---

\# Message Metadata

Message metadata allows you to attach custom information to messages at the message level. This is useful for tracking timestamps, model information, token usage, user context, and other message-level data.

\## Overview

Message metadata differs from \[data parts](/docs/ai-sdk-ui/streaming-data) in that it's attached at the message level rather than being part of the message content. While data parts are ideal for dynamic content that forms part of the message, metadata is perfect for information about the message itself.

\## Getting Started

Here's a simple example of using message metadata to track timestamps and model information:

\### Defining Metadata Types

First, define your metadata type for type safety:

```tsx filename="app/types.ts"

import { UIMessage } from 'ai';

import { z } from 'zod';



// Define your metadata schema

export const messageMetadataSchema = z.object({

&nbsp; createdAt: z.number().optional(),

&nbsp; model: z.string().optional(),

&nbsp; totalTokens: z.number().optional(),

});



export type MessageMetadata = z.infer<typeof messageMetadataSchema>;



// Create a typed UIMessage

export type MyUIMessage = UIMessage<MessageMetadata>;

```

\### Sending Metadata from the Server

Use the `messageMetadata` callback in `toUIMessageStreamResponse` to send metadata at different streaming stages:

```ts filename="app/api/chat/route.ts" highlight="11-20"

import { convertToModelMessages, streamText } from 'ai';

\_\_PROVIDER\_IMPORT\_\_;

import type { MyUIMessage } from '@/types';



export async function POST(req: Request) {

&nbsp; const { messages }: { messages: MyUIMessage\[] } = await req.json();



&nbsp; const result = streamText({

&nbsp;   model: \_\_MODEL\_\_,

&nbsp;   messages: await convertToModelMessages(messages),

&nbsp; });



&nbsp; return result.toUIMessageStreamResponse({

&nbsp;   originalMessages: messages, // pass this in for type-safe return objects

&nbsp;   messageMetadata: ({ part }) => {

&nbsp;     // Send metadata when streaming starts

&nbsp;     if (part.type === 'start') {

&nbsp;       return {

&nbsp;         createdAt: Date.now(),

&nbsp;         model: 'your-model-id',

&nbsp;       };

&nbsp;     }



&nbsp;     // Send additional metadata when streaming completes

&nbsp;     if (part.type === 'finish') {

&nbsp;       return {

&nbsp;         totalTokens: part.totalUsage.totalTokens,

&nbsp;       };

&nbsp;     }

&nbsp;   },

&nbsp; });

}

```

<Note>

&nbsp; To enable type-safe metadata return object in `messageMetadata`, pass in the

&nbsp; `originalMessages` parameter typed to your UIMessage type.

</Note>

\### Accessing Metadata on the Client

Access metadata through the `message.metadata` property:

```tsx filename="app/page.tsx" highlight="8,18-23"

'use client';



import { useChat } from '@ai-sdk/react';

import { DefaultChatTransport } from 'ai';

import type { MyUIMessage } from '@/types';



export default function Chat() {

&nbsp; const { messages } = useChat<MyUIMessage>({

&nbsp;   transport: new DefaultChatTransport({

&nbsp;     api: '/api/chat',

&nbsp;   }),

&nbsp; });



&nbsp; return (

&nbsp;   <div>

&nbsp;     {messages.map(message => (

&nbsp;       <div key={message.id}>

&nbsp;         <div>

&nbsp;           {message.role === 'user' ? 'User: ' : 'AI: '}

&nbsp;           {message.metadata?.createdAt \&\& (

&nbsp;             <span className="text-sm text-gray-500">

&nbsp;               {new Date(message.metadata.createdAt).toLocaleTimeString()}

&nbsp;             </span>

&nbsp;           )}

&nbsp;         </div>



&nbsp;         {/\* Render message content \*/}

&nbsp;         {message.parts.map((part, index) =>

&nbsp;           part.type === 'text' ? <div key={index}>{part.text}</div> : null,

&nbsp;         )}



&nbsp;         {/\* Display additional metadata \*/}

&nbsp;         {message.metadata?.totalTokens \&\& (

&nbsp;           <div className="text-xs text-gray-400">

&nbsp;             {message.metadata.totalTokens} tokens

&nbsp;           </div>

&nbsp;         )}

&nbsp;       </div>

&nbsp;     ))}

&nbsp;   </div>

&nbsp; );

}

```

<Note>

&nbsp; For streaming arbitrary data that changes during generation, consider using

&nbsp; \[data parts](/docs/ai-sdk-ui/streaming-data) instead.

</Note>

\## Common Use Cases

Message metadata is ideal for:

\- \*\*Timestamps\*\*: When messages were created or completed

\- \*\*Model Information\*\*: Which AI model was used

\- \*\*Token Usage\*\*: Track costs and usage limits

\- \*\*User Context\*\*: User IDs, session information

\- \*\*Performance Metrics\*\*: Generation time, time to first token

\- \*\*Quality Indicators\*\*: Finish reason, confidence scores

\## See Also

\- \[Chatbot Guide](/docs/ai-sdk-ui/chatbot#message-metadata) - Message metadata in the context of building chatbots

\- \[Streaming Data](/docs/ai-sdk-ui/streaming-data#message-metadata-vs-data-parts) - Comparison with data parts

\- \[UIMessage Reference](/docs/reference/ai-sdk-core/ui-message) - Complete UIMessage type reference

---

title: AI_APICallError

description: Learn how to fix AI_APICallError

---

\# AI_APICallError

This error occurs when an API call fails.

\## Properties

\- `url`: The URL of the API request that failed

\- `requestBodyValues`: The request body values sent to the API

\- `statusCode`: The HTTP status code returned by the API

\- `responseHeaders`: The response headers returned by the API

\- `responseBody`: The response body returned by the API

\- `isRetryable`: Whether the request can be retried based on the status code

\- `data`: Any additional data associated with the error

\## Checking for this Error

You can check if an error is an instance of `AI\_APICallError` using:

```typescript

import { APICallError } from 'ai';



if (APICallError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_DownloadError

description: Learn how to fix AI_DownloadError

---

\# AI_DownloadError

This error occurs when a download fails.

\## Properties

\- `url`: The URL that failed to download

\- `statusCode`: The HTTP status code returned by the server

\- `statusText`: The HTTP status text returned by the server

\- `message`: The error message containing details about the download failure

\## Checking for this Error

You can check if an error is an instance of `AI\_DownloadError` using:

```typescript

import { DownloadError } from 'ai';



if (DownloadError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_EmptyResponseBodyError

description: Learn how to fix AI_EmptyResponseBodyError

---

\# AI_EmptyResponseBodyError

This error occurs when the server returns an empty response body.

\## Properties

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_EmptyResponseBodyError` using:

```typescript

import { EmptyResponseBodyError } from 'ai';



if (EmptyResponseBodyError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidArgumentError

description: Learn how to fix AI_InvalidArgumentError

---

\# AI_InvalidArgumentError

This error occurs when an invalid argument was provided.

\## Properties

\- `parameter`: The name of the parameter that is invalid

\- `value`: The invalid value

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidArgumentError` using:

```typescript

import { InvalidArgumentError } from 'ai';



if (InvalidArgumentError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidDataContentError

description: How to fix AI_InvalidDataContentError

---

\# AI_InvalidDataContentError

This error occurs when the data content provided in a multi-modal message part is invalid. Check out the \[ prompt examples for multi-modal messages ](/docs/foundations/prompts#message-prompts).

\## Properties

\- `content`: The invalid content value

\- `message`: The error message describing the expected and received content types

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidDataContentError` using:

```typescript

import { InvalidDataContentError } from 'ai';



if (InvalidDataContentError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidDataContent

description: Learn how to fix AI_InvalidDataContent

---

\# AI_InvalidDataContent

This error occurs when invalid data content is provided.

\## Properties

\- `content`: The invalid content value

\- `message`: The error message

\- `cause`: The cause of the error

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidDataContent` using:

```typescript

import { InvalidDataContent } from 'ai';



if (InvalidDataContent.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidMessageRoleError

description: Learn how to fix AI_InvalidMessageRoleError

---

\# AI_InvalidMessageRoleError

This error occurs when an invalid message role is provided.

\## Properties

\- `role`: The invalid role value

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidMessageRoleError` using:

```typescript

import { InvalidMessageRoleError } from 'ai';



if (InvalidMessageRoleError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidPromptError

description: Learn how to fix AI_InvalidPromptError

---

\# AI_InvalidPromptError

This error occurs when the prompt provided is invalid.

\## Potential Causes

\### UI Messages

You are passing a `UIMessage\[]` as messages into e.g. `streamText`.

You need to first convert them to a `ModelMessage\[]` using `convertToModelMessages()`.

```typescript

import { type UIMessage, generateText, convertToModelMessages } from 'ai';



const messages: UIMessage\[] = \[

&nbsp; /\* ... \*/

];



const result = await generateText({

&nbsp; // ...

&nbsp; messages: await convertToModelMessages(messages),

});

```

\## Properties

\- `prompt`: The invalid prompt value

\- `message`: The error message

\- `cause`: The cause of the error

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidPromptError` using:

```typescript

import { InvalidPromptError } from 'ai';



if (InvalidPromptError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidResponseDataError

description: Learn how to fix AI_InvalidResponseDataError

---

\# AI_InvalidResponseDataError

This error occurs when the server returns a response with invalid data content.

\## Properties

\- `data`: The invalid response data value

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidResponseDataError` using:

```typescript

import { InvalidResponseDataError } from 'ai';



if (InvalidResponseDataError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidToolApprovalError

description: Learn how to fix AI_InvalidToolApprovalError

---

\# AI_InvalidToolApprovalError

This error occurs when a tool approval response references an unknown `approvalId`. No matching `tool-approval-request` was found in the message history.

\## Properties

\- `approvalId`: The approval ID that was not found

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidToolApprovalError` using:

```typescript

import { InvalidToolApprovalError } from 'ai';



if (InvalidToolApprovalError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_InvalidToolInputError

description: Learn how to fix AI_InvalidToolInputError

---

\# AI_InvalidToolInputError

This error occurs when invalid tool input was provided.

\## Properties

\- `toolName`: The name of the tool with invalid inputs

\- `toolInput`: The invalid tool inputs

\- `message`: The error message

\- `cause`: The cause of the error

\## Checking for this Error

You can check if an error is an instance of `AI\_InvalidToolInputError` using:

```typescript

import { InvalidToolInputError } from 'ai';



if (InvalidToolInputError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_JSONParseError

description: Learn how to fix AI_JSONParseError

---

\# AI_JSONParseError

This error occurs when JSON fails to parse.

\## Properties

\- `text`: The text value that could not be parsed

\- `message`: The error message including parse error details

\## Checking for this Error

You can check if an error is an instance of `AI\_JSONParseError` using:

```typescript

import { JSONParseError } from 'ai';



if (JSONParseError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_LoadAPIKeyError

description: Learn how to fix AI_LoadAPIKeyError

---

\# AI_LoadAPIKeyError

This error occurs when API key is not loaded successfully.

\## Properties

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_LoadAPIKeyError` using:

```typescript

import { LoadAPIKeyError } from 'ai';



if (LoadAPIKeyError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_LoadSettingError

description: Learn how to fix AI_LoadSettingError

---

\# AI_LoadSettingError

This error occurs when a setting is not loaded successfully.

\## Properties

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_LoadSettingError` using:

```typescript

import { LoadSettingError } from 'ai';



if (LoadSettingError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_MessageConversionError

description: Learn how to fix AI_MessageConversionError

---

\# AI_MessageConversionError

This error occurs when message conversion fails.

\## Properties

\- `originalMessage`: The original message that failed conversion

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_MessageConversionError` using:

```typescript

import { MessageConversionError } from 'ai';



if (MessageConversionError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_NoContentGeneratedError

description: Learn how to fix AI_NoContentGeneratedError

---

\# AI_NoContentGeneratedError

This error occurs when the AI provider fails to generate content.

\## Properties

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_NoContentGeneratedError` using:

```typescript

import { NoContentGeneratedError } from 'ai';



if (NoContentGeneratedError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_NoImageGeneratedError

description: Learn how to fix AI_NoImageGeneratedError

---

\# AI_NoImageGeneratedError

This error occurs when the AI provider fails to generate an image.

It can arise due to the following reasons:

\- The model failed to generate a response.

\- The model generated an invalid response.

\## Properties

\- `message`: The error message.

\- `responses`: Metadata about the image model responses, including timestamp, model, and headers.

\- `cause`: The cause of the error. You can use this for more detailed error handling.

\## Checking for this Error

You can check if an error is an instance of `AI\_NoImageGeneratedError` using:

```typescript

import { generateImage, NoImageGeneratedError } from 'ai';



try {

&nbsp; await generateImage({ model, prompt });

} catch (error) {

&nbsp; if (NoImageGeneratedError.isInstance(error)) {

&nbsp;   console.log('NoImageGeneratedError');

&nbsp;   console.log('Cause:', error.cause);

&nbsp;   console.log('Responses:', error.responses);

&nbsp; }

}

```

---

title: AI_NoObjectGeneratedError

description: Learn how to fix AI_NoObjectGeneratedError

---

\# AI_NoObjectGeneratedError

This error occurs when the AI provider fails to generate a parsable object that conforms to the schema.

It can arise due to the following reasons:

\- The model failed to generate a response.

\- The model generated a response that could not be parsed.

\- The model generated a response that could not be validated against the schema.

\## Properties

\- `message`: The error message.

\- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.

\- `response`: Metadata about the language model response, including response id, timestamp, and model.

\- `usage`: Request token usage.

\- `finishReason`: Request finish reason. For example 'length' if model generated maximum number of tokens, this could result in a JSON parsing error.

\- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.

\## Checking for this Error

You can check if an error is an instance of `AI\_NoObjectGeneratedError` using:

```typescript

import { generateObject, NoObjectGeneratedError } from 'ai';



try {

&nbsp; await generateObject({ model, schema, prompt });

} catch (error) {

&nbsp; if (NoObjectGeneratedError.isInstance(error)) {

&nbsp;   console.log('NoObjectGeneratedError');

&nbsp;   console.log('Cause:', error.cause);

&nbsp;   console.log('Text:', error.text);

&nbsp;   console.log('Response:', error.response);

&nbsp;   console.log('Usage:', error.usage);

&nbsp;   console.log('Finish Reason:', error.finishReason);

&nbsp; }

}

```

---

title: AI_NoSpeechGeneratedError

description: Learn how to fix AI_NoSpeechGeneratedError

---

\# AI_NoSpeechGeneratedError

This error occurs when no audio could be generated from the input.

\## Properties

\- `responses`: Array of responses

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_NoSpeechGeneratedError` using:

```typescript

import { NoSpeechGeneratedError } from 'ai';



if (NoSpeechGeneratedError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_NoSuchModelError

description: Learn how to fix AI_NoSuchModelError

---

\# AI_NoSuchModelError

This error occurs when a model ID is not found.

\## Properties

\- `modelId`: The ID of the model that was not found

\- `modelType`: The type of model

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_NoSuchModelError` using:

```typescript

import { NoSuchModelError } from 'ai';



if (NoSuchModelError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_NoSuchProviderError

description: Learn how to fix AI_NoSuchProviderError

---

\# AI_NoSuchProviderError

This error occurs when a provider ID is not found.

\## Properties

\- `providerId`: The ID of the provider that was not found

\- `availableProviders`: Array of available provider IDs

\- `modelId`: The ID of the model

\- `modelType`: The type of model

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_NoSuchProviderError` using:

```typescript

import { NoSuchProviderError } from 'ai';



if (NoSuchProviderError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_NoSuchToolError

description: Learn how to fix AI_NoSuchToolError

---

\# AI_NoSuchToolError

This error occurs when a model tries to call an unavailable tool.

\## Properties

\- `toolName`: The name of the tool that was not found

\- `availableTools`: Array of available tool names

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_NoSuchToolError` using:

```typescript

import { NoSuchToolError } from 'ai';



if (NoSuchToolError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_NoTranscriptGeneratedError

description: Learn how to fix AI_NoTranscriptGeneratedError

---

\# AI_NoTranscriptGeneratedError

This error occurs when no transcript could be generated from the input.

\## Properties

\- `responses`: Array of responses

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_NoTranscriptGeneratedError` using:

```typescript

import { NoTranscriptGeneratedError } from 'ai';



if (NoTranscriptGeneratedError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_RetryError

description: Learn how to fix AI_RetryError

---

\# AI_RetryError

This error occurs when a retry operation fails.

\## Properties

\- `reason`: The reason for the retry failure

\- `lastError`: The most recent error that occurred during retries

\- `errors`: Array of all errors that occurred during retry attempts

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_RetryError` using:

```typescript

import { RetryError } from 'ai';



if (RetryError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_TooManyEmbeddingValuesForCallError

description: Learn how to fix AI_TooManyEmbeddingValuesForCallError

---

\# AI_TooManyEmbeddingValuesForCallError

This error occurs when too many values are provided in a single embedding call.

\## Properties

\- `provider`: The AI provider name

\- `modelId`: The ID of the embedding model

\- `maxEmbeddingsPerCall`: The maximum number of embeddings allowed per call

\- `values`: The array of values that was provided

\## Checking for this Error

You can check if an error is an instance of `AI\_TooManyEmbeddingValuesForCallError` using:

```typescript

import { TooManyEmbeddingValuesForCallError } from 'ai';



if (TooManyEmbeddingValuesForCallError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_ToolCallNotFoundForApprovalError

description: Learn how to fix AI_ToolCallNotFoundForApprovalError

---

\# AI_ToolCallNotFoundForApprovalError

This error occurs when a tool approval request references a tool call that was not found. This can happen when processing provider-emitted approval requests (e.g., MCP flows) where the referenced tool call ID does not exist.

\## Properties

\- `toolCallId`: The tool call ID that was not found

\- `approvalId`: The approval request ID

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_ToolCallNotFoundForApprovalError` using:

```typescript

import { ToolCallNotFoundForApprovalError } from 'ai';



if (ToolCallNotFoundForApprovalError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: ToolCallRepairError

description: Learn how to fix AI SDK ToolCallRepairError

---

\# ToolCallRepairError

This error occurs when there is a failure while attempting to repair an invalid tool call.

This typically happens when the AI attempts to fix either

a `NoSuchToolError` or `InvalidToolInputError`.

\## Properties

\- `originalError`: The original error that triggered the repair attempt (either `NoSuchToolError` or `InvalidToolInputError`)

\- `message`: The error message

\- `cause`: The underlying error that caused the repair to fail

\## Checking for this Error

You can check if an error is an instance of `ToolCallRepairError` using:

```typescript

import { ToolCallRepairError } from 'ai';



if (ToolCallRepairError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_TypeValidationError

description: Learn how to fix AI_TypeValidationError

---

\# AI_TypeValidationError

This error occurs when type validation fails.

\## Properties

\- `value`: The value that failed validation

\- `message`: The error message including validation details

\## Checking for this Error

You can check if an error is an instance of `AI\_TypeValidationError` using:

```typescript

import { TypeValidationError } from 'ai';



if (TypeValidationError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI_UIMessageStreamError

description: Learn how to fix AI_UIMessageStreamError

---

\# AI_UIMessageStreamError

This error occurs when a UI message stream contains invalid or out-of-sequence chunks.

Common causes:

\- Receiving a `text-delta` chunk without a preceding `text-start` chunk

\- Receiving a `text-end` chunk without a preceding `text-start` chunk

\- Receiving a `reasoning-delta` chunk without a preceding `reasoning-start` chunk

\- Receiving a `reasoning-end` chunk without a preceding `reasoning-start` chunk

\- Receiving a `tool-input-delta` chunk without a preceding `tool-input-start` chunk

\- Attempting to access a tool invocation that doesn't exist

This error often surfaces when an upstream request fails \*\*before any tokens are streamed\*\* and a custom transport tries to write an inline error message to the UI stream without the proper start chunk.

\## Properties

\- `chunkType`: The type of chunk that caused the error (e.g., `text-delta`, `reasoning-end`, `tool-input-delta`)

\- `chunkId`: The ID associated with the failing chunk (part ID or toolCallId)

\- `message`: The error message with details about what went wrong

\## Checking for this Error

You can check if an error is an instance of `AI\_UIMessageStreamError` using:

```typescript

import { UIMessageStreamError } from 'ai';



if (UIMessageStreamError.isInstance(error)) {

&nbsp; console.log('Chunk type:', error.chunkType);

&nbsp; console.log('Chunk ID:', error.chunkId);

&nbsp; // Handle the error

}

```

\## Common Solutions

1\. \*\*Ensure proper chunk ordering\*\*: Always send a `\*-start` chunk before any `\*-delta` or `\*-end` chunks for the same ID:

&nbsp; ```typescript

&nbsp; // Correct order

&nbsp; writer.write({ type: 'text-start', id: 'my-text' });

&nbsp; writer.write({ type: 'text-delta', id: 'my-text', delta: 'Hello' });

&nbsp; writer.write({ type: 'text-end', id: 'my-text' });

&nbsp; ```

2\. \*\*Verify IDs match\*\*: Ensure the `id` used in `\*-delta` and `\*-end` chunks matches the `id` used in the corresponding `\*-start` chunk.

3\. \*\*Handle error paths correctly\*\*: When writing error messages in custom transports, ensure you emit the full start/delta/end sequence:

&nbsp; ```typescript

&nbsp; // When handling errors in custom transports

&nbsp; writer.write({ type: 'text-start', id: errorId });

&nbsp; writer.write({

&nbsp; type: 'text-delta',

&nbsp; id: errorId,

&nbsp; delta: 'Request failed...',

&nbsp; });

&nbsp; writer.write({ type: 'text-end', id: errorId });

&nbsp; ```

4\. \*\*Check stream producer logic\*\*: Review your streaming implementation to ensure chunks are sent in the correct order, especially when dealing with concurrent operations or merged streams.

---

title: AI_UnsupportedFunctionalityError

description: Learn how to fix AI_UnsupportedFunctionalityError

---

\# AI_UnsupportedFunctionalityError

This error occurs when functionality is not unsupported.

\## Properties

\- `functionality`: The name of the unsupported functionality

\- `message`: The error message

\## Checking for this Error

You can check if an error is an instance of `AI\_UnsupportedFunctionalityError` using:

```typescript

import { UnsupportedFunctionalityError } from 'ai';



if (UnsupportedFunctionalityError.isInstance(error)) {

&nbsp; // Handle the error

}

```

---

title: AI Gateway

description: Learn how to use the AI Gateway provider with the AI SDK.

---

\# AI Gateway Provider

The \[AI Gateway](https://vercel.com/docs/ai-gateway) provider connects you to models from multiple AI providers through a single interface. Instead of integrating with each provider separately, you can access OpenAI, Anthropic, Google, Meta, xAI, and other providers and their models.

\## Features

\- Access models from multiple providers without having to install additional provider modules/dependencies

\- Use the same code structure across different AI providers

\- Switch between models and providers easily

\- Automatic authentication when deployed on Vercel

\- View pricing information across providers

\- Observability for AI model usage through the Vercel dashboard

\## Setup

The Vercel AI Gateway provider is part of the AI SDK.

\## Basic Usage

For most use cases, you can use the AI Gateway directly with a model string:

```ts

// use plain model string with global provider

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'openai/gpt-5',

&nbsp; prompt: 'Hello world',

});

```

```ts

// use provider instance (requires version 5.0.36 or later)

import { generateText, gateway } from 'ai';



const { text } = await generateText({

&nbsp; model: gateway('openai/gpt-5'),

&nbsp; prompt: 'Hello world',

});

```

The AI SDK automatically uses the AI Gateway when you pass a model string in the `creator/model-name` format.

\## Provider Instance

<Note>

&nbsp; The `gateway` provider instance is available from the `ai` package in version

&nbsp; 5.0.36 and later.

</Note>

You can also import the default provider instance `gateway` from `ai`:

```ts
import { gateway } from "ai";
```

You may want to create a custom provider instance when you need to:

\- Set custom configuration options (API key, base URL, headers)

\- Use the provider in a \[provider registry](/docs/ai-sdk-core/provider-management)

\- Wrap the provider with \[middleware](/docs/ai-sdk-core/middleware)

\- Use different settings for different parts of your application

To create a custom provider instance, import `createGateway` from `ai`:

```ts

import { createGateway } from 'ai';



const gateway = createGateway({

&nbsp; apiKey: process.env.AI\_GATEWAY\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the AI Gateway provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls. The default prefix is `https://ai-gateway.vercel.sh/v3/ai`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `AI\_GATEWAY\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\- \*\*metadataCacheRefreshMillis\*\* \_number\_

&nbsp; How frequently to refresh the metadata cache in milliseconds. Defaults to 5 minutes (300,000ms).

\## Authentication

The Gateway provider supports two authentication methods:

\### API Key Authentication

Set your API key via environment variable:

```bash

AI\_GATEWAY\_API\_KEY=your\_api\_key\_here

```

Or pass it directly to the provider:

```ts

import { createGateway } from 'ai';



const gateway = createGateway({

&nbsp; apiKey: 'your\_api\_key\_here',

});

```

\### OIDC Authentication (Vercel Deployments)

When deployed to Vercel, the AI Gateway provider supports authenticating using \[OIDC (OpenID Connect)

tokens](https://vercel.com/docs/oidc) without API Keys.

\#### How OIDC Authentication Works

1\. \*\*In Production/Preview Deployments\*\*:

&nbsp; - OIDC authentication is automatically handled

&nbsp; - No manual configuration needed

&nbsp; - Tokens are automatically obtained and refreshed

2\. \*\*In Local Development\*\*:

&nbsp; - First, install and authenticate with the \[Vercel CLI](https://vercel.com/docs/cli)

&nbsp; - Run `vercel env pull` to download your project's OIDC token locally

&nbsp; - For automatic token management:

&nbsp; - Use `vercel dev` to start your development server - this will handle token refreshing automatically

&nbsp; - For manual token management:

&nbsp; - If not using `vercel dev`, note that OIDC tokens expire after 12 hours

&nbsp; - You'll need to run `vercel env pull` again to refresh the token before it expires

<Note>

&nbsp; If an API Key is present (either passed directly or via environment), it will

&nbsp; always be used, even if invalid.

</Note>

Read more about using OIDC tokens in the \[Vercel AI Gateway docs](https://vercel.com/docs/ai-gateway#using-the-ai-gateway-with-a-vercel-oidc-token).

\## Bring Your Own Key (BYOK)

You can connect your own provider credentials to use with Vercel AI Gateway. This lets you use your existing provider accounts and access private resources.

To set up BYOK, add your provider credentials in your Vercel team's AI Gateway settings. Once configured, AI Gateway automatically uses your credentials. No code changes are needed.

Learn more in the \[BYOK documentation](https://vercel.com/docs/ai-gateway/byok).

\## Language Models

You can create language models using a provider instance. The first argument is the model ID in the format `creator/model-name`:

```ts

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'openai/gpt-5',

&nbsp; prompt: 'Explain quantum computing in simple terms',

});

```

AI Gateway language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see \[AI SDK Core](/docs/ai-sdk-core)).

\## Available Models

The AI Gateway supports models from OpenAI, Anthropic, Google, Meta, xAI, Mistral, DeepSeek, Amazon Bedrock, Cohere, Perplexity, Alibaba, and other providers.

For the complete list of available models, see the \[AI Gateway documentation](https://vercel.com/docs/ai-gateway).

\## Dynamic Model Discovery

You can discover available models programmatically:

```ts

import { gateway, generateText } from 'ai';



const availableModels = await gateway.getAvailableModels();



// List all available models

availableModels.models.forEach(model => {

&nbsp; console.log(`${model.id}: ${model.name}`);

&nbsp; if (model.description) {

&nbsp;   console.log(`  Description: ${model.description}`);

&nbsp; }

&nbsp; if (model.pricing) {

&nbsp;   console.log(`  Input: $${model.pricing.input}/token`);

&nbsp;   console.log(`  Output: $${model.pricing.output}/token`);

&nbsp;   if (model.pricing.cachedInputTokens) {

&nbsp;     console.log(

&nbsp;       `  Cached input (read): $${model.pricing.cachedInputTokens}/token`,

&nbsp;     );

&nbsp;   }

&nbsp;   if (model.pricing.cacheCreationInputTokens) {

&nbsp;     console.log(

&nbsp;       `  Cache creation (write): $${model.pricing.cacheCreationInputTokens}/token`,

&nbsp;     );

&nbsp;   }

&nbsp; }

});



// Use any discovered model with plain string

const { text } = await generateText({

&nbsp; model: availableModels.models\[0].id, // e.g., 'openai/gpt-4o'

&nbsp; prompt: 'Hello world',

});

```

\## Credit Usage

You can check your team's current credit balance and usage:

```ts

import { gateway } from 'ai';



const credits = await gateway.getCredits();



console.log(`Team balance: ${credits.balance} credits`);

console.log(`Team total used: ${credits.total\_used} credits`);

```

The `getCredits()` method returns your team's credit information based on the authenticated API key or OIDC token:

\- \*\*balance\*\* \_number\_ - Your team's current available credit balance

\- \*\*total_used\*\* \_number\_ - Total credits consumed by your team

\## Examples

\### Basic Text Generation

```ts

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4',

&nbsp; prompt: 'Write a haiku about programming',

});



console.log(text);

```

\### Streaming

```ts

import { streamText } from 'ai';



const { textStream } = await streamText({

&nbsp; model: 'openai/gpt-5',

&nbsp; prompt: 'Explain the benefits of serverless architecture',

});



for await (const textPart of textStream) {

&nbsp; process.stdout.write(textPart);

}

```

\### Tool Usage

```ts

import { generateText, tool } from 'ai';

import { z } from 'zod';



const { text } = await generateText({

&nbsp; model: 'xai/grok-4',

&nbsp; prompt: 'What is the weather like in San Francisco?',

&nbsp; tools: {

&nbsp;   getWeather: tool({

&nbsp;     description: 'Get the current weather for a location',

&nbsp;     parameters: z.object({

&nbsp;       location: z.string().describe('The location to get weather for'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => {

&nbsp;       // Your weather API call here

&nbsp;       return `It's sunny in ${location}`;

&nbsp;     },

&nbsp;   }),

&nbsp; },

});

```

\### Provider-Executed Tools

Some providers offer tools that are executed by the provider itself, such as \[OpenAI's web search tool](/providers/ai-sdk-providers/openai#web-search-tool). To use these tools through AI Gateway, import the provider to access the tool definitions:

```ts

import { generateText, stepCountIs } from 'ai';

import { openai } from '@ai-sdk/openai';



const result = await generateText({

&nbsp; model: 'openai/gpt-5-mini',

&nbsp; prompt: 'What is the Vercel AI Gateway?',

&nbsp; stopWhen: stepCountIs(10),

&nbsp; tools: {

&nbsp;   web\_search: openai.tools.webSearch({}),

&nbsp; },

});



console.dir(result.text);

```

<Note>

&nbsp; Some provider-executed tools require account-specific configuration (such as

&nbsp; Claude Agent Skills) and may not work through AI Gateway. To use these tools,

&nbsp; you must bring your own key (BYOK) directly to the provider.

</Note>

\### Gateway Tools

The AI Gateway provider includes built-in tools that are executed by the gateway itself. These tools can be used with any model through the gateway.

\#### Perplexity Search

The Perplexity Search tool enables models to search the web using \[Perplexity's search API](https://docs.perplexity.ai/guides/search-quickstart). This tool is executed by the AI Gateway and returns web search results that the model can use to provide up-to-date information.

```ts

import { gateway, generateText } from 'ai';



const result = await generateText({

&nbsp; model: 'openai/gpt-5-nano',

&nbsp; prompt: 'Search for news about AI regulations in January 2025.',

&nbsp; tools: {

&nbsp;   perplexity\_search: gateway.tools.perplexitySearch(),

&nbsp; },

});



console.log(result.text);

console.log('Tool calls:', JSON.stringify(result.toolCalls, null, 2));

console.log('Tool results:', JSON.stringify(result.toolResults, null, 2));

```

You can also configure the search with optional parameters:

```ts

import { gateway, generateText } from 'ai';



const result = await generateText({

&nbsp; model: 'openai/gpt-5-nano',

&nbsp; prompt:

&nbsp;   'Search for news about AI regulations from the first week of January 2025.',

&nbsp; tools: {

&nbsp;   perplexity\_search: gateway.tools.perplexitySearch({

&nbsp;     maxResults: 5,

&nbsp;     searchLanguageFilter: \['en'],

&nbsp;     country: 'US',

&nbsp;     searchDomainFilter: \['reuters.com', 'bbc.com', 'nytimes.com'],

&nbsp;   }),

&nbsp; },

});



console.log(result.text);

console.log('Tool calls:', JSON.stringify(result.toolCalls, null, 2));

console.log('Tool results:', JSON.stringify(result.toolResults, null, 2));

```

The Perplexity Search tool supports the following optional configuration options:

\- \*\*maxResults\*\* \_number\_

&nbsp; The maximum number of search results to return (1-20, default: 10).

\- \*\*maxTokensPerPage\*\* \_number\_

&nbsp; The maximum number of tokens to extract per search result page (256-2048, default: 2048).

\- \*\*maxTokens\*\* \_number\_

&nbsp; The maximum total tokens across all search results (default: 25000, max: 1000000).

\- \*\*searchLanguageFilter\*\* \_string\[]\_

&nbsp; Filter search results by language using ISO 639-1 language codes (e.g., `\['en']` for English, `\['en', 'es']` for English and Spanish).

\- \*\*country\*\* \_string\_

&nbsp; Filter search results by country using ISO 3166-1 alpha-2 country codes (e.g., `'US'` for United States, `'GB'` for United Kingdom).

\- \*\*searchDomainFilter\*\* \_string\[]\_

&nbsp; Limit search results to specific domains (e.g., `\['reuters.com', 'bbc.com']`). This is useful for restricting results to trusted sources.

\- \*\*searchRecencyFilter\*\* \_'day' | 'week' | 'month' | 'year'\_

&nbsp; Filter search results by relative time period. Useful for always getting recent results (e.g., 'week' for results from the last week).

The tool works with both `generateText` and `streamText`:

```ts

import { gateway, streamText } from 'ai';



const result = streamText({

&nbsp; model: 'openai/gpt-5-nano',

&nbsp; prompt: 'Search for the latest news about AI regulations.',

&nbsp; tools: {

&nbsp;   perplexity\_search: gateway.tools.perplexitySearch(),

&nbsp; },

});



for await (const part of result.fullStream) {

&nbsp; switch (part.type) {

&nbsp;   case 'text-delta':

&nbsp;     process.stdout.write(part.text);

&nbsp;     break;

&nbsp;   case 'tool-call':

&nbsp;     console.log('\\nTool call:', JSON.stringify(part, null, 2));

&nbsp;     break;

&nbsp;   case 'tool-result':

&nbsp;     console.log('\\nTool result:', JSON.stringify(part, null, 2));

&nbsp;     break;

&nbsp; }

}

```

\### Usage Tracking with User and Tags

Track usage per end-user and categorize requests with tags:

```ts

import type { GatewayProviderOptions } from '@ai-sdk/gateway';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'openai/gpt-5',

&nbsp; prompt: 'Summarize this document...',

&nbsp; providerOptions: {

&nbsp;   gateway: {

&nbsp;     user: 'user-abc-123', // Track usage for this specific end-user

&nbsp;     tags: \['document-summary', 'premium-feature'], // Categorize for reporting

&nbsp;   } satisfies GatewayProviderOptions,

&nbsp; },

});

```

This allows you to:

\- View usage and costs broken down by end-user in your analytics

\- Filter and analyze spending by feature or use case using tags

\- Track which users or features are driving the most AI usage

\## Provider Options

The AI Gateway provider accepts provider options that control routing behavior and provider-specific configurations.

\### Gateway Provider Options

You can use the `gateway` key in `providerOptions` to control how AI Gateway routes requests:

```ts

import type { GatewayProviderOptions } from '@ai-sdk/gateway';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4',

&nbsp; prompt: 'Explain quantum computing',

&nbsp; providerOptions: {

&nbsp;   gateway: {

&nbsp;     order: \['vertex', 'anthropic'], // Try Vertex AI first, then Anthropic

&nbsp;     only: \['vertex', 'anthropic'], // Only use these providers

&nbsp;   } satisfies GatewayProviderOptions,

&nbsp; },

});

```

The following gateway provider options are available:

\- \*\*order\*\* \_string\[]\_

&nbsp; Specifies the sequence of providers to attempt when routing requests. The gateway will try providers in the order specified. If a provider fails or is unavailable, it will move to the next provider in the list.

&nbsp; Example: `order: \['bedrock', 'anthropic']` will attempt Amazon Bedrock first, then fall back to Anthropic.

\- \*\*only\*\* \_string\[]\_

&nbsp; Restricts routing to only the specified providers. When set, the gateway will never route to providers not in this list, even if they would otherwise be available.

&nbsp; Example: `only: \['anthropic', 'vertex']` will only allow routing to Anthropic or Vertex AI.

\- \*\*models\*\* \_string\[]\_

&nbsp; Specifies fallback models to use when the primary model fails or is unavailable. The gateway will try the primary model first (specified in the `model` parameter), then try each model in this array in order until one succeeds.

&nbsp; Example: `models: \['openai/gpt-5-nano', 'gemini-2.0-flash']` will try the fallback models in order if the primary model fails.

\- \*\*user\*\* \_string\_

&nbsp; Optional identifier for the end user on whose behalf the request is being made. This is used for spend tracking and attribution purposes, allowing you to track usage per end-user in your application.

&nbsp; Example: `user: 'user-123'` will associate this request with end-user ID "user-123" in usage reports.

\- \*\*tags\*\* \_string\[]\_

&nbsp; Optional array of tags for categorizing and filtering usage in reports. Useful for tracking spend by feature, prompt version, or any other dimension relevant to your application.

&nbsp; Example: `tags: \['chat', 'v2']` will tag this request with "chat" and "v2" for filtering in usage analytics.

\- \*\*byok\*\* \_Record\&lt;string, Array\&lt;Record\&lt;string, unknown\&gt;\&gt;\&gt;\_

&nbsp; Request-scoped BYOK (Bring Your Own Key) credentials to use for this request. When provided, any cached BYOK credentials configured in the gateway system are not considered. Requests may still fall back to use system credentials if the provided credentials fail.

&nbsp; Each provider can have multiple credentials (tried in order). The structure is a record where keys are provider slugs and values are arrays of credential objects.

&nbsp; Examples:

&nbsp; - Single provider: `byok: { 'anthropic': \[{ apiKey: 'sk-ant-...' }] }`

&nbsp; - Multiple credentials: `byok: { 'vertex': \[{ project: 'proj-1', googleCredentials: { privateKey: '...', clientEmail: '...' } }, { project: 'proj-2', googleCredentials: { privateKey: '...', clientEmail: '...' } }] }`

&nbsp; - Multiple providers: `byok: { 'anthropic': \[{ apiKey: '...' }], 'bedrock': \[{ accessKeyId: '...', secretAccessKey: '...' }] }`

\- \*\*zeroDataRetention\*\* \_boolean\_

&nbsp; Restricts routing requests to providers that have zero data retention policies.

You can combine these options to have fine-grained control over routing and tracking:

```ts

import type { GatewayProviderOptions } from '@ai-sdk/gateway';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4',

&nbsp; prompt: 'Write a haiku about programming',

&nbsp; providerOptions: {

&nbsp;   gateway: {

&nbsp;     order: \['vertex'], // Prefer Vertex AI

&nbsp;     only: \['anthropic', 'vertex'], // Only allow these providers

&nbsp;   } satisfies GatewayProviderOptions,

&nbsp; },

});

```

\#### Model Fallbacks Example

The `models` option enables automatic fallback to alternative models when the primary model fails:

```ts

import type { GatewayProviderOptions } from '@ai-sdk/gateway';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'openai/gpt-4o', // Primary model

&nbsp; prompt: 'Write a TypeScript haiku',

&nbsp; providerOptions: {

&nbsp;   gateway: {

&nbsp;     models: \['openai/gpt-5-nano', 'gemini-2.0-flash'], // Fallback models

&nbsp;   } satisfies GatewayProviderOptions,

&nbsp; },

});



// This will:

// 1. Try openai/gpt-4o first

// 2. If it fails, try openai/gpt-5-nano

// 3. If that fails, try gemini-2.0-flash

// 4. Return the result from the first model that succeeds

```

\#### Zero Data Retention Example

Set `zeroDataRetention` to true to ensure requests are only routed to providers

that have zero data retention policies. When `zeroDataRetention` is `false` or not

specified, there is no enforcement of restricting routing.

```ts

import type { GatewayProviderOptions } from '@ai-sdk/gateway';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4.5',

&nbsp; prompt: 'Analyze this sensitive document...',

&nbsp; providerOptions: {

&nbsp;   gateway: {

&nbsp;     zeroDataRetention: true,

&nbsp;   } satisfies GatewayProviderOptions,

&nbsp; },

});

```

\### Provider-Specific Options

When using provider-specific options through AI Gateway, use the actual provider name (e.g. `anthropic`, `openai`, not `gateway`) as the key:

```ts

import type { AnthropicProviderOptions } from '@ai-sdk/anthropic';

import type { GatewayProviderOptions } from '@ai-sdk/gateway';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4',

&nbsp; prompt: 'Explain quantum computing',

&nbsp; providerOptions: {

&nbsp;   gateway: {

&nbsp;     order: \['vertex', 'anthropic'],

&nbsp;   } satisfies GatewayProviderOptions,

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});

```

This works with any provider supported by AI Gateway. Each provider has its own set of options - see the individual \[provider documentation pages](/providers/ai-sdk-providers) for details on provider-specific options.

\### Available Providers

AI Gateway supports routing to 20+ providers.

For a complete list of available providers and their slugs, see the \[AI Gateway documentation](https://vercel.com/docs/ai-gateway/provider-options#available-providers).

\## Model Capabilities

Model capabilities depend on the specific provider and model you're using. For detailed capability information, see:

\- \[AI Gateway provider options](https://vercel.com/docs/ai-gateway/provider-options#available-providers) for an overview of available providers

\- Individual \[AI SDK provider pages](/providers/ai-sdk-providers) for specific model capabilities and features

---

title: xAI Grok

description: Learn how to use xAI Grok.

---

\# xAI Grok Provider

The \[xAI Grok](https://x.ai) provider contains language model support for the \[xAI API](https://x.ai/api).

\## Setup

The xAI Grok provider is available via the `@ai-sdk/xai` module. You can

install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/xai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/xai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/xai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/xai" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `xai` from `@ai-sdk/xai`:

```ts
import { xai } from "@ai-sdk/xai";
```

If you need a customized setup, you can import `createXai` from `@ai-sdk/xai`

and create a provider instance with your settings:

```ts

import { createXai } from '@ai-sdk/xai';



const xai = createXai({

&nbsp; apiKey: 'your-api-key',

});

```

You can use the following optional settings to customize the xAI provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.x.ai/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `XAI\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create \[xAI models](https://console.x.ai) using a provider instance. The

first argument is the model id, e.g. `grok-3`.

```ts
const model = xai("grok-3");
```

By default, `xai(modelId)` uses the Chat API. To use the Responses API with server-side agentic tools, explicitly use `xai.responses(modelId)`.

\### Example

You can use xAI language models to generate text with the `generateText` function:

```ts

import { xai } from '@ai-sdk/xai';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: xai('grok-3'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

xAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

\### Provider Options

xAI chat models support additional provider options that are not part of

the \[standard call settings](/docs/ai-sdk-core/settings). You can pass them in the `providerOptions` argument:

```ts

const model = xai('grok-3-mini');



await generateText({

&nbsp; model,

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     reasoningEffort: 'high',

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for xAI chat models:

\- \*\*reasoningEffort\*\* \_'low' | 'medium' | 'high'\_

&nbsp; Reasoning effort for reasoning models.

\- \*\*store\*\* \_boolean\_

&nbsp; Whether to store the generation. Defaults to `true`.

\- \*\*previousResponseId\*\* \_string\_

&nbsp; The ID of the previous response. You can use it to continue a conversation. Defaults to `undefined`.

\## Responses API (Agentic Tools)

You can use the xAI Responses API with the `xai.responses(modelId)` factory method for server-side agentic tool calling. This enables the model to autonomously orchestrate tool calls and research on xAI's servers.

```ts
const model = xai.responses("grok-4-fast");
```

The Responses API provides server-side tools that the model can autonomously execute during its reasoning process:

\- \*\*web_search\*\*: Real-time web search and page browsing

\- \*\*x_search\*\*: Search X (Twitter) posts, users, and threads

\- \*\*code_execution\*\*: Execute Python code for calculations and data analysis

\- \*\*mcp_server\*\*: Connect to remote MCP servers and use their tools

\### Vision

The Responses API supports image input with vision models:

```ts

import { xai } from '@ai-sdk/xai';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: xai.responses('grok-2-vision-1212'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'What do you see in this image?' },

&nbsp;       { type: 'image', image: fs.readFileSync('./image.png') },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

\### Web Search Tool

The web search tool enables autonomous web research with optional domain filtering and image understanding:

```ts

import { xai } from '@ai-sdk/xai';

import { generateText } from 'ai';



const { text, sources } = await generateText({

&nbsp; model: xai.responses('grok-4-fast'),

&nbsp; prompt: 'What are the latest developments in AI?',

&nbsp; tools: {

&nbsp;   web\_search: xai.tools.webSearch({

&nbsp;     allowedDomains: \['arxiv.org', 'openai.com'],

&nbsp;     enableImageUnderstanding: true,

&nbsp;   }),

&nbsp; },

});



console.log(text);

console.log('Citations:', sources);

```

\#### Web Search Parameters

\- \*\*allowedDomains\*\* \_string\[]\_

&nbsp; Only search within specified domains (max 5). Cannot be used with `excludedDomains`.

\- \*\*excludedDomains\*\* \_string\[]\_

&nbsp; Exclude specified domains from search (max 5). Cannot be used with `allowedDomains`.

\- \*\*enableImageUnderstanding\*\* \_boolean\_

&nbsp; Enable the model to view and analyze images found during search. Increases token usage.

\### X Search Tool

The X search tool enables searching X (Twitter) for posts, with filtering by handles and date ranges:

```ts

const { text, sources } = await generateText({

&nbsp; model: xai.responses('grok-4-fast'),

&nbsp; prompt: 'What are people saying about AI on X this week?',

&nbsp; tools: {

&nbsp;   x\_search: xai.tools.xSearch({

&nbsp;     allowedXHandles: \['elonmusk', 'xai'],

&nbsp;     fromDate: '2025-10-23',

&nbsp;     toDate: '2025-10-30',

&nbsp;     enableImageUnderstanding: true,

&nbsp;     enableVideoUnderstanding: true,

&nbsp;   }),

&nbsp; },

});

```

\#### X Search Parameters

\- \*\*allowedXHandles\*\* \_string\[]\_

&nbsp; Only search posts from specified X handles (max 10). Cannot be used with `excludedXHandles`.

\- \*\*excludedXHandles\*\* \_string\[]\_

&nbsp; Exclude posts from specified X handles (max 10). Cannot be used with `allowedXHandles`.

\- \*\*fromDate\*\* \_string\_

&nbsp; Start date for posts in ISO8601 format (`YYYY-MM-DD`).

\- \*\*toDate\*\* \_string\_

&nbsp; End date for posts in ISO8601 format (`YYYY-MM-DD`).

\- \*\*enableImageUnderstanding\*\* \_boolean\_

&nbsp; Enable the model to view and analyze images in X posts.

\- \*\*enableVideoUnderstanding\*\* \_boolean\_

&nbsp; Enable the model to view and analyze videos in X posts.

\### Code Execution Tool

The code execution tool enables the model to write and execute Python code for calculations and data analysis:

```ts

const { text } = await generateText({

&nbsp; model: xai.responses('grok-4-fast'),

&nbsp; prompt:

&nbsp;   'Calculate the compound interest for $10,000 at 5% annually for 10 years',

&nbsp; tools: {

&nbsp;   code\_execution: xai.tools.codeExecution(),

&nbsp; },

});

```

\### MCP Server Tool

The MCP server tool enables the model to connect to remote \[Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers and use their tools:

```ts

const { text } = await generateText({

&nbsp; model: xai.responses('grok-4-fast'),

&nbsp; prompt: 'Use the weather tool to check conditions in San Francisco',

&nbsp; tools: {

&nbsp;   weather\_server: xai.tools.mcpServer({

&nbsp;     serverUrl: 'https://example.com/mcp',

&nbsp;     serverLabel: 'weather-service',

&nbsp;     serverDescription: 'Weather data provider',

&nbsp;     allowedTools: \['get\_weather', 'get\_forecast'],

&nbsp;   }),

&nbsp; },

});

```

\#### MCP Server Parameters

\- \*\*serverUrl\*\* \_string\_ (required)

&nbsp; The URL of the remote MCP server.

\- \*\*serverLabel\*\* \_string\_ (required)

&nbsp; A label to identify the MCP server.

\- \*\*serverDescription\*\* \_string\_

&nbsp; A description of what the MCP server provides.

\- \*\*allowedTools\*\* \_string\[]\_

&nbsp; List of tool names that the model is allowed to use from the MCP server. If not specified, all tools are allowed.

\- \*\*headers\*\* \_Record\&lt;string, string\&gt;\_

&nbsp; Custom headers to include when connecting to the MCP server.

\- \*\*authorization\*\* \_string\_

&nbsp; Authorization header value for authenticating with the MCP server (e.g., `'Bearer token123'`).

\### File Search Tool

xAI supports file search through OpenAI compatibility. You can use the OpenAI provider with xAI's base URL to search vector stores:

```ts

import { createOpenAI } from '@ai-sdk/openai';

import { streamText } from 'ai';



const openai = createOpenAI({

&nbsp; baseURL: 'https://api.x.ai/v1',

&nbsp; apiKey: process.env.XAI\_API\_KEY,

});



const result = streamText({

&nbsp; model: openai('grok-4'),

&nbsp; prompt: 'What documents do you have access to?',

&nbsp; tools: {

&nbsp;   file\_search: openai.tools.fileSearch({

&nbsp;     vectorStoreIds: \['your-vector-store-id'],

&nbsp;     maxNumResults: 5,

&nbsp;   }),

&nbsp; },

});

```

<Note>

&nbsp; File search requires grok-4 family models. See the \[OpenAI

&nbsp; provider](/providers/ai-sdk-providers/openai) documentation for additional

&nbsp; file search options like filters and ranking.

</Note>

\### Multiple Tools

You can combine multiple server-side tools for comprehensive research:

```ts

import { xai } from '@ai-sdk/xai';

import { streamText } from 'ai';



const { fullStream } = streamText({

&nbsp; model: xai.responses('grok-4-fast'),

&nbsp; prompt: 'Research AI safety developments and calculate risk metrics',

&nbsp; tools: {

&nbsp;   web\_search: xai.tools.webSearch(),

&nbsp;   x\_search: xai.tools.xSearch(),

&nbsp;   code\_execution: xai.tools.codeExecution(),

&nbsp;   data\_service: xai.tools.mcpServer({

&nbsp;     serverUrl: 'https://data.example.com/mcp',

&nbsp;     serverLabel: 'data-service',

&nbsp;   }),

&nbsp; },

});



for await (const part of fullStream) {

&nbsp; if (part.type === 'text-delta') {

&nbsp;   process.stdout.write(part.text);

&nbsp; } else if (part.type === 'source' \&\& part.sourceType === 'url') {

&nbsp;   console.log('\\nSource:', part.url);

&nbsp; }

}

```

\### Provider Options

The Responses API supports the following provider options:

```ts

import { xai } from '@ai-sdk/xai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: xai.responses('grok-4-fast'),

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     reasoningEffort: 'high',

&nbsp;   },

&nbsp; },

&nbsp; // ...

});

```

The following provider options are available:

\- \*\*reasoningEffort\*\* \_'low' | 'high'\_

&nbsp; Control the reasoning effort for the model. Higher effort may produce more thorough results at the cost of increased latency and token usage.

<Note>

&nbsp; The Responses API only supports server-side tools. You cannot mix server-side

&nbsp; tools with client-side function tools in the same request.

</Note>

\## Live Search

xAI models support Live Search functionality, allowing them to query real-time data from various sources and include it in responses with citations.

\### Basic Search

To enable search, specify `searchParameters` with a search mode:

```ts

import { xai } from '@ai-sdk/xai';

import { generateText } from 'ai';



const { text, sources } = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'What are the latest developments in AI?',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'auto', // 'auto', 'on', or 'off'

&nbsp;       returnCitations: true,

&nbsp;       maxSearchResults: 5,

&nbsp;     },

&nbsp;   },

&nbsp; },

});



console.log(text);

console.log('Sources:', sources);

```

\### Search Parameters

The following search parameters are available:

\- \*\*mode\*\* \_'auto' | 'on' | 'off'\_

&nbsp; Search mode preference:

&nbsp; - `'auto'` (default): Model decides whether to search

&nbsp; - `'on'`: Always enables search

&nbsp; - `'off'`: Disables search completely

\- \*\*returnCitations\*\* \_boolean\_

&nbsp; Whether to return citations in the response. Defaults to `true`.

\- \*\*fromDate\*\* \_string\_

&nbsp; Start date for search data in ISO8601 format (`YYYY-MM-DD`).

\- \*\*toDate\*\* \_string\_

&nbsp; End date for search data in ISO8601 format (`YYYY-MM-DD`).

\- \*\*maxSearchResults\*\* \_number\_

&nbsp; Maximum number of search results to consider. Defaults to 20, max 50.

\- \*\*sources\*\* \_Array\&lt;SearchSource\&gt;\_

&nbsp; Data sources to search from. Defaults to `\["web", "x"]` if not specified.

\### Search Sources

You can specify different types of data sources for search:

\#### Web Search

```ts

const result = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'Best ski resorts in Switzerland',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'on',

&nbsp;       sources: \[

&nbsp;         {

&nbsp;           type: 'web',

&nbsp;           country: 'CH', // ISO alpha-2 country code

&nbsp;           allowedWebsites: \['ski.com', 'snow-forecast.com'],

&nbsp;           safeSearch: true,

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

\#### Web source parameters

\- \*\*country\*\* \_string\_: ISO alpha-2 country code

\- \*\*allowedWebsites\*\* \_string\[]\_: Max 5 allowed websites

\- \*\*excludedWebsites\*\* \_string\[]\_: Max 5 excluded websites

\- \*\*safeSearch\*\* \_boolean\_: Enable safe search (default: true)

\#### X (Twitter) Search

```ts

const result = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'Latest updates on Grok AI',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'on',

&nbsp;       sources: \[

&nbsp;         {

&nbsp;           type: 'x',

&nbsp;           includedXHandles: \['grok', 'xai'],

&nbsp;           excludedXHandles: \['openai'],

&nbsp;           postFavoriteCount: 10,

&nbsp;           postViewCount: 100,

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

\#### X source parameters

\- \*\*includedXHandles\*\* \_string\[]\_: Array of X handles to search (without @ symbol)

\- \*\*excludedXHandles\*\* \_string\[]\_: Array of X handles to exclude from search (without @ symbol)

\- \*\*postFavoriteCount\*\* \_number\_: Minimum favorite count of the X posts to consider.

\- \*\*postViewCount\*\* \_number\_: Minimum view count of the X posts to consider.

\#### News Search

```ts

const result = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'Recent tech industry news',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'on',

&nbsp;       sources: \[

&nbsp;         {

&nbsp;           type: 'news',

&nbsp;           country: 'US',

&nbsp;           excludedWebsites: \['tabloid.com'],

&nbsp;           safeSearch: true,

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

\#### News source parameters

\- \*\*country\*\* \_string\_: ISO alpha-2 country code

\- \*\*excludedWebsites\*\* \_string\[]\_: Max 5 excluded websites

\- \*\*safeSearch\*\* \_boolean\_: Enable safe search (default: true)

\#### RSS Feed Search

```ts

const result = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'Latest status updates',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'on',

&nbsp;       sources: \[

&nbsp;         {

&nbsp;           type: 'rss',

&nbsp;           links: \['https://status.x.ai/feed.xml'],

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

\#### RSS source parameters

\- \*\*links\*\* \_string\[]\_: Array of RSS feed URLs (max 1 currently supported)

\### Multiple Sources

You can combine multiple data sources in a single search:

```ts

const result = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'Comprehensive overview of recent AI breakthroughs',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'on',

&nbsp;       returnCitations: true,

&nbsp;       maxSearchResults: 15,

&nbsp;       sources: \[

&nbsp;         {

&nbsp;           type: 'web',

&nbsp;           allowedWebsites: \['arxiv.org', 'openai.com'],

&nbsp;         },

&nbsp;         {

&nbsp;           type: 'news',

&nbsp;           country: 'US',

&nbsp;         },

&nbsp;         {

&nbsp;           type: 'x',

&nbsp;           includedXHandles: \['openai', 'deepmind'],

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

\### Sources and Citations

When search is enabled with `returnCitations: true`, the response includes sources that were used to generate the answer:

```ts

const { text, sources } = await generateText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'What are the latest developments in AI?',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'auto',

&nbsp;       returnCitations: true,

&nbsp;     },

&nbsp;   },

&nbsp; },

});



// Access the sources used

for (const source of sources) {

&nbsp; if (source.sourceType === 'url') {

&nbsp;   console.log('Source:', source.url);

&nbsp; }

}

```

\### Streaming with Search

Live Search works with streaming responses. Citations are included when the stream completes:

```ts

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: xai('grok-3-latest'),

&nbsp; prompt: 'What has happened in tech recently?',

&nbsp; providerOptions: {

&nbsp;   xai: {

&nbsp;     searchParameters: {

&nbsp;       mode: 'auto',

&nbsp;       returnCitations: true,

&nbsp;     },

&nbsp;   },

&nbsp; },

});



for await (const textPart of result.textStream) {

&nbsp; process.stdout.write(textPart);

}



console.log('Sources:', await result.sources);

```

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming | Reasoning |

| --------------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `grok-4-fast-non-reasoning` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-4-fast-reasoning` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `grok-code-fast-1` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `grok-4` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-3` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-3-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-3-fast` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-3-fast-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-3-mini` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `grok-3-mini-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `grok-3-mini-fast` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `grok-3-mini-fast-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `grok-2` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-2-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-2-1212` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-2-vision` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-2-vision-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-2-vision-1212` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-beta` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `grok-vision-beta` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[xAI

&nbsp; docs](https://docs.x.ai/docs#models) for a full list of available models. You

&nbsp; can also pass any available provider model ID as a string if needed.

</Note>

\## Image Models

You can create xAI image models using the `.image()` factory method. For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

```ts

import { xai } from '@ai-sdk/xai';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: xai.image('grok-2-image'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

});

```

<Note>

&nbsp; The xAI image model does not currently support the `aspectRatio` or `size`

&nbsp; parameters. Image size defaults to 1024x768.

</Note>

\### Model-specific options

You can customize the image generation behavior with model-specific settings:

```ts

import { xai } from '@ai-sdk/xai';

import { generateImage } from 'ai';



const { images } = await generateImage({

&nbsp; model: xai.image('grok-2-image'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; maxImagesPerCall: 5, // Default is 10

&nbsp; n: 2, // Generate 2 images

});

```

\### Model Capabilities

| Model | Sizes | Notes |

| -------------- | ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| `grok-2-image` | 1024x768 (default) | xAI's text-to-image generation model, designed to create high-quality images from text prompts. It's trained on a diverse dataset and can generate images across various styles, subjects, and settings. |

---

title: Vercel

description: Learn how to use Vercel's v0 models with the AI SDK.

---

\# Vercel Provider

The \[Vercel](https://vercel.com) provider gives you access to the \[v0 API](https://v0.app/docs/api/model), designed for building modern web applications. The v0 models support text and image inputs and provide fast streaming responses.

You can create your Vercel API key at \[v0.dev](https://v0.dev/chat/settings/keys).

<Note>

&nbsp; The v0 API is currently in beta and requires a Premium or Team plan with

&nbsp; usage-based billing enabled. For details, visit the \[pricing

&nbsp; page](https://v0.dev/pricing). To request a higher limit, contact Vercel at

&nbsp; support@v0.dev.

</Note>

\## Features

\- \*\*Framework aware completions\*\*: Evaluated on modern stacks like Next.js and Vercel

\- \*\*Auto-fix\*\*: Identifies and corrects common coding issues during generation

\- \*\*Quick edit\*\*: Streams inline edits as they're available

\- \*\*Multimodal\*\*: Supports both text and image inputs

\## Setup

The Vercel provider is available via the `@ai-sdk/vercel` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/vercel" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/vercel" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/vercel" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/vercel" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `vercel` from `@ai-sdk/vercel`:

```ts
import { vercel } from "@ai-sdk/vercel";
```

If you need a customized setup, you can import `createVercel` from `@ai-sdk/vercel` and create a provider instance with your settings:

```ts

import { createVercel } from '@ai-sdk/vercel';



const vercel = createVercel({

&nbsp; apiKey: process.env.VERCEL\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Vercel provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls. The default prefix is `https://api.v0.dev/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `VERCEL\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create language models using a provider instance. The first argument is the model ID, for example:

```ts

import { vercel } from '@ai-sdk/vercel';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: vercel('v0-1.0-md'),

&nbsp; prompt: 'Create a Next.js AI chatbot',

});

```

Vercel language models can also be used in the `streamText` function (see \[AI SDK Core](/docs/ai-sdk-core)).

\## Models

\### v0-1.5-md

The `v0-1.5-md` model is for everyday tasks and UI generation.

\### v0-1.5-lg

The `v0-1.5-lg` model is for advanced thinking or reasoning.

\### v0-1.0-md (legacy)

The `v0-1.0-md` model is the legacy model served by the v0 API.

All v0 models have the following capabilities:

\- Supports text and image inputs (multimodal)

\- Supports function/tool calls

\- Streaming responses with low latency

\- Optimized for frontend and full-stack web development

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ----------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `v0-1.5-md` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `v0-1.5-lg` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `v0-1.0-md` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

---

title: OpenAI

description: Learn how to use the OpenAI provider for the AI SDK.

---

\# OpenAI Provider

The \[OpenAI](https://openai.com/) provider contains language model support for the OpenAI responses, chat, and completion APIs, as well as embedding model support for the OpenAI embeddings API.

\## Setup

The OpenAI provider is available in the `@ai-sdk/openai` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/openai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/openai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/openai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/openai" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `openai` from `@ai-sdk/openai`:

```ts
import { openai } from "@ai-sdk/openai";
```

If you need a customized setup, you can import `createOpenAI` from `@ai-sdk/openai` and create a provider instance with your settings:

```ts

import { createOpenAI } from '@ai-sdk/openai';



const openai = createOpenAI({

&nbsp; // custom settings, e.g.

&nbsp; headers: {

&nbsp;   'header-name': 'header-value',

&nbsp; },

});

```

You can use the following optional settings to customize the OpenAI provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.openai.com/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `OPENAI\_API\_KEY` environment variable.

\- \*\*name\*\* \_string\_

&nbsp; The provider name. You can set this when using OpenAI compatible providers

&nbsp; to change the model provider property. Defaults to `openai`.

\- \*\*organization\*\* \_string\_

&nbsp; OpenAI Organization.

\- \*\*project\*\* \_string\_

&nbsp; OpenAI project.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

The OpenAI provider instance is a function that you can invoke to create a language model:

```ts
const model = openai("gpt-5");
```

It automatically selects the correct API based on the model id.

You can also pass additional settings in the second argument:

```ts

const model = openai('gpt-5', {

&nbsp; // additional settings

});

```

The available options depend on the API that's automatically chosen for the model (see below).

If you want to explicitly select a specific model API, you can use `.responses`, `.chat`, or `.completion`.

<Note>

&nbsp; Since AI SDK 5, the OpenAI responses API is called by default (unless you

&nbsp; specify e.g. 'openai.chat')

</Note>

\### Example

You can use OpenAI language models to generate text with the `generateText` function:

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

OpenAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

\### Responses Models

You can use the OpenAI responses API with the `openai(modelId)` or `openai.responses(modelId)` factory methods. It is the default API that is used by the OpenAI provider (since AI SDK 5).

```ts
const model = openai("gpt-5");
```

Further configuration can be done using OpenAI provider options.

You can validate the provider options using the `OpenAIResponsesProviderOptions` type.

```ts

import { openai, OpenAIResponsesProviderOptions } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5'), // or openai.responses('gpt-5')

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     parallelToolCalls: false,

&nbsp;     store: false,

&nbsp;     user: 'user\_123',

&nbsp;     // ...

&nbsp;   } satisfies OpenAIResponsesProviderOptions,

&nbsp; },

&nbsp; // ...

});

```

The following provider options are available:

\- \*\*parallelToolCalls\*\* \_boolean\_

&nbsp; Whether to use parallel tool calls. Defaults to `true`.

\- \*\*store\*\* \_boolean\_

&nbsp; Whether to store the generation. Defaults to `true`.

\- \*\*maxToolCalls\*\* \_integer\_

&nbsp; The maximum number of total calls to built-in tools that can be processed in a response.

&nbsp; This maximum number applies across all built-in tool calls, not per individual tool.

&nbsp; Any further attempts to call a tool by the model will be ignored.

\- \*\*metadata\*\* \_Record\&lt;string, string\&gt;\_

&nbsp; Additional metadata to store with the generation.

\- \*\*conversation\*\* \_string\_

&nbsp; The ID of the OpenAI Conversation to continue.

&nbsp; You must create a conversation first via the \[OpenAI API](https://platform.openai.com/docs/api-reference/conversations/create).

&nbsp; Cannot be used in conjunction with `previousResponseId`.

&nbsp; Defaults to `undefined`.

\- \*\*previousResponseId\*\* \_string\_

&nbsp; The ID of the previous response. You can use it to continue a conversation. Defaults to `undefined`.

\- \*\*instructions\*\* \_string\_

&nbsp; Instructions for the model.

&nbsp; They can be used to change the system or developer message when continuing a conversation using the `previousResponseId` option.

&nbsp; Defaults to `undefined`.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Defaults to `undefined`.

\- \*\*reasoningEffort\*\* \_'none' | 'minimal' | 'low' | 'medium' | 'high' | 'xhigh'\_

&nbsp; Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.

<Note>

&nbsp; The 'none' type for `reasoningEffort` is only available for OpenAI's GPT-5.1

&nbsp; models. Also, the 'xhigh' type for `reasoningEffort` is only available for

&nbsp; OpenAI's GPT-5.1-Codex-Max model. Setting `reasoningEffort` to 'none' or

&nbsp; 'xhigh' with unsupported models will result in an error.

</Note>

\- \*\*reasoningSummary\*\* \_'auto' | 'detailed'\_

&nbsp; Controls whether the model returns its reasoning process. Set to `'auto'` for a condensed summary, `'detailed'` for more comprehensive reasoning. Defaults to `undefined` (no reasoning summaries). When enabled, reasoning summaries appear in the stream as events with type `'reasoning'` and in non-streaming responses within the `reasoning` field.

\- \*\*strictJsonSchema\*\* \_boolean\_

&nbsp; Whether to use strict JSON schema validation. Defaults to `true`.

<Note type="warning">

&nbsp; OpenAI structured outputs have several

&nbsp; \[limitations](https://openai.com/index/introducing-structured-outputs-in-the-api),

&nbsp; in particular around the \[supported

&nbsp; schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas),

&nbsp; and are therefore opt-in. For example, optional schema properties are not

&nbsp; supported. You need to change Zod `.nullish()` and `.optional()` to

&nbsp; `.nullable()`.

</Note>

\- \*\*serviceTier\*\* \_'auto' | 'flex' | 'priority' | 'default'\_

&nbsp; Service tier for the request. Set to 'flex' for 50% cheaper processing

&nbsp; at the cost of increased latency (available for o3, o4-mini, and gpt-5 models).

&nbsp; Set to 'priority' for faster processing with Enterprise access (available for gpt-4, gpt-5, gpt-5-mini, o3, o4-mini; gpt-5-nano is not supported).

&nbsp; Defaults to 'auto'.

\- \*\*textVerbosity\*\* \_'low' | 'medium' | 'high'\_

&nbsp; Controls the verbosity of the model's response. Lower values result in more concise responses,

&nbsp; while higher values result in more verbose responses. Defaults to `'medium'`.

\- \*\*include\*\* \_Array\&lt;string\&gt;\_

&nbsp; Specifies additional content to include in the response. Supported values:

&nbsp; `\['file\_search\_call.results']` for including file search results in responses.

&nbsp; `\['message.output\_text.logprobs']` for logprobs.

&nbsp; Defaults to `undefined`.

\- \*\*truncation\*\* \_string\_

&nbsp; The truncation strategy to use for the model response.

&nbsp; - Auto: If the input to this Response exceeds the model's context window size, the model will truncate the response to fit the context window by dropping items from the beginning of the conversation.

&nbsp; - disabled (default): If the input size will exceed the context window size for a model, the request will fail with a 400 error.

\- \*\*promptCacheKey\*\* \_string\_

&nbsp; A cache key for manual prompt caching control. Used by OpenAI to cache responses for similar requests to optimize your cache hit rates.

\- \*\*promptCacheRetention\*\* \_'in_memory' | '24h'\_

&nbsp; The retention policy for the prompt cache. Set to `'24h'` to enable extended prompt caching, which keeps cached prefixes active for up to 24 hours. Defaults to `'in\_memory'` for standard prompt caching. Note: `'24h'` is currently only available for the 5.1 series of models.

\- \*\*safetyIdentifier\*\* \_string\_

&nbsp; A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should be a string that uniquely identifies each user.

\- \*\*systemMessageMode\*\* \_'system' | 'developer' | 'remove'\_

&nbsp; Controls the role of the system message when making requests. By default (when omitted), for models that support reasoning the `system` message is automatically converted to a `developer` message. Setting `systemMessageMode` to `system` passes the system message as a system-level instruction; `developer` passes it as a developer message; `remove` omits the system message from the request.

\- \*\*forceReasoning\*\* \_boolean\_

&nbsp; Force treating this model as a reasoning model. This is useful for "stealth" reasoning models (e.g. via a custom baseURL) where the model ID is not recognized by the SDK's allowlist. When enabled, the SDK applies reasoning-model parameter compatibility rules and defaults `systemMessageMode` to `developer` unless overridden.

The OpenAI responses provider also returns provider-specific metadata:

```ts

const { providerMetadata } = await generateText({

&nbsp; model: openai.responses('gpt-5'),

});



const openaiMetadata = providerMetadata?.openai;

```

The following OpenAI-specific metadata is returned:

\- \*\*responseId\*\* \_string\_

&nbsp; The ID of the response. Can be used to continue a conversation.

\- \*\*cachedPromptTokens\*\* \_number\_

&nbsp; The number of prompt tokens that were a cache hit.

\- \*\*reasoningTokens\*\* \_number\_

&nbsp; The number of reasoning tokens that the model generated.

\#### Reasoning Output

For reasoning models like `gpt-5`, you can enable reasoning summaries to see the model's thought process. Different models support different summarizers—for example, `o4-mini` supports detailed summaries. Set `reasoningSummary: "auto"` to automatically receive the richest level available.

```ts highlight="8-9,16"

import { openai } from '@ai-sdk/openai';

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Tell me about the Mission burrito debate in San Francisco.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     reasoningSummary: 'detailed', // 'auto' for condensed or 'detailed' for comprehensive

&nbsp;   },

&nbsp; },

});



for await (const part of result.fullStream) {

&nbsp; if (part.type === 'reasoning') {

&nbsp;   console.log(`Reasoning: ${part.textDelta}`);

&nbsp; } else if (part.type === 'text-delta') {

&nbsp;   process.stdout.write(part.textDelta);

&nbsp; }

}

```

For non-streaming calls with `generateText`, the reasoning summaries are available in the `reasoning` field of the response:

```ts highlight="8-9,13"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Tell me about the Mission burrito debate in San Francisco.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     reasoningSummary: 'auto',

&nbsp;   },

&nbsp; },

});

console.log('Reasoning:', result.reasoning);

```

Learn more about reasoning summaries in the \[OpenAI documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries).

\#### Verbosity Control

You can control the length and detail of model responses using the `textVerbosity` parameter:

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5-mini'),

&nbsp; prompt: 'Write a poem about a boy and his first pet dog.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     textVerbosity: 'low', // 'low' for concise, 'medium' (default), or 'high' for verbose

&nbsp;   },

&nbsp; },

});

```

The `textVerbosity` parameter scales output length without changing the underlying prompt:

\- `'low'`: Produces terse, minimal responses

\- `'medium'`: Balanced detail (default)

\- `'high'`: Verbose responses with comprehensive detail

\#### Web Search Tool

The OpenAI responses API supports web search through the `openai.tools.webSearch` tool.

```ts

const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'What happened in San Francisco last week?',

&nbsp; tools: {

&nbsp;   web\_search: openai.tools.webSearch({

&nbsp;     // optional configuration:

&nbsp;     externalWebAccess: true,

&nbsp;     searchContextSize: 'high',

&nbsp;     userLocation: {

&nbsp;       type: 'approximate',

&nbsp;       city: 'San Francisco',

&nbsp;       region: 'California',

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; // Force web search tool (optional):

&nbsp; toolChoice: { type: 'tool', toolName: 'web\_search' },

});



// URL sources directly from `results`

const sources = result.sources;



// Or access sources from tool results

for (const toolResult of result.toolResults) {

&nbsp; if (toolResult.toolName === 'web\_search') {

&nbsp;   console.log('Query:', toolResult.output.action.query);

&nbsp;   console.log('Sources:', toolResult.output.sources);

&nbsp;   // `sources` is an array of object: { type: 'url', url: string }

&nbsp; }

}

```

For detailed information on configuration options see the \[OpenAI Web Search Tool documentation](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses).

\#### File Search Tool

The OpenAI responses API supports file search through the `openai.tools.fileSearch` tool.

You can force the use of the file search tool by setting the `toolChoice` parameter to `{ type: 'tool', toolName: 'file\_search' }`.

```ts

const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'What does the document say about user authentication?',

&nbsp; tools: {

&nbsp;   file\_search: openai.tools.fileSearch({

&nbsp;     vectorStoreIds: \['vs\_123'],

&nbsp;     // configuration below is optional:

&nbsp;     maxNumResults: 5,

&nbsp;     filters: {

&nbsp;       key: 'author',

&nbsp;       type: 'eq',

&nbsp;       value: 'Jane Smith',

&nbsp;     },

&nbsp;     ranking: {

&nbsp;       ranker: 'auto',

&nbsp;       scoreThreshold: 0.5,

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     // optional: include results

&nbsp;     include: \['file\_search\_call.results'],

&nbsp;   } satisfies OpenAIResponsesProviderOptions,

&nbsp; },

});

```

\#### Image Generation Tool

OpenAI's Responses API supports multi-modal image generation as a provider-defined tool.

Availability is restricted to specific models (for example, `gpt-5` variants).

You can use the image tool with either `generateText` or `streamText`:

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt:

&nbsp;   'Generate an image of an echidna swimming across the Mozambique channel.',

&nbsp; tools: {

&nbsp;   image\_generation: openai.tools.imageGeneration({ outputFormat: 'webp' }),

&nbsp; },

});



for (const toolResult of result.staticToolResults) {

&nbsp; if (toolResult.toolName === 'image\_generation') {

&nbsp;   const base64Image = toolResult.output.result;

&nbsp; }

}

```

```ts

import { openai } from '@ai-sdk/openai';

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt:

&nbsp;   'Generate an image of an echidna swimming across the Mozambique channel.',

&nbsp; tools: {

&nbsp;   image\_generation: openai.tools.imageGeneration({

&nbsp;     outputFormat: 'webp',

&nbsp;     quality: 'low',

&nbsp;   }),

&nbsp; },

});



for await (const part of result.fullStream) {

&nbsp; if (part.type == 'tool-result' \&\& !part.dynamic) {

&nbsp;   const base64Image = part.output.result;

&nbsp; }

}

```

<Note>

&nbsp; When you set `store: false`, then previously generated images will not be

&nbsp; accessible by the model. We recommend using the image generation tool without

&nbsp; setting `store: false`.

</Note>

For complete details on model availability, image quality controls, supported sizes, and tool-specific parameters,

refer to the OpenAI documentation:

\- Image generation overview and models: \[OpenAI Image Generation](https://platform.openai.com/docs/guides/image-generation)

\- Image generation tool parameters (background, size, quality, format, etc.): \[Image Generation Tool Options](https://platform.openai.com/docs/guides/tools-image-generation#tool-options)

\#### Code Interpreter Tool

The OpenAI responses API supports the code interpreter tool through the `openai.tools.codeInterpreter` tool.

This allows models to write and execute Python code.

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Write and run Python code to calculate the factorial of 10',

&nbsp; tools: {

&nbsp;   code\_interpreter: openai.tools.codeInterpreter({

&nbsp;     // optional configuration:

&nbsp;     container: {

&nbsp;       fileIds: \['file-123', 'file-456'], // optional file IDs to make available

&nbsp;     },

&nbsp;   }),

&nbsp; },

});

```

The code interpreter tool can be configured with:

\- \*\*container\*\*: Either a container ID string or an object with `fileIds` to specify uploaded files that should be available to the code interpreter

<Note>

&nbsp; When working with files generated by the Code Interpreter, reference

&nbsp; information can be obtained from both \[annotations in Text

&nbsp; Parts](#typed-providermetadata-in-text-parts) and \[`providerMetadata` in

&nbsp; Source Document Parts](#typed-providermetadata-in-source-document-parts).

</Note>

\#### MCP Tool

The OpenAI responses API supports connecting to \[Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers through the `openai.tools.mcp` tool. This allows models to call tools exposed by remote MCP servers or service connectors.

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; prompt: 'Search the web for the latest news about AI developments',

&nbsp; tools: {

&nbsp;   mcp: openai.tools.mcp({

&nbsp;     serverLabel: 'web-search',

&nbsp;     serverUrl: 'https://mcp.exa.ai/mcp',

&nbsp;     serverDescription: 'A web-search API for AI agents',

&nbsp;   }),

&nbsp; },

});

```

The MCP tool can be configured with:

\- \*\*serverLabel\*\* \_string\_ (required)

&nbsp; A label to identify the MCP server. This label is used in tool calls to distinguish between multiple MCP servers.

\- \*\*serverUrl\*\* \_string\_ (required if `connectorId` is not provided)

&nbsp; The URL for the MCP server. Either `serverUrl` or `connectorId` must be provided.

\- \*\*connectorId\*\* \_string\_ (required if `serverUrl` is not provided)

&nbsp; Identifier for a service connector. Either `serverUrl` or `connectorId` must be provided.

\- \*\*serverDescription\*\* \_string\_ (optional)

&nbsp; Optional description of the MCP server that helps the model understand its purpose.

\- \*\*allowedTools\*\* \_string\[] | object\_ (optional)

&nbsp; Controls which tools from the MCP server are available. Can be:

&nbsp; - An array of tool names: `\['tool1', 'tool2']`

&nbsp; - An object with filters:

&nbsp; ```ts

&nbsp; {

&nbsp; readOnly: true, // Only allow read-only tools

&nbsp; toolNames: \['tool1', 'tool2'] // Specific tool names

&nbsp; }

&nbsp; ```

\- \*\*authorization\*\* \_string\_ (optional)

&nbsp; OAuth access token for authenticating with the MCP server or connector.

\- \*\*headers\*\* \_Record\&lt;string, string\&gt;\_ (optional)

&nbsp; Optional HTTP headers to include in requests to the MCP server.

\- \*\*requireApproval\*\* \_'always' | 'never' | object\_ (optional)

&nbsp; Controls which MCP tool calls require user approval before execution. Can be:

&nbsp; - `'always'`: All MCP tool calls require approval

&nbsp; - `'never'`: No MCP tool calls require approval (default)

&nbsp; - An object with filters:

&nbsp; ```ts

&nbsp; {

&nbsp; never: {

&nbsp; toolNames: \['safe_tool', 'another_safe_tool']; // Skip approval for these tools

&nbsp; }

&nbsp; }

&nbsp; ```

&nbsp; When approval is required, the model will return a `tool-approval-request` content part that you can use to prompt the user for approval. See \[Human in the Loop](/cookbook/next/human-in-the-loop) for more details on implementing approval workflows.

<Note>

&nbsp; When `requireApproval` is not set, tool calls are approved by default. Be sure

&nbsp; to connect to only trusted MCP servers, who you trust to share your data with.

</Note>

<Note>

&nbsp; The OpenAI MCP tool is different from the general MCP client approach

&nbsp; documented in \[MCP Tools](/docs/ai-sdk-core/mcp-tools). The OpenAI MCP tool is

&nbsp; a built-in provider-defined tool that allows OpenAI models to directly connect

&nbsp; to MCP servers, while the general MCP client requires you to convert MCP tools

&nbsp; to AI SDK tools first.

</Note>

\#### Local Shell Tool

The OpenAI responses API support the local shell tool for Codex models through the `openai.tools.localShell` tool.

Local shell is a tool that allows agents to run shell commands locally on a machine you or the user provides.

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.responses('gpt-5-codex'),

&nbsp; tools: {

&nbsp;   local\_shell: openai.tools.localShell({

&nbsp;     execute: async ({ action }) => {

&nbsp;       // ... your implementation, e.g. sandbox access ...

&nbsp;       return { output: stdout };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'List the files in my home directory.',

&nbsp; stopWhen: stepCountIs(2),

});

```

\#### Shell Tool

The OpenAI Responses API supports the shell tool for GPT-5.1 models through the `openai.tools.shell` tool.

The shell tool allows allows running bash commands and interacting with a command line.

The model proposes shell commands; your integration executes them and returns the outputs.

<Note type="warning">

&nbsp; Running arbitrary shell commands can be dangerous. Always sandbox execution or

&nbsp; add strict allow-/deny-lists before forwarding a command to the system shell.

</Note>

```ts

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';

import { exec } from 'child\_process';

import { promisify } from 'util';



const execAsync = promisify(exec);



const result = await generateText({

&nbsp; model: openai('gpt-5.1'),

&nbsp; tools: {

&nbsp;   shell: openai.tools.shell({

&nbsp;     execute: async ({ action }) => {

&nbsp;       // ... your implementation, e.g. sandbox access ...

&nbsp;       return { output: results };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'List the files in the current directory and show disk usage.',

});

```

Your execute function must return an output array with results for each command:

\- \*\*stdout\*\* \_string\_ - Standard output from the command

\- \*\*stderr\*\* \_string\_ - Standard error from the command

\- \*\*outcome\*\* - Either `{ type: 'timeout' }` or `{ type: 'exit', exitCode: number }`

\#### Apply Patch Tool

The OpenAI Responses API supports the apply patch tool for GPT-5.1 models through the `openai.tools.applyPatch` tool.

The apply patch tool lets the model create, update, and delete files in your codebase using structured diffs.

Instead of just suggesting edits, the model emits patch operations that your application applies and reports back on,

enabling iterative, multi-step code editing workflows.

```ts

import { openai } from '@ai-sdk/openai';

import { generateText, stepCountIs } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-5.1'),

&nbsp; tools: {

&nbsp;   apply\_patch: openai.tools.applyPatch({

&nbsp;     execute: async ({ callId, operation }) => {

&nbsp;       // ... your implementation for applying the diffs.

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'Create a python file that calculates the factorial of a number',

&nbsp; stopWhen: stepCountIs(5),

});

```

Your execute function must return:

\- \*\*status\*\* \_'completed' | 'failed'\_ - Whether the patch was applied successfully

\- \*\*output\*\* \_string\_ (optional) - Human-readable log text (e.g., results or error messages)

\#### Image Inputs

The OpenAI Responses API supports Image inputs for appropriate models.

You can pass Image files as part of the message content using the 'image' type:

```ts

const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'Please describe the image.',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'image',

&nbsp;         image: readFileSync('./data/image.png'),

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

The model will have access to the image and will respond to questions about it.

The image should be passed using the `image` field.

You can also pass a file-id from the OpenAI Files API.

```ts

{

&nbsp; type: 'image',

&nbsp; image: 'file-8EFBcWHsQxZV7YGezBC1fq'

}

```

You can also pass the URL of an image.

```ts

{

&nbsp; type: 'image',

&nbsp; image: 'https://sample.edu/image.png',

}

```

\#### PDF Inputs

The OpenAI Responses API supports reading PDF files.

You can pass PDF files as part of the message content using the `file` type:

```ts

const result = await generateText({

&nbsp; model: openai('gpt-5'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;         filename: 'ai.pdf', // optional

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

You can also pass a file-id from the OpenAI Files API.

```ts

{

&nbsp; type: 'file',

&nbsp; data: 'file-8EFBcWHsQxZV7YGezBC1fq',

&nbsp; mediaType: 'application/pdf',

}

```

You can also pass the URL of a pdf.

```ts

{

&nbsp; type: 'file',

&nbsp; data: 'https://sample.edu/example.pdf',

&nbsp; mediaType: 'application/pdf',

&nbsp; filename: 'ai.pdf', // optional

}

```

The model will have access to the contents of the PDF file and

respond to questions about it.

The PDF file should be passed using the `data` field,

and the `mediaType` should be set to `'application/pdf'`.

\#### Structured Outputs

The OpenAI Responses API supports structured outputs. You can enforce structured outputs using `generateObject` or `streamObject`, which expose a `schema` option. Additionally, you can pass a Zod or JSON Schema object to the `output` option when using `generateText` or `streamText`.

```ts

// Using generateObject

const result = await generateObject({

&nbsp; model: openai('gpt-4.1'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(

&nbsp;       z.object({

&nbsp;         name: z.string(),

&nbsp;         amount: z.string(),

&nbsp;       }),

&nbsp;     ),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});



// Using generateText

const result = await generateText({

&nbsp; model: openai('gpt-4.1'),

&nbsp; prompt: 'How do I make a pizza?',

&nbsp; output: Output.object({

&nbsp;   schema: z.object({

&nbsp;     ingredients: z.array(z.string()),

&nbsp;     steps: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

});

```

\#### Typed providerMetadata in Text Parts

When using the OpenAI Responses API, the SDK attaches OpenAI-specific metadata to output parts via `providerMetadata`.

This metadata can be used on the client side for tasks such as rendering citations or downloading files generated by the Code Interpreter.

To enable type-safe handling of this metadata, the AI SDK exports dedicated TypeScript types.

For text parts, when `part.type === 'text'`, the `providerMetadata` is provided in the form of `OpenaiResponsesTextProviderMetadata`.

This metadata includes the following fields:

\- `itemId`

&nbsp; The ID of the output item in the Responses API.

\- `annotations` (optional)

&nbsp; An array of annotation objects generated by the model.

&nbsp; If no annotations are present, this property itself may be omitted (`undefined`).

&nbsp; Each element in `annotations` is a discriminated union with a required `type` field. Supported types include, for example:

&nbsp; - `url\_citation`

&nbsp; - `file\_citation`

&nbsp; - `container\_file\_citation`

&nbsp; - `file\_path`

&nbsp; These annotations directly correspond to the annotation objects defined by the Responses API and can be used for inline reference rendering or output analysis.

&nbsp; For details, see the official OpenAI documentation:

&nbsp; \[Responses API – output text annotations](https://platform.openai.com/docs/api-reference/responses/object?lang=javascript#responses-object-output-output\_message-content-output\_text-annotations).

```ts

import {

&nbsp; openai,

&nbsp; type OpenaiResponsesTextProviderMetadata,

} from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-4.1-mini'),

&nbsp; prompt:

&nbsp;   'Create a program that generates five random numbers between 1 and 100 with two decimal places, and show me the execution results. Also save the result to a file.',

&nbsp; tools: {

&nbsp;   code\_interpreter: openai.tools.codeInterpreter(),

&nbsp;   web\_search: openai.tools.webSearch(),

&nbsp;   file\_search: openai.tools.fileSearch({ vectorStoreIds: \['vs\_1234'] }), // requires a configured vector store

&nbsp; },

});



for (const part of result.content) {

&nbsp; if (part.type === 'text') {

&nbsp;   const providerMetadata = part.providerMetadata as

&nbsp;     | OpenaiResponsesTextProviderMetadata

&nbsp;     | undefined;

&nbsp;   if (!providerMetadata) continue;

&nbsp;   const { itemId: \_itemId, annotations } = providerMetadata.openai;



&nbsp;   if (!annotations) continue;

&nbsp;   for (const annotation of annotations) {

&nbsp;     switch (annotation.type) {

&nbsp;       case 'url\_citation':

&nbsp;         // url\_citation is returned from web\_search and provides:

&nbsp;         // properties: type, url, title, start\_index and end\_index

&nbsp;         break;

&nbsp;       case 'file\_citation':

&nbsp;         // file\_citation is returned from file\_search and provides:

&nbsp;         // properties: type, file\_id, filename and index

&nbsp;         break;

&nbsp;       case 'container\_file\_citation':

&nbsp;         // container\_file\_citation is returned from code\_interpreter and provides:

&nbsp;         // properties: type, container\_id, file\_id, filename, start\_index and end\_index

&nbsp;         break;

&nbsp;       case 'file\_path':

&nbsp;         // file\_path provides:

&nbsp;         // properties: type, file\_id and index

&nbsp;         break;

&nbsp;       default: {

&nbsp;         const \_exhaustiveCheck: never = annotation;

&nbsp;         throw new Error(

&nbsp;           `Unhandled annotation: ${JSON.stringify(\_exhaustiveCheck)}`,

&nbsp;         );

&nbsp;       }

&nbsp;     }

&nbsp;   }

&nbsp; }

}

```

<Note>

&nbsp; When implementing file downloads for files generated by the Code Interpreter,

&nbsp; the `container\_id` and `file\_id` available in `providerMetadata` can be used

&nbsp; to retrieve the file content. For details, see the \[Retrieve container file

&nbsp; content](https://platform.openai.com/docs/api-reference/container-files/retrieveContainerFileContent)

&nbsp; API.

</Note>

\#### Typed providerMetadata in Source Document Parts

For source document parts, when `part.type === 'source'` and `sourceType === 'document'`, the `providerMetadata` is provided as `OpenaiResponsesSourceDocumentProviderMetadata`.

This metadata is also a discriminated union with a required `type` field. Supported types include:

\- `file\_citation`

\- `container\_file\_citation`

\- `file\_path`

Each type includes the identifiers required to work with the referenced resource, such as `fileId` and `containerId`.

```ts

import {

&nbsp; openai,

&nbsp; type OpenaiResponsesSourceDocumentProviderMetadata,

} from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai('gpt-4.1-mini'),

&nbsp; prompt:

&nbsp;   'Create a program that generates five random numbers between 1 and 100 with two decimal places, and show me the execution results. Also save the result to a file.',

&nbsp; tools: {

&nbsp;   code\_interpreter: openai.tools.codeInterpreter(),

&nbsp;   web\_search: openai.tools.webSearch(),

&nbsp;   file\_search: openai.tools.fileSearch({ vectorStoreIds: \['vs\_1234'] }), // requires a configured vector store

&nbsp; },

});



for (const part of result.content) {

&nbsp; if (part.type === 'source') {

&nbsp;   if (part.sourceType === 'document') {

&nbsp;     const providerMetadata = part.providerMetadata as

&nbsp;       | OpenaiResponsesSourceDocumentProviderMetadata

&nbsp;       | undefined;

&nbsp;     if (!providerMetadata) continue;

&nbsp;     const annotation = providerMetadata.openai;

&nbsp;     switch (annotation.type) {

&nbsp;       case 'file\_citation':

&nbsp;         // file\_citation is returned from file\_search and provides:

&nbsp;         // properties: type, fileId and index

&nbsp;         // The filename can be accessed via part.filename.

&nbsp;         break;

&nbsp;       case 'container\_file\_citation':

&nbsp;         // container\_file\_citation is returned from code\_interpreter and provides:

&nbsp;         // properties: type, containerId and fileId

&nbsp;         // The filename can be accessed via part.filename.

&nbsp;         break;

&nbsp;       case 'file\_path':

&nbsp;         // file\_path provides:

&nbsp;         // properties: type, fileId and index

&nbsp;         break;

&nbsp;       default: {

&nbsp;         const \_exhaustiveCheck: never = annotation;

&nbsp;         throw new Error(

&nbsp;           `Unhandled annotation: ${JSON.stringify(\_exhaustiveCheck)}`,

&nbsp;         );

&nbsp;       }

&nbsp;     }

&nbsp;   }

&nbsp; }

}

```

<Note>

&nbsp; Annotations in text parts follow the OpenAI Responses API specification and

&nbsp; therefore use snake_case properties (e.g. `file\_id`, `container\_id`). In

&nbsp; contrast, `providerMetadata` for source document parts is normalized by the

&nbsp; SDK to camelCase (e.g. `fileId`, `containerId`). Fields that depend on the

&nbsp; original text content, such as `start\_index` and `end\_index`, are omitted, as

&nbsp; are fields like `filename` that are directly available on the source object.

</Note>

\### Chat Models

You can create models that call the \[OpenAI chat API](https://platform.openai.com/docs/api-reference/chat) using the `.chat()` factory method.

The first argument is the model id, e.g. `gpt-4`.

The OpenAI chat models support tool calls and some have multi-modal capabilities.

```ts
const model = openai.chat("gpt-5");
```

OpenAI chat models support also some model specific provider options that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them in the `providerOptions` argument:

```ts

import { openai, type OpenAIChatLanguageModelOptions } from '@ai-sdk/openai';



const model = openai.chat('gpt-5');



await generateText({

&nbsp; model,

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     logitBias: {

&nbsp;       // optional likelihood for specific tokens

&nbsp;       '50256': -100,

&nbsp;     },

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;   } satisfies OpenAIChatLanguageModelOptions,

&nbsp; },

});

```

The following optional provider options are available for OpenAI chat models:

\- \*\*logitBias\*\* \_Record\&lt;number, number\&gt;\_

&nbsp; Modifies the likelihood of specified tokens appearing in the completion.

&nbsp; Accepts a JSON object that maps tokens (specified by their token ID in

&nbsp; the GPT tokenizer) to an associated bias value from -100 to 100. You

&nbsp; can use this tokenizer tool to convert text to token IDs. Mathematically,

&nbsp; the bias is added to the logits generated by the model prior to sampling.

&nbsp; The exact effect will vary per model, but values between -1 and 1 should

&nbsp; decrease or increase likelihood of selection; values like -100 or 100

&nbsp; should result in a ban or exclusive selection of the relevant token.

&nbsp; As an example, you can pass `{"50256": -100}` to prevent the token from being generated.

\- \*\*logprobs\*\* \_boolean | number\_

&nbsp; Return the log probabilities of the tokens. Including logprobs will increase

&nbsp; the response size and can slow down response times. However, it can

&nbsp; be useful to better understand how the model is behaving.

&nbsp; Setting to true will return the log probabilities of the tokens that

&nbsp; were generated.

&nbsp; Setting to a number will return the log probabilities of the top n

&nbsp; tokens that were generated.

\- \*\*parallelToolCalls\*\* \_boolean\_

&nbsp; Whether to enable parallel function calling during tool use. Defaults to `true`.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to

&nbsp; monitor and detect abuse. \[Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).

\- \*\*reasoningEffort\*\* \_'minimal' | 'low' | 'medium' | 'high' | 'xhigh'\_

&nbsp; Reasoning effort for reasoning models. Defaults to `medium`. If you use

&nbsp; `providerOptions` to set the `reasoningEffort` option, this

&nbsp; model setting will be ignored.

\- \*\*maxCompletionTokens\*\* \_number\_

&nbsp; Maximum number of completion tokens to generate. Useful for reasoning models.

\- \*\*store\*\* \_boolean\_

&nbsp; Whether to enable persistence in Responses API.

\- \*\*metadata\*\* \_Record\&lt;string, string\&gt;\_

&nbsp; Metadata to associate with the request.

\- \*\*prediction\*\* \_Record\&lt;string, any\&gt;\_

&nbsp; Parameters for prediction mode.

\- \*\*serviceTier\*\* \_'auto' | 'flex' | 'priority' | 'default'\_

&nbsp; Service tier for the request. Set to 'flex' for 50% cheaper processing

&nbsp; at the cost of increased latency (available for o3, o4-mini, and gpt-5 models).

&nbsp; Set to 'priority' for faster processing with Enterprise access (available for gpt-4, gpt-5, gpt-5-mini, o3, o4-mini; gpt-5-nano is not supported).

&nbsp; Defaults to 'auto'.

\- \*\*strictJsonSchema\*\* \_boolean\_

&nbsp; Whether to use strict JSON schema validation.

&nbsp; Defaults to `true`.

\- \*\*textVerbosity\*\* \_'low' | 'medium' | 'high'\_

&nbsp; Controls the verbosity of the model's responses. Lower values will result in more concise responses, while higher values will result in more verbose responses.

\- \*\*promptCacheKey\*\* \_string\_

&nbsp; A cache key for manual prompt caching control. Used by OpenAI to cache responses for similar requests to optimize your cache hit rates.

\- \*\*promptCacheRetention\*\* \_'in_memory' | '24h'\_

&nbsp; The retention policy for the prompt cache. Set to `'24h'` to enable extended prompt caching, which keeps cached prefixes active for up to 24 hours. Defaults to `'in\_memory'` for standard prompt caching. Note: `'24h'` is currently only available for the 5.1 series of models.

\- \*\*safetyIdentifier\*\* \_string\_

&nbsp; A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should be a string that uniquely identifies each user.

\#### Reasoning

OpenAI has introduced the `o1`,`o3`, and `o4` series of \[reasoning models](https://platform.openai.com/docs/guides/reasoning).

Currently, `o4-mini`, `o3`, `o3-mini`, and `o1` are available via both the chat and responses APIs. The

models `codex-mini-latest` and `computer-use-preview` are available only via the \[responses API](#responses-models).

Reasoning models currently only generate text, have several limitations, and are only supported using `generateText` and `streamText`.

They support additional settings and response metadata:

\- You can use `providerOptions` to set

&nbsp; - the `reasoningEffort` option (or alternatively the `reasoningEffort` model setting), which determines the amount of reasoning the model performs.

\- You can use response `providerMetadata` to access the number of reasoning tokens that the model generated.

```ts highlight="4,7-11,17"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const { text, usage, providerMetadata } = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; prompt: 'Invent a new holiday and describe its traditions.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     reasoningEffort: 'low',

&nbsp;   },

&nbsp; },

});



console.log(text);

console.log('Usage:', {

&nbsp; ...usage,

&nbsp; reasoningTokens: providerMetadata?.openai?.reasoningTokens,

});

```

<Note>

&nbsp; System messages are automatically converted to OpenAI developer messages for

&nbsp; reasoning models when supported.

</Note>

\- You can control how system messages are handled by providerOptions `systemMessageMode`:

&nbsp; - `developer`: treat the prompt as a developer message (default for reasoning models).

&nbsp; - `system`: keep the system message as a system-level instruction.

&nbsp; - `remove`: remove the system message from the messages.

```ts highlight="12"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; messages: \[

&nbsp;   { role: 'system', content: 'You are a helpful assistant.' },

&nbsp;   { role: 'user', content: 'Tell me a joke.' },

&nbsp; ],

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     systemMessageMode: 'system',

&nbsp;   },

&nbsp; },

});

```

<Note>

&nbsp; Reasoning models require additional runtime inference to complete their

&nbsp; reasoning phase before generating a response. This introduces longer latency

&nbsp; compared to other models.

</Note>

<Note>

&nbsp; `maxOutputTokens` is automatically mapped to `max\_completion\_tokens` for

&nbsp; reasoning models.

</Note>

\#### Strict Structured Outputs

Strict structured outputs are enabled by default.

You can disable them by setting the `strictJsonSchema` option to `false`.

```ts highlight="7"

import { openai, OpenAIChatLanguageModelOptions } from '@ai-sdk/openai';

import { generateObject } from 'ai';

import { z } from 'zod';



const result = await generateObject({

&nbsp; model: openai.chat('gpt-4o-2024-08-06'),

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     strictJsonSchema: false,

&nbsp;   } satisfies OpenAIChatLanguageModelOptions,

&nbsp; },

&nbsp; schemaName: 'recipe',

&nbsp; schemaDescription: 'A recipe for lasagna.',

&nbsp; schema: z.object({

&nbsp;   name: z.string(),

&nbsp;   ingredients: z.array(

&nbsp;     z.object({

&nbsp;       name: z.string(),

&nbsp;       amount: z.string(),

&nbsp;     }),

&nbsp;   ),

&nbsp;   steps: z.array(z.string()),

&nbsp; }),

&nbsp; prompt: 'Generate a lasagna recipe.',

});



console.log(JSON.stringify(result.object, null, 2));

```

<Note type="warning">

&nbsp; OpenAI structured outputs have several

&nbsp; \[limitations](https://openai.com/index/introducing-structured-outputs-in-the-api),

&nbsp; in particular around the \[supported schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas),

&nbsp; and are therefore opt-in.

For example, optional schema properties are not supported.

You need to change Zod `.nullish()` and `.optional()` to `.nullable()`.

</Note>

\#### Logprobs

OpenAI provides logprobs information for completion/chat models.

You can access it in the `providerMetadata` object.

```ts highlight="11"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     // this can also be a number,

&nbsp;     // refer to logprobs provider options section for more

&nbsp;     logprobs: true,

&nbsp;   },

&nbsp; },

});



const openaiMetadata = (await result.providerMetadata)?.openai;



const logprobs = openaiMetadata?.logprobs;

```

\#### Image Support

The OpenAI Chat API supports Image inputs for appropriate models.

You can pass Image files as part of the message content using the 'image' type:

```ts

const result = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'Please describe the image.',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'image',

&nbsp;         image: readFileSync('./data/image.png'),

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

The model will have access to the image and will respond to questions about it.

The image should be passed using the `image` field.

You can also pass the URL of an image.

```ts

{

&nbsp; type: 'image',

&nbsp; image: 'https://sample.edu/image.png',

}

```

\#### PDF support

The OpenAI Chat API supports reading PDF files.

You can pass PDF files as part of the message content using the `file` type:

```ts

const result = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;         filename: 'ai.pdf', // optional

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

The model will have access to the contents of the PDF file and

respond to questions about it.

The PDF file should be passed using the `data` field,

and the `mediaType` should be set to `'application/pdf'`.

You can also pass a file-id from the OpenAI Files API.

```ts

{

&nbsp; type: 'file',

&nbsp; data: 'file-8EFBcWHsQxZV7YGezBC1fq',

&nbsp; mediaType: 'application/pdf',

}

```

You can also pass the URL of a PDF.

```ts

{

&nbsp; type: 'file',

&nbsp; data: 'https://sample.edu/example.pdf',

&nbsp; mediaType: 'application/pdf',

&nbsp; filename: 'ai.pdf', // optional

}

```

\#### Predicted Outputs

OpenAI supports \[predicted outputs](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs) for `gpt-4o` and `gpt-4o-mini`.

Predicted outputs help you reduce latency by allowing you to specify a base text that the model should modify.

You can enable predicted outputs by adding the `prediction` option to the `providerOptions.openai` object:

```ts highlight="15-18"

const result = streamText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: 'Replace the Username property with an Email property.',

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: existingCode,

&nbsp;   },

&nbsp; ],

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     prediction: {

&nbsp;       type: 'content',

&nbsp;       content: existingCode,

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

OpenAI provides usage information for predicted outputs (`acceptedPredictionTokens` and `rejectedPredictionTokens`).

You can access it in the `providerMetadata` object.

```ts highlight="11"
const openaiMetadata = (await result.providerMetadata)?.openai;

const acceptedPredictionTokens = openaiMetadata?.acceptedPredictionTokens;

const rejectedPredictionTokens = openaiMetadata?.rejectedPredictionTokens;
```

<Note type="warning">

&nbsp; OpenAI Predicted Outputs have several

&nbsp; \[limitations](https://platform.openai.com/docs/guides/predicted-outputs#limitations),

&nbsp; e.g. unsupported API parameters and no tool calling support.

</Note>

\#### Image Detail

You can use the `openai` provider option to set the \[image input detail](https://platform.openai.com/docs/guides/images-vision?api-mode=responses#specify-image-input-detail-level) to `high`, `low`, or `auto`:

```ts highlight="13-16"

const result = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'Describe the image in detail.' },

&nbsp;       {

&nbsp;         type: 'image',

&nbsp;         image:

&nbsp;           'https://github.com/vercel/ai/blob/main/examples/ai-functions/data/comic-cat.png?raw=true',



&nbsp;         // OpenAI specific options - image detail:

&nbsp;         providerOptions: {

&nbsp;           openai: { imageDetail: 'low' },

&nbsp;         },

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

<Note type="warning">

&nbsp; Because the `UIMessage` type (used by AI SDK UI hooks like `useChat`) does not

&nbsp; support the `providerOptions` property, you can use `convertToModelMessages`

&nbsp; first before passing the messages to functions like `generateText` or

&nbsp; `streamText`. For more details on `providerOptions` usage, see

&nbsp; \[here](/docs/foundations/prompts#provider-options).

</Note>

\#### Distillation

OpenAI supports model distillation for some models.

If you want to store a generation for use in the distillation process, you can add the `store` option to the `providerOptions.openai` object.

This will save the generation to the OpenAI platform for later use in distillation.

```typescript highlight="9-16"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';

import 'dotenv/config';



async function main() {

&nbsp; const { text, usage } = await generateText({

&nbsp;   model: openai.chat('gpt-4o-mini'),

&nbsp;   prompt: 'Who worked on the original macintosh?',

&nbsp;   providerOptions: {

&nbsp;     openai: {

&nbsp;       store: true,

&nbsp;       metadata: {

&nbsp;         custom: 'value',

&nbsp;       },

&nbsp;     },

&nbsp;   },

&nbsp; });



&nbsp; console.log(text);

&nbsp; console.log();

&nbsp; console.log('Usage:', usage);

}



main().catch(console.error);

```

\#### Prompt Caching

OpenAI has introduced \[Prompt Caching](https://platform.openai.com/docs/guides/prompt-caching) for supported models

including `gpt-4o` and `gpt-4o-mini`.

\- Prompt caching is automatically enabled for these models, when the prompt is 1024 tokens or longer. It does

&nbsp; not need to be explicitly enabled.

\- You can use response `providerMetadata` to access the number of prompt tokens that were a cache hit.

\- Note that caching behavior is dependent on load on OpenAI's infrastructure. Prompt prefixes generally remain in the

&nbsp; cache following 5-10 minutes of inactivity before they are evicted, but during off-peak periods they may persist for up

&nbsp; to an hour.

```ts highlight="11"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const { text, usage, providerMetadata } = await generateText({

&nbsp; model: openai.chat('gpt-4o-mini'),

&nbsp; prompt: `A 1024-token or longer prompt...`,

});



console.log(`usage:`, {

&nbsp; ...usage,

&nbsp; cachedPromptTokens: providerMetadata?.openai?.cachedPromptTokens,

});

```

To improve cache hit rates, you can manually control caching using the `promptCacheKey` option:

```ts highlight="7-11"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const { text, usage, providerMetadata } = await generateText({

&nbsp; model: openai.chat('gpt-5'),

&nbsp; prompt: `A 1024-token or longer prompt...`,

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     promptCacheKey: 'my-custom-cache-key-123',

&nbsp;   },

&nbsp; },

});



console.log(`usage:`, {

&nbsp; ...usage,

&nbsp; cachedPromptTokens: providerMetadata?.openai?.cachedPromptTokens,

});

```

For GPT-5.1 models, you can enable extended prompt caching that keeps cached prefixes active for up to 24 hours:

```ts highlight="7-12"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const { text, usage, providerMetadata } = await generateText({

&nbsp; model: openai.chat('gpt-5.1'),

&nbsp; prompt: `A 1024-token or longer prompt...`,

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     promptCacheKey: 'my-custom-cache-key-123',

&nbsp;     promptCacheRetention: '24h', // Extended caching for GPT-5.1

&nbsp;   },

&nbsp; },

});



console.log(`usage:`, {

&nbsp; ...usage,

&nbsp; cachedPromptTokens: providerMetadata?.openai?.cachedPromptTokens,

});

```

\#### Audio Input

With the `gpt-4o-audio-preview` model, you can pass audio files to the model.

<Note type="warning">

&nbsp; The `gpt-4o-audio-preview` model is currently in preview and requires at least

&nbsp; some audio inputs. It will not work with non-audio data.

</Note>

```ts highlight="12-14"

import { openai } from '@ai-sdk/openai';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: openai.chat('gpt-4o-audio-preview'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'What is the audio saying?' },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         mediaType: 'audio/mpeg',

&nbsp;         data: readFileSync('./data/galileo.mp3'),

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

\### Completion Models

You can create models that call the \[OpenAI completions API](https://platform.openai.com/docs/api-reference/completions) using the `.completion()` factory method.

The first argument is the model id.

Currently only `gpt-3.5-turbo-instruct` is supported.

```ts
const model = openai.completion("gpt-3.5-turbo-instruct");
```

OpenAI completion models support also some model specific settings that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them as an options argument:

```ts

const model = openai.completion('gpt-3.5-turbo-instruct');



await model.doGenerate({

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     echo: true, // optional, echo the prompt in addition to the completion

&nbsp;     logitBias: {

&nbsp;       // optional likelihood for specific tokens

&nbsp;       '50256': -100,

&nbsp;     },

&nbsp;     suffix: 'some text', // optional suffix that comes after a completion of inserted text

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for OpenAI completion models:

\- \*\*echo\*\*: \_boolean\_

&nbsp; Echo back the prompt in addition to the completion.

\- \*\*logitBias\*\* \_Record\&lt;number, number\&gt;\_

&nbsp; Modifies the likelihood of specified tokens appearing in the completion.

&nbsp; Accepts a JSON object that maps tokens (specified by their token ID in

&nbsp; the GPT tokenizer) to an associated bias value from -100 to 100. You

&nbsp; can use this tokenizer tool to convert text to token IDs. Mathematically,

&nbsp; the bias is added to the logits generated by the model prior to sampling.

&nbsp; The exact effect will vary per model, but values between -1 and 1 should

&nbsp; decrease or increase likelihood of selection; values like -100 or 100

&nbsp; should result in a ban or exclusive selection of the relevant token.

&nbsp; As an example, you can pass `{"50256": -100}` to prevent the \&lt;|endoftext|\&gt;

&nbsp; token from being generated.

\- \*\*logprobs\*\* \_boolean | number\_

&nbsp; Return the log probabilities of the tokens. Including logprobs will increase

&nbsp; the response size and can slow down response times. However, it can

&nbsp; be useful to better understand how the model is behaving.

&nbsp; Setting to true will return the log probabilities of the tokens that

&nbsp; were generated.

&nbsp; Setting to a number will return the log probabilities of the top n

&nbsp; tokens that were generated.

\- \*\*suffix\*\* \_string\_

&nbsp; The suffix that comes after a completion of inserted text.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to

&nbsp; monitor and detect abuse. \[Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).

\### Model Capabilities

| Model | Image Input | Audio Input | Object Generation | Tool Usage |

| --------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `gpt-5.2-pro` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5.2-chat-latest` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5.2` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5.1-codex-mini` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5.1-codex` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5.1-chat-latest` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5.1` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5-pro` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5-mini` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5-nano` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5-codex` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-5-chat-latest` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `gpt-4.1` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-4.1-mini` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-4.1-nano` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-4o` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-4o-mini` | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[OpenAI

&nbsp; docs](https://platform.openai.com/docs/models) for a full list of available

&nbsp; models. The table above lists popular models. You can also pass any available

&nbsp; provider model ID as a string if needed.

</Note>

\## Embedding Models

You can create models that call the \[OpenAI embeddings API](https://platform.openai.com/docs/api-reference/embeddings)

using the `.embedding()` factory method.

```ts
const model = openai.embedding("text-embedding-3-large");
```

OpenAI embedding models support several additional provider options.

You can pass them as an options argument:

```ts

import { openai } from '@ai-sdk/openai';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: openai.embedding('text-embedding-3-large'),

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     dimensions: 512, // optional, number of dimensions for the embedding

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for OpenAI embedding models:

\- \*\*dimensions\*\*: \_number\_

&nbsp; The number of dimensions the resulting output embeddings should have.

&nbsp; Only supported in text-embedding-3 and later models.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to

&nbsp; monitor and detect abuse. \[Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).

\### Model Capabilities

| Model | Default Dimensions | Custom Dimensions |

| ------------------------ | ------------------ | ------------------- |

| `text-embedding-3-large` | 3072 | <Check size={18} /> |

| `text-embedding-3-small` | 1536 | <Check size={18} /> |

| `text-embedding-ada-002` | 1536 | <Cross size={18} /> |

\## Image Models

You can create models that call the \[OpenAI image generation API](https://platform.openai.com/docs/api-reference/images)

using the `.image()` factory method.

```ts
const model = openai.image("dall-e-3");
```

<Note>

&nbsp; Dall-E models do not support the `aspectRatio` parameter. Use the `size`

&nbsp; parameter instead.

</Note>

\### Image Editing

OpenAI's `gpt-image-1` model supports powerful image editing capabilities. Pass input images via `prompt.images` to transform, combine, or edit existing images.

\#### Basic Image Editing

Transform an existing image using text prompts:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: openai.image('gpt-image-1'),

&nbsp; prompt: {

&nbsp;   text: 'Turn the cat into a dog but retain the style of the original image',

&nbsp;   images: \[imageBuffer],

&nbsp; },

});

```

\#### Inpainting with Mask

Edit specific parts of an image using a mask. Transparent areas in the mask indicate where the image should be edited:

```ts

const image = readFileSync('./input-image.png');

const mask = readFileSync('./mask.png'); // Transparent areas = edit regions



const { images } = await generateImage({

&nbsp; model: openai.image('gpt-image-1'),

&nbsp; prompt: {

&nbsp;   text: 'A sunlit indoor lounge area with a pool containing a flamingo',

&nbsp;   images: \[image],

&nbsp;   mask: mask,

&nbsp; },

});

```

\#### Background Removal

Remove the background from an image by setting `background` to `transparent`:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: openai.image('gpt-image-1'),

&nbsp; prompt: {

&nbsp;   text: 'do not change anything',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     background: 'transparent',

&nbsp;     output\_format: 'png',

&nbsp;   },

&nbsp; },

});

```

\#### Multi-Image Combining

Combine multiple reference images into a single output. `gpt-image-1` supports up to 16 input images:

```ts

const cat = readFileSync('./cat.png');

const dog = readFileSync('./dog.png');

const owl = readFileSync('./owl.png');

const bear = readFileSync('./bear.png');



const { images } = await generateImage({

&nbsp; model: openai.image('gpt-image-1'),

&nbsp; prompt: {

&nbsp;   text: 'Combine these animals into a group photo, retaining the original style',

&nbsp;   images: \[cat, dog, owl, bear],

&nbsp; },

});

```

<Note>

&nbsp; Input images can be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`, or

&nbsp; base64-encoded strings. For `gpt-image-1`, each image should be a `png`,

&nbsp; `webp`, or `jpg` file less than 50MB.

</Note>

\### Model Capabilities

| Model | Sizes |

| ------------------ | ------------------------------- |

| `gpt-image-1.5` | 1024x1024, 1536x1024, 1024x1536 |

| `gpt-image-1-mini` | 1024x1024, 1536x1024, 1024x1536 |

| `gpt-image-1` | 1024x1024, 1536x1024, 1024x1536 |

| `dall-e-3` | 1024x1024, 1792x1024, 1024x1792 |

| `dall-e-2` | 256x256, 512x512, 1024x1024 |

You can pass optional `providerOptions` to the image model. These are prone to change by OpenAI and are model dependent. For example, the `gpt-image-1` model supports the `quality` option:

```ts

const { image, providerMetadata } = await generateImage({

&nbsp; model: openai.image('gpt-image-1.5'),

&nbsp; prompt: 'A salamander at sunrise in a forest pond in the Seychelles.',

&nbsp; providerOptions: {

&nbsp;   openai: { quality: 'high' },

&nbsp; },

});

```

For more on `generateImage()` see \[Image Generation](/docs/ai-sdk-core/image-generation).

OpenAI's image models return additional metadata in the response that can be

accessed via `providerMetadata.openai`. The following OpenAI-specific metadata

is available:

\- \*\*images\*\* \_Array\&lt;object\&gt;\_

&nbsp; Array of image-specific metadata. Each image object may contain:

&nbsp; - `revisedPrompt` \_string\_ - The revised prompt that was actually used to generate the image (OpenAI may modify your prompt for safety or clarity)

&nbsp; - `created` \_number\_ - The Unix timestamp (in seconds) of when the image was created

&nbsp; - `size` \_string\_ - The size of the generated image. One of `1024x1024`, `1024x1536`, or `1536x1024`

&nbsp; - `quality` \_string\_ - The quality of the generated image. One of `low`, `medium`, or `high`

&nbsp; - `background` \_string\_ - The background parameter used for the image generation. Either `transparent` or `opaque`

&nbsp; - `outputFormat` \_string\_ - The output format of the generated image. One of `png`, `webp`, or `jpeg`

For more information on the available OpenAI image model options, see the \[OpenAI API reference](https://platform.openai.com/docs/api-reference/images/create).

\## Transcription Models

You can create models that call the \[OpenAI transcription API](https://platform.openai.com/docs/api-reference/audio/transcribe)

using the `.transcription()` factory method.

The first argument is the model id e.g. `whisper-1`.

```ts
const model = openai.transcription("whisper-1");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format will improve accuracy and latency.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { openai } from '@ai-sdk/openai';



const result = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: new Uint8Array(\[1, 2, 3, 4]),

&nbsp; providerOptions: { openai: { language: 'en' } },

});

```

To get word-level timestamps, specify the granularity:

```ts highlight="8-9"

import { experimental\_transcribe as transcribe } from 'ai';

import { openai } from '@ai-sdk/openai';



const result = await transcribe({

&nbsp; model: openai.transcription('whisper-1'),

&nbsp; audio: new Uint8Array(\[1, 2, 3, 4]),

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     //timestampGranularities: \['word'],

&nbsp;     timestampGranularities: \['segment'],

&nbsp;   },

&nbsp; },

});



// Access word-level timestamps

console.log(result.segments); // Array of segments with startSecond/endSecond

```

The following provider options are available:

\- \*\*timestampGranularities\*\* \_string\[]\_

&nbsp; The granularity of the timestamps in the transcription.

&nbsp; Defaults to `\['segment']`.

&nbsp; Possible values are `\['word']`, `\['segment']`, and `\['word', 'segment']`.

&nbsp; Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

\- \*\*language\*\* \_string\_

&nbsp; The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency.

&nbsp; Optional.

\- \*\*prompt\*\* \_string\_

&nbsp; An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.

&nbsp; Optional.

\- \*\*temperature\*\* \_number\_

&nbsp; The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.

&nbsp; Defaults to 0.

&nbsp; Optional.

\- \*\*include\*\* \_string\[]\_

&nbsp; Additional information to include in the transcription response.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| ------------------------ | ------------------- | ------------------- | ------------------- | ------------------- |

| `whisper-1` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-4o-mini-transcribe` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `gpt-4o-transcribe` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

\## Speech Models

You can create models that call the \[OpenAI speech API](https://platform.openai.com/docs/api-reference/audio/speech)

using the `.speech()` factory method.

The first argument is the model id e.g. `tts-1`.

```ts
const model = openai.speech("tts-1");
```

The `voice` argument can be set to one of OpenAI's available voices: `alloy`, `ash`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, or `shimmer`.

```ts highlight="6"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { openai } from '@ai-sdk/openai';



const result = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

&nbsp; voice: 'alloy', // OpenAI voice ID

});

```

You can also pass additional provider-specific options using the `providerOptions` argument:

```ts highlight="7-9"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { openai } from '@ai-sdk/openai';



const result = await generateSpeech({

&nbsp; model: openai.speech('tts-1'),

&nbsp; text: 'Hello, world!',

&nbsp; voice: 'alloy',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     speed: 1.2,

&nbsp;   },

&nbsp; },

});

```

\- \*\*instructions\*\* \_string\_

&nbsp; Control the voice of your generated audio with additional instructions e.g. "Speak in a slow and steady tone".

&nbsp; Does not work with `tts-1` or `tts-1-hd`.

&nbsp; Optional.

\- \*\*response_format\*\* \_string\_

&nbsp; The format to audio in.

&nbsp; Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.

&nbsp; Defaults to `mp3`.

&nbsp; Optional.

\- \*\*speed\*\* \_number\_

&nbsp; The speed of the generated audio.

&nbsp; Select a value from 0.25 to 4.0.

&nbsp; Defaults to 1.0.

&nbsp; Optional.

\### Model Capabilities

| Model | Instructions |

| ----------------- | ------------------- |

| `tts-1` | <Check size={18} /> |

| `tts-1-hd` | <Check size={18} /> |

| `gpt-4o-mini-tts` | <Check size={18} /> |

---

title: Azure OpenAI

description: Learn how to use the Azure OpenAI provider for the AI SDK.

---

\# Azure OpenAI Provider

The \[Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service) provider contains language model support for the Azure OpenAI chat API.

\## Setup

The Azure OpenAI provider is available in the `@ai-sdk/azure` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/azure" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/azure" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/azure" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/azure" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `azure` from `@ai-sdk/azure`:

```ts
import { azure } from "@ai-sdk/azure";
```

If you need a customized setup, you can import `createAzure` from `@ai-sdk/azure` and create a provider instance with your settings:

```ts

import { createAzure } from '@ai-sdk/azure';



const azure = createAzure({

&nbsp; resourceName: 'your-resource-name', // Azure resource name

&nbsp; apiKey: 'your-api-key',

});

```

You can use the following optional settings to customize the OpenAI provider instance:

\- \*\*resourceName\*\* \_string\_

&nbsp; Azure resource name.

&nbsp; It defaults to the `AZURE\_RESOURCE\_NAME` environment variable.

&nbsp; The resource name is used in the assembled URL: `https://{resourceName}.openai.azure.com/openai/v1{path}`.

&nbsp; You can use `baseURL` instead to specify the URL prefix.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `api-key` header.

&nbsp; It defaults to the `AZURE\_API\_KEY` environment variable.

\- \*\*apiVersion\*\* \_string\_

&nbsp; Sets a custom \[api version](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation).

&nbsp; Defaults to `v1`.

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; Either this or `resourceName` can be used.

&nbsp; When a baseURL is provided, the resourceName is ignored.

&nbsp; With a baseURL, the resolved URL is `{baseURL}/v1{path}`.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\- \*\*useDeploymentBasedUrls\*\* \_boolean\_

&nbsp; Use deployment-based URLs for API calls. Set to `true` to use the legacy deployment format:

&nbsp; `{baseURL}/deployments/{deploymentId}{path}?api-version={apiVersion}` instead of

&nbsp; `{baseURL}/v1{path}?api-version={apiVersion}`.

&nbsp; Defaults to `false`.

&nbsp; This option is useful for compatibility with certain Azure OpenAI models or deployments

&nbsp; that require the legacy endpoint format.

\## Language Models

The Azure OpenAI provider instance is a function that you can invoke to create a language model:

```ts
const model = azure("your-deployment-name");
```

You need to pass your deployment name as the first argument.

\### Reasoning Models

Azure exposes the thinking of `DeepSeek-R1` in the generated text using the `<think>` tag.

You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```ts

import { azure } from '@ai-sdk/azure';

import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';



const enhancedModel = wrapLanguageModel({

&nbsp; model: azure('your-deepseek-r1-deployment-name'),

&nbsp; middleware: extractReasoningMiddleware({ tagName: 'think' }),

});

```

You can then use that enhanced model in functions like `generateText` and `streamText`.

<Note>

&nbsp; The Azure provider calls the Responses API by default (unless you specify e.g.

&nbsp; `azure.chat`).

</Note>

\### Example

You can use OpenAI language models to generate text with the `generateText` function:

```ts

import { azure } from '@ai-sdk/azure';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: azure('your-deployment-name'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

OpenAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

<Note>

&nbsp; Azure OpenAI sends larger chunks than OpenAI. This can lead to the perception

&nbsp; that the response is slower. See \[Troubleshooting: Azure OpenAI Slow To

&nbsp; Stream](/docs/troubleshooting/common-issues/azure-stream-slow)

</Note>

\### Provider Options

When using OpenAI language models on Azure, you can configure provider-specific options using `providerOptions.openai`. More information on available configuration options are on \[the OpenAI provider page](/providers/ai-sdk-providers/openai#language-models).

```ts highlight="12-14,22-24"

const messages = \[

&nbsp; {

&nbsp;   role: 'user',

&nbsp;   content: \[

&nbsp;     {

&nbsp;       type: 'text',

&nbsp;       text: 'What is the capital of the moon?',

&nbsp;     },

&nbsp;     {

&nbsp;       type: 'image',

&nbsp;       image: 'https://example.com/image.png',

&nbsp;       providerOptions: {

&nbsp;         openai: { imageDetail: 'low' },

&nbsp;       },

&nbsp;     },

&nbsp;   ],

&nbsp; },

];



const { text } = await generateText({

&nbsp; model: azure('your-deployment-name'),

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     reasoningEffort: 'low',

&nbsp;   },

&nbsp; },

});

```

\### Chat Models

<Note>

&nbsp; The URL for calling Azure chat models will be constructed as follows:

&nbsp; `https://RESOURCE\_NAME.openai.azure.com/openai/v1/chat/completions?api-version=v1`

</Note>

You can create models that call the Azure OpenAI chat completions API using the `.chat()` factory method:

```ts
const model = azure.chat("your-deployment-name");
```

Azure OpenAI chat models support also some model specific settings that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them as an options argument:

```ts

import { azure } from '@ai-sdk/azure';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: azure.chat('your-deployment-name'),

&nbsp; prompt: 'Write a short story about a robot.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     logitBias: {

&nbsp;       // optional likelihood for specific tokens

&nbsp;       '50256': -100,

&nbsp;     },

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for OpenAI chat models:

\- \*\*logitBias\*\* \_Record\&lt;number, number\&gt;\_

&nbsp; Modifies the likelihood of specified tokens appearing in the completion.

&nbsp; Accepts a JSON object that maps tokens (specified by their token ID in

&nbsp; the GPT tokenizer) to an associated bias value from -100 to 100. You

&nbsp; can use this tokenizer tool to convert text to token IDs. Mathematically,

&nbsp; the bias is added to the logits generated by the model prior to sampling.

&nbsp; The exact effect will vary per model, but values between -1 and 1 should

&nbsp; decrease or increase likelihood of selection; values like -100 or 100

&nbsp; should result in a ban or exclusive selection of the relevant token.

&nbsp; As an example, you can pass `{"50256": -100}` to prevent the token from being generated.

\- \*\*logprobs\*\* \_boolean | number\_

&nbsp; Return the log probabilities of the tokens. Including logprobs will increase

&nbsp; the response size and can slow down response times. However, it can

&nbsp; be useful to better understand how the model is behaving.

&nbsp; Setting to true will return the log probabilities of the tokens that

&nbsp; were generated.

&nbsp; Setting to a number will return the log probabilities of the top n

&nbsp; tokens that were generated.

\- \*\*parallelToolCalls\*\* \_boolean\_

&nbsp; Whether to enable parallel function calling during tool use. Default to true.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to

&nbsp; monitor and detect abuse. Learn more.

\### Responses Models

Azure OpenAI uses responses API as default with the `azure(deploymentName)` factory method.

```ts
const model = azure("your-deployment-name");
```

Further configuration can be done using OpenAI provider options.

You can validate the provider options using the `OpenAIResponsesProviderOptions` type.

<Note>

&nbsp; In the Responses API, use `azure` as the provider name in `providerOptions`

&nbsp; instead of `openai`. The `openai` key is still supported for `providerOptions`

&nbsp; input.

</Note>

```ts

import { azure, OpenAIResponsesProviderOptions } from '@ai-sdk/azure';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: azure('your-deployment-name'),

&nbsp; providerOptions: {

&nbsp;   azure: {

&nbsp;     parallelToolCalls: false,

&nbsp;     store: false,

&nbsp;     user: 'user\_123',

&nbsp;     // ...

&nbsp;   } satisfies OpenAIResponsesProviderOptions,

&nbsp; },

&nbsp; // ...

});

```

The following provider options are available:

\- \*\*parallelToolCalls\*\* \_boolean\_

&nbsp; Whether to use parallel tool calls. Defaults to `true`.

\- \*\*store\*\* \_boolean\_

&nbsp; Whether to store the generation. Defaults to `true`.

\- \*\*metadata\*\* \_Record\&lt;string, string\&gt;\_

&nbsp; Additional metadata to store with the generation.

\- \*\*previousResponseId\*\* \_string\_

&nbsp; The ID of the previous response. You can use it to continue a conversation. Defaults to `undefined`.

\- \*\*instructions\*\* \_string\_

&nbsp; Instructions for the model.

&nbsp; They can be used to change the system or developer message when continuing a conversation using the `previousResponseId` option.

&nbsp; Defaults to `undefined`.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Defaults to `undefined`.

\- \*\*reasoningEffort\*\* \_'low' | 'medium' | 'high'\_

&nbsp; Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.

\- \*\*strictJsonSchema\*\* \_boolean\_

&nbsp; Whether to use strict JSON schema validation. Defaults to `false`.

The Azure OpenAI provider also returns provider-specific metadata:

```ts

const { providerMetadata } = await generateText({

&nbsp; model: azure('your-deployment-name'),

});



const openaiMetadata = providerMetadata?.openai;

```

The following OpenAI-specific metadata is returned:

\- \*\*responseId\*\* \_string\_

&nbsp; The ID of the response. Can be used to continue a conversation.

\- \*\*cachedPromptTokens\*\* \_number\_

&nbsp; The number of prompt tokens that were a cache hit.

\- \*\*reasoningTokens\*\* \_number\_

&nbsp; The number of reasoning tokens that the model generated.

<Note>

&nbsp; The providerMetadata is only returned with the default responses API, and is

&nbsp; not supported when using 'azure.chat' or 'azure.completion'

</Note>

\#### Web Search Tool

The Azure OpenAI responses API supports web search(preview) through the `azure.tools.webSearchPreview` tool.

```ts

const result = await generateText({

&nbsp; model: azure('gpt-4.1-mini'),

&nbsp; prompt: 'What happened in San Francisco last week?',

&nbsp; tools: {

&nbsp;   web\_search\_preview: azure.tools.webSearchPreview({

&nbsp;     // optional configuration:

&nbsp;     searchContextSize: 'low',

&nbsp;     userLocation: {

&nbsp;       type: 'approximate',

&nbsp;       city: 'San Francisco',

&nbsp;       region: 'California',

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; // Force web search tool (optional):

&nbsp; toolChoice: { type: 'tool', toolName: 'web\_search\_preview' },

});



console.log(result.text);



// URL sources directly from `results`

const sources = result.sources;

for (const source of sources) {

&nbsp; console.log('source:', source);

}

```

<Note>

&nbsp; The tool must be named `web\_search\_preview` when using Azure OpenAI's web

&nbsp; search(preview) functionality. This name is required by Azure OpenAI's API

&nbsp; specification and cannot be customized.

</Note>

<Note>

&nbsp; The 'web_search_preview' tool is only supported with the default responses

&nbsp; API, and is not supported when using 'azure.chat' or 'azure.completion'

</Note>

\#### File Search Tool

The Azure OpenAI provider supports file search through the `azure.tools.fileSearch` tool.

You can force the use of the file search tool by setting the `toolChoice` parameter to `{ type: 'tool', toolName: 'file\_search' }`.

```ts

const result = await generateText({

&nbsp; model: azure('gpt-5'),

&nbsp; prompt: 'What does the document say about user authentication?',

&nbsp; tools: {

&nbsp;   file\_search: azure.tools.fileSearch({

&nbsp;     // optional configuration:

&nbsp;     vectorStoreIds: \['vs\_123', 'vs\_456'],

&nbsp;     maxNumResults: 10,

&nbsp;     ranking: {

&nbsp;       ranker: 'auto',

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; // Force file search tool:

&nbsp; toolChoice: { type: 'tool', toolName: 'file\_search' },

});

```

<Note>

&nbsp; The tool must be named `file\_search` when using Azure OpenAI's file search

&nbsp; functionality. This name is required by Azure OpenAI's API specification and

&nbsp; cannot be customized.

</Note>

<Note>

&nbsp; The 'file_search' tool is only supported with the default responses API, and

&nbsp; is not supported when using 'azure.chat' or 'azure.completion'

</Note>

\#### Image Generation Tool

Azure OpenAI's Responses API supports multi-modal image generation as a provider-defined tool.

Availability is restricted to specific models (for example, `gpt-5` variants).

```ts

import { createAzure } from '@ai-sdk/azure';

import { generateText } from 'ai';



const azure = createAzure({

&nbsp; headers: {

&nbsp;   'x-ms-oai-image-generation-deployment': 'gpt-image-1', // use your own image model deployment

&nbsp; },

});



const result = await generateText({

&nbsp; model: azure('gpt-5'),

&nbsp; prompt:

&nbsp;   'Generate an image of an echidna swimming across the Mozambique channel.',

&nbsp; tools: {

&nbsp;   image\_generation: azure.tools.imageGeneration({ outputFormat: 'png' }),

&nbsp; },

});



for (const toolResult of result.staticToolResults) {

&nbsp; if (toolResult.toolName === 'image\_generation') {

&nbsp;   const base64Image = toolResult.output.result;

&nbsp; }

}

```

<Note>

&nbsp; The tool must be named `image\_generation` when using Azure OpenAI's image

&nbsp; generation functionality. This name is required by Azure OpenAI's API

&nbsp; specification and cannot be customized.

</Note>

<Note>

&nbsp; The 'image_generation' tool is only supported with the default responses API,

&nbsp; and is not supported when using 'azure.chat' or 'azure.completion'

</Note>

<Note>

&nbsp; To use image_generation, you must first create an image generation model. You

&nbsp; must add a deployment specification to the header

&nbsp; `x-ms-oai-image-generation-deployment`. Please note that the Responses API

&nbsp; model and the image generation model must be in the same resource.

</Note>

<Note>

&nbsp; When you set `store: false`, then previously generated images will not be

&nbsp; accessible by the model. We recommend using the image generation tool without

&nbsp; setting `store: false`.

</Note>

\#### Code Interpreter Tool

The Azure OpenAI provider supports the code interpreter tool through the `azure.tools.codeInterpreter` tool. This allows models to write and execute Python code.

```ts

import { azure } from '@ai-sdk/azure';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: azure('gpt-5'),

&nbsp; prompt: 'Write and run Python code to calculate the factorial of 10',

&nbsp; tools: {

&nbsp;   code\_interpreter: azure.tools.codeInterpreter({

&nbsp;     // optional configuration:

&nbsp;     container: {

&nbsp;       fileIds: \['assistant-123', 'assistant-456'], // optional file IDs to make available

&nbsp;     },

&nbsp;   }),

&nbsp; },

});

```

The code interpreter tool can be configured with:

\- \*\*container\*\*: Either a container ID string or an object with `fileIds` to specify uploaded files that should be available to the code interpreter

<Note>

&nbsp; The tool must be named `code\_interpreter` when using Azure OpenAI's code

&nbsp; interpreter functionality. This name is required by Azure OpenAI's API

&nbsp; specification and cannot be customized.

</Note>

<Note>

&nbsp; The 'code_interpreter' tool is only supported with the default responses API,

&nbsp; and is not supported when using 'azure.chat' or 'azure.completion'

</Note>

<Note>

&nbsp; When working with files generated by the Code Interpreter, reference

&nbsp; information can be obtained from both \[annotations in Text

&nbsp; Parts](#typed-providermetadata-in-text-parts) and \[`providerMetadata` in

&nbsp; Source Document Parts](#typed-providermetadata-in-source-document-parts).

</Note>

\#### PDF support

The Azure OpenAI provider supports reading PDF files.

You can pass PDF files as part of the message content using the `file` type:

```ts

const result = await generateText({

&nbsp; model: azure('your-deployment-name'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: fs.readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;         filename: 'ai.pdf', // optional

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

The model will have access to the contents of the PDF file and

respond to questions about it.

The PDF file should be passed using the `data` field,

and the `mediaType` should be set to `'application/pdf'`.

<Note>

&nbsp; Reading PDF files are only supported with the default responses API, and is

&nbsp; not supported when using 'azure.chat' or 'azure.completion'

</Note>

\#### Typed providerMetadata in Text Parts

When using the Azure OpenAI Responses API, the SDK attaches Azure OpenAI-specific metadata to output parts via `providerMetadata`.

This metadata can be used on the client side for tasks such as rendering citations or downloading files generated by the Code Interpreter.

To enable type-safe handling of this metadata, the AI SDK exports dedicated TypeScript types.

For text parts, when `part.type === 'text'`, the `providerMetadata` is provided in the form of `AzureResponsesTextProviderMetadata`.

This metadata includes the following fields:

\- `itemId`

&nbsp; The ID of the output item in the Responses API.

\- `annotations` (optional)

&nbsp; An array of annotation objects generated by the model.

&nbsp; If no annotations are present, this property itself may be omitted (`undefined`).

&nbsp; Each element in `annotations` is a discriminated union with a required `type` field. Supported types include, for example:

&nbsp; - `url\_citation`

&nbsp; - `file\_citation`

&nbsp; - `container\_file\_citation`

&nbsp; - `file\_path`

&nbsp; These annotations directly correspond to the annotation objects defined by the Responses API and can be used for inline reference rendering or output analysis.

&nbsp; For details, see the official OpenAI documentation:

&nbsp; \[Responses API – output text annotations](https://platform.openai.com/docs/api-reference/responses/object?lang=javascript#responses-object-output-output\_message-content-output\_text-annotations).

```ts

import { azure, type AzureResponsesTextProviderMetadata } from '@ai-sdk/azure';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: azure('gpt-4.1-mini'),

&nbsp; prompt:

&nbsp;   'Create a program that generates five random numbers between 1 and 100 with two decimal places, and show me the execution results. Also save the result to a file.',

&nbsp; tools: {

&nbsp;   code\_interpreter: azure.tools.codeInterpreter(),

&nbsp;   web\_search\_preview: azure.tools.webSearchPreview({}),

&nbsp;   file\_search: azure.tools.fileSearch({ vectorStoreIds: \['vs\_1234'] }), // requires a configured vector store

&nbsp; },

});



for (const part of result.content) {

&nbsp; if (part.type === 'text') {

&nbsp;   const providerMetadata = part.providerMetadata as

&nbsp;     | AzureResponsesTextProviderMetadata

&nbsp;     | undefined;

&nbsp;   if (!providerMetadata) continue;

&nbsp;   const { itemId: \_itemId, annotations } = providerMetadata.azure;



&nbsp;   if (!annotations) continue;

&nbsp;   for (const annotation of annotations) {

&nbsp;     switch (annotation.type) {

&nbsp;       case 'url\_citation':

&nbsp;         // url\_citation is returned from web\_search and provides:

&nbsp;         // properties: type, url, title, start\_index and end\_index

&nbsp;         break;

&nbsp;       case 'file\_citation':

&nbsp;         // file\_citation is returned from file\_search and provides:

&nbsp;         // properties: type, file\_id, filename and index

&nbsp;         break;

&nbsp;       case 'container\_file\_citation':

&nbsp;         // container\_file\_citation is returned from code\_interpreter and provides:

&nbsp;         // properties: type, container\_id, file\_id, filename, start\_index and end\_index

&nbsp;         break;

&nbsp;       case 'file\_path':

&nbsp;         // file\_path provides:

&nbsp;         // properties: type, file\_id and index

&nbsp;         break;

&nbsp;       default: {

&nbsp;         const \_exhaustiveCheck: never = annotation;

&nbsp;         throw new Error(

&nbsp;           `Unhandled annotation: ${JSON.stringify(\_exhaustiveCheck)}`,

&nbsp;         );

&nbsp;       }

&nbsp;     }

&nbsp;   }

&nbsp; }

}

```

<Note>

&nbsp; When implementing file downloads for files generated by the Code Interpreter,

&nbsp; the `container\_id` and `file\_id` available in `providerMetadata` can be used

&nbsp; to retrieve the file content. For details, see the \[Retrieve container file

&nbsp; content](https://platform.openai.com/docs/api-reference/container-files/retrieveContainerFileContent)

&nbsp; API.

</Note>

\#### Typed providerMetadata in Source Document Parts

For source document parts, when `part.type === 'source'` and `sourceType === 'document'`, the `providerMetadata` is provided as `AzureResponsesSourceDocumentProviderMetadata`.

This metadata is also a discriminated union with a required `type` field. Supported types include:

\- `file\_citation`

\- `container\_file\_citation`

\- `file\_path`

Each type includes the identifiers required to work with the referenced resource, such as `fileId` and `containerId`.

```ts

import {

&nbsp; azure,

&nbsp; type AzureResponsesSourceDocumentProviderMetadata,

} from '@ai-sdk/azure';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: azure('gpt-4.1-mini'),

&nbsp; prompt:

&nbsp;   'Create a program that generates five random numbers between 1 and 100 with two decimal places, and show me the execution results. Also save the result to a file.',

&nbsp; tools: {

&nbsp;   code\_interpreter: azure.tools.codeInterpreter(),

&nbsp;   web\_search\_preview: azure.tools.webSearchPreview({}),

&nbsp;   file\_search: azure.tools.fileSearch({ vectorStoreIds: \['vs\_1234'] }), // requires a configured vector store

&nbsp; },

});



for (const part of result.content) {

&nbsp; if (part.type === 'source') {

&nbsp;   if (part.sourceType === 'document') {

&nbsp;     const providerMetadata = part.providerMetadata as

&nbsp;       | AzureResponsesSourceDocumentProviderMetadata

&nbsp;       | undefined;

&nbsp;     if (!providerMetadata) continue;

&nbsp;     const annotation = providerMetadata.azure;

&nbsp;     switch (annotation.type) {

&nbsp;       case 'file\_citation':

&nbsp;         // file\_citation is returned from file\_search and provides:

&nbsp;         // properties: type, fileId and index

&nbsp;         // The filename can be accessed via part.filename.

&nbsp;         break;

&nbsp;       case 'container\_file\_citation':

&nbsp;         // container\_file\_citation is returned from code\_interpreter and provides:

&nbsp;         // properties: type, containerId and fileId

&nbsp;         // The filename can be accessed via part.filename.

&nbsp;         break;

&nbsp;       case 'file\_path':

&nbsp;         // file\_path provides:

&nbsp;         // properties: type, fileId and index

&nbsp;         break;

&nbsp;       default: {

&nbsp;         const \_exhaustiveCheck: never = annotation;

&nbsp;         throw new Error(

&nbsp;           `Unhandled annotation: ${JSON.stringify(\_exhaustiveCheck)}`,

&nbsp;         );

&nbsp;       }

&nbsp;     }

&nbsp;   }

&nbsp; }

}

```

<Note>

&nbsp; Annotations in text parts follow the OpenAI Responses API specification and

&nbsp; therefore use snake_case properties (e.g. `file\_id`, `container\_id`). In

&nbsp; contrast, `providerMetadata` for source document parts is normalized by the

&nbsp; SDK to camelCase (e.g. `fileId`, `containerId`). Fields that depend on the

&nbsp; original text content, such as `start\_index` and `end\_index`, are omitted, as

&nbsp; are fields like `filename` that are directly available on the source object.

</Note>

\### Completion Models

You can create models that call the completions API using the `.completion()` factory method.

The first argument is the model id.

Currently only `gpt-35-turbo-instruct` is supported.

```ts
const model = azure.completion("your-gpt-35-turbo-instruct-deployment");
```

OpenAI completion models support also some model specific settings that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them as an options argument:

```ts

import { azure } from '@ai-sdk/azure';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: azure.completion('your-gpt-35-turbo-instruct-deployment'),

&nbsp; prompt: 'Write a haiku about coding.',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     echo: true, // optional, echo the prompt in addition to the completion

&nbsp;     logitBias: {

&nbsp;       // optional likelihood for specific tokens

&nbsp;       '50256': -100,

&nbsp;     },

&nbsp;     suffix: 'some text', // optional suffix that comes after a completion of inserted text

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Azure OpenAI completion models:

\- \*\*echo\*\*: \_boolean\_

&nbsp; Echo back the prompt in addition to the completion.

\- \*\*logitBias\*\* \_Record\&lt;number, number\&gt;\_

&nbsp; Modifies the likelihood of specified tokens appearing in the completion.

&nbsp; Accepts a JSON object that maps tokens (specified by their token ID in

&nbsp; the GPT tokenizer) to an associated bias value from -100 to 100. You

&nbsp; can use this tokenizer tool to convert text to token IDs. Mathematically,

&nbsp; the bias is added to the logits generated by the model prior to sampling.

&nbsp; The exact effect will vary per model, but values between -1 and 1 should

&nbsp; decrease or increase likelihood of selection; values like -100 or 100

&nbsp; should result in a ban or exclusive selection of the relevant token.

&nbsp; As an example, you can pass `{"50256": -100}` to prevent the \&lt;|endoftext|\&gt;

&nbsp; token from being generated.

\- \*\*logprobs\*\* \_boolean | number\_

&nbsp; Return the log probabilities of the tokens. Including logprobs will increase

&nbsp; the response size and can slow down response times. However, it can

&nbsp; be useful to better understand how the model is behaving.

&nbsp; Setting to true will return the log probabilities of the tokens that

&nbsp; were generated.

&nbsp; Setting to a number will return the log probabilities of the top n

&nbsp; tokens that were generated.

\- \*\*suffix\*\* \_string\_

&nbsp; The suffix that comes after a completion of inserted text.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to

&nbsp; monitor and detect abuse. Learn more.

\## Embedding Models

You can create models that call the Azure OpenAI embeddings API

using the `.embedding()` factory method.

```ts
const model = azure.embedding("your-embedding-deployment");
```

Azure OpenAI embedding models support several additional settings.

You can pass them as an options argument:

```ts

import { azure } from '@ai-sdk/azure';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: azure.embedding('your-embedding-deployment'),

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     dimensions: 512, // optional, number of dimensions for the embedding

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Azure OpenAI embedding models:

\- \*\*dimensions\*\*: \_number\_

&nbsp; The number of dimensions the resulting output embeddings should have.

&nbsp; Only supported in text-embedding-3 and later models.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help OpenAI to

&nbsp; monitor and detect abuse. Learn more.

\## Image Models

You can create models that call the Azure OpenAI image generation API (DALL-E) using the `.image()` factory method. The first argument is your deployment name for the DALL-E model.

```ts
const model = azure.image("your-dalle-deployment-name");
```

Azure OpenAI image models support several additional settings. You can pass them as `providerOptions.openai` when generating the image:

```ts

await generateImage({

&nbsp; model: azure.image('your-dalle-deployment-name'),

&nbsp; prompt: 'A photorealistic image of a cat astronaut floating in space',

&nbsp; size: '1024x1024', // '1024x1024', '1792x1024', or '1024x1792' for DALL-E 3

&nbsp; providerOptions: {

&nbsp;   openai: {

&nbsp;     user: 'test-user', // optional unique user identifier

&nbsp;     responseFormat: 'url', // 'url' or 'b64\_json', defaults to 'url'

&nbsp;   },

&nbsp; },

});

```

\### Example

You can use Azure OpenAI image models to generate images with the `generateImage` function:

```ts

import { azure } from '@ai-sdk/azure';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: azure.image('your-dalle-deployment-name'),

&nbsp; prompt: 'A photorealistic image of a cat astronaut floating in space',

&nbsp; size: '1024x1024', // '1024x1024', '1792x1024', or '1024x1792' for DALL-E 3

});



// image contains the URL or base64 data of the generated image

console.log(image);

```

\### Model Capabilities

Azure OpenAI supports DALL-E 2 and DALL-E 3 models through deployments. The capabilities depend on which model version your deployment is using:

| Model Version | Sizes |

| ------------- | ------------------------------- |

| DALL-E 3 | 1024x1024, 1792x1024, 1024x1792 |

| DALL-E 2 | 256x256, 512x512, 1024x1024 |

<Note>

&nbsp; DALL-E models do not support the `aspectRatio` parameter. Use the `size`

&nbsp; parameter instead.

</Note>

<Note>

&nbsp; When creating your Azure OpenAI deployment, make sure to set the DALL-E model

&nbsp; version you want to use.

</Note>

\## Transcription Models

You can create models that call the Azure OpenAI transcription API using the `.transcription()` factory method.

The first argument is the model id e.g. `whisper-1`.

```ts
const model = azure.transcription("whisper-1");
```

<Note>

&nbsp; If you encounter a "DeploymentNotFound" error with transcription models,

&nbsp; try enabling deployment-based URLs:

```ts

const azure = createAzure({

&nbsp; useDeploymentBasedUrls: true,

&nbsp; apiVersion: '2025-04-01-preview',

});

```

This uses the legacy endpoint format which may be required for certain Azure OpenAI deployments.

When using useDeploymentBasedUrls, the default api-version is not valid. You must set it to `2025-04-01-preview` or an earlier value.

</Note>

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format will improve accuracy and latency.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { azure } from '@ai-sdk/azure';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: azure.transcription('whisper-1'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { openai: { language: 'en' } },

});

```

The following provider options are available:

\- \*\*timestampGranularities\*\* \_string\[]\_

&nbsp; The granularity of the timestamps in the transcription.

&nbsp; Defaults to `\['segment']`.

&nbsp; Possible values are `\['word']`, `\['segment']`, and `\['word', 'segment']`.

&nbsp; Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

\- \*\*language\*\* \_string\_

&nbsp; The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency.

&nbsp; Optional.

\- \*\*prompt\*\* \_string\_

&nbsp; An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.

&nbsp; Optional.

\- \*\*temperature\*\* \_number\_

&nbsp; The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.

&nbsp; Defaults to 0.

&nbsp; Optional.

\- \*\*include\*\* \_string\[]\_

&nbsp; Additional information to include in the transcription response.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| ------------------------ | ------------------- | ------------------- | ------------------- | ------------------- |

| `whisper-1` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-4o-mini-transcribe` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `gpt-4o-transcribe` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

---

title: Anthropic

description: Learn how to use the Anthropic provider for the AI SDK.

---

\# Anthropic Provider

The \[Anthropic](https://www.anthropic.com/) provider contains language model support for the \[Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages\_post).

\## Setup

The Anthropic provider is available in the `@ai-sdk/anthropic` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/anthropic" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/anthropic" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/anthropic" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/anthropic" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `anthropic` from `@ai-sdk/anthropic`:

```ts
import { anthropic } from "@ai-sdk/anthropic";
```

If you need a customized setup, you can import `createAnthropic` from `@ai-sdk/anthropic` and create a provider instance with your settings:

```ts

import { createAnthropic } from '@ai-sdk/anthropic';



const anthropic = createAnthropic({

&nbsp; // custom settings

});

```

You can use the following optional settings to customize the Anthropic provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.anthropic.com/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `x-api-key` header.

&nbsp; It defaults to the `ANTHROPIC\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create models that call the \[Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages\_post) using the provider instance.

The first argument is the model id, e.g. `claude-3-haiku-20240307`.

Some models have multi-modal capabilities.

```ts
const model = anthropic("claude-3-haiku-20240307");
```

You can use Anthropic language models to generate text with the `generateText` function:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: anthropic('claude-3-haiku-20240307'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Anthropic language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

The following optional provider options are available for Anthropic models:

\- `disableParallelToolUse` \_boolean\_

&nbsp; Optional. Disables the use of parallel tool calls. Defaults to `false`.

&nbsp; When set to `true`, the model will only call one tool at a time instead of potentially calling multiple tools in parallel.

\- `sendReasoning` \_boolean\_

&nbsp; Optional. Include reasoning content in requests sent to the model. Defaults to `true`.

&nbsp; If you are experiencing issues with the model handling requests involving

&nbsp; reasoning content, you can set this to `false` to omit them from the request.

\- `effort` \_"high" | "medium" | "low"\_

&nbsp; Optional. See \[Effort section](#effort) for more details.

\- `thinking` \_object\_

&nbsp; Optional. See \[Reasoning section](#reasoning) for more details.

\- `toolStreaming` \_boolean\_

&nbsp; Whether to enable tool streaming (and structured output streaming). Default to `true`.

\- `structuredOutputMode` \_"outputFormat" | "jsonTool" | "auto"\_

&nbsp; Determines how structured outputs are generated. Optional.

&nbsp; - `"outputFormat"`: Use the `output\_format` parameter to specify the structured output format.

&nbsp; - `"jsonTool"`: Use a special `"json"` tool to specify the structured output format.

&nbsp; - `"auto"`: Use `"outputFormat"` when supported, otherwise fall back to `"jsonTool"` (default).

\### Structured Outputs and Tool Input Streaming

Tool call streaming is enabled by default. You can opt out by setting the

`toolStreaming` provider option to `false`.

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { streamText, tool } from 'ai';

import { z } from 'zod';



const result = streamText({

&nbsp; model: anthropic('claude-sonnet-4-20250514'),

&nbsp; tools: {

&nbsp;   writeFile: tool({

&nbsp;     description: 'Write content to a file',

&nbsp;     inputSchema: z.object({

&nbsp;       path: z.string(),

&nbsp;       content: z.string(),

&nbsp;     }),

&nbsp;     execute: async ({ path, content }) => {

&nbsp;       // Implementation

&nbsp;       return { success: true };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'Write a short story to story.txt',

});

```

\### Effort

Anthropic introduced an `effort` option with `claude-opus-4-5` that affects thinking, text responses, and function calls. Effort defaults to `high` and you can set it to `medium` or `low` to save tokens and to lower time-to-last-token latency (TTLT).

```ts highlight="8-10"

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text, usage } = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     effort: 'low',

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});



console.log(text); // resulting text

console.log(usage); // token usage

```

\### Reasoning

Anthropic has reasoning support for `claude-opus-4-20250514`, `claude-sonnet-4-20250514`, and `claude-3-7-sonnet-20250219` models.

You can enable it using the `thinking` provider option

and specifying a thinking budget in tokens.

```ts highlight="4,8-10"

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});



console.log(reasoningText); // reasoning text

console.log(reasoning); // reasoning details including redacted reasoning

console.log(text); // text response

```

See \[AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details

on how to integrate reasoning into your chatbot.

\### Context Management

Anthropic's Context Management feature allows you to automatically manage conversation context by clearing tool uses or thinking content when certain conditions are met. This helps optimize token usage and manage long conversations more efficiently.

You can configure context management using the `contextManagement` provider option:

```ts highlight="7-20"

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: anthropic('claude-3-7-sonnet-20250219'),

&nbsp; prompt: 'Continue our conversation...',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     contextManagement: {

&nbsp;       edits: \[

&nbsp;         {

&nbsp;           type: 'clear\_tool\_uses\_20250919',

&nbsp;           trigger: { type: 'input\_tokens', value: 10000 },

&nbsp;           keep: { type: 'tool\_uses', value: 5 },

&nbsp;           clearAtLeast: { type: 'input\_tokens', value: 1000 },

&nbsp;           clearToolInputs: true,

&nbsp;           excludeTools: \['important\_tool'],

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});



// Check what was cleared

console.log(result.providerMetadata?.anthropic?.contextManagement);

```

\#### Clear Tool Uses

The `clear\_tool\_uses\_20250919` edit type removes old tool calls from the conversation history:

\- \*\*trigger\*\* - Condition that triggers the clearing (e.g., `{ type: 'input\_tokens', value: 10000 }`)

\- \*\*keep\*\* - How many recent tool uses to preserve (e.g., `{ type: 'tool\_uses', value: 5 }`)

\- \*\*clearAtLeast\*\* - Minimum amount to clear (e.g., `{ type: 'input\_tokens', value: 1000 }`)

\- \*\*clearToolInputs\*\* - Whether to clear tool input parameters (boolean)

\- \*\*excludeTools\*\* - Array of tool names to never clear

\#### Clear Thinking

The `clear\_thinking\_20251015` edit type removes thinking/reasoning content:

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'Continue reasoning...',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;     contextManagement: {

&nbsp;       edits: \[

&nbsp;         {

&nbsp;           type: 'clear\_thinking\_20251015',

&nbsp;           keep: { type: 'thinking\_turns', value: 2 },

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});

```

\#### Applied Edits Metadata

After generation, you can check which edits were applied in the provider metadata:

```ts

const metadata = result.providerMetadata?.anthropic?.contextManagement;



if (metadata?.appliedEdits) {

&nbsp; metadata.appliedEdits.forEach(edit => {

&nbsp;   if (edit.type === 'clear\_tool\_uses\_20250919') {

&nbsp;     console.log(`Cleared ${edit.clearedToolUses} tool uses`);

&nbsp;     console.log(`Freed ${edit.clearedInputTokens} tokens`);

&nbsp;   } else if (edit.type === 'clear\_thinking\_20251015') {

&nbsp;     console.log(`Cleared ${edit.clearedThinkingTurns} thinking turns`);

&nbsp;     console.log(`Freed ${edit.clearedInputTokens} tokens`);

&nbsp;   }

&nbsp; });

}

```

For more details, see \[Anthropic's Context Management documentation](https://docs.anthropic.com/en/docs/build-with-claude/context-management).

\### Cache Control

In the messages and message parts, you can use the `providerOptions` property to set cache control breakpoints.

You need to set the `anthropic` property in the `providerOptions` object to `{ cacheControl: { type: 'ephemeral' } }` to set a cache control breakpoint.

The cache creation input tokens are then returned in the `providerMetadata` object

for `generateText` and `generateObject`, again under the `anthropic` property.

When you use `streamText` or `streamObject`, the response contains a promise

that resolves to the metadata. Alternatively you can receive it in the

`onFinish` callback.

```ts highlight="8,18-20,29-30"

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const errorMessage = '... long error message ...';



const result = await generateText({

&nbsp; model: anthropic('claude-3-5-sonnet-20240620'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'You are a JavaScript expert.' },

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: `Error message: ${errorMessage}`,

&nbsp;         providerOptions: {

&nbsp;           anthropic: { cacheControl: { type: 'ephemeral' } },

&nbsp;         },

&nbsp;       },

&nbsp;       { type: 'text', text: 'Explain the error message.' },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});



console.log(result.text);

console.log(result.providerMetadata?.anthropic);

// e.g. { cacheCreationInputTokens: 2118 }

```

You can also use cache control on system messages by providing multiple system messages at the head of your messages array:

```ts highlight="3,7-9"

const result = await generateText({

&nbsp; model: anthropic('claude-3-5-sonnet-20240620'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'system',

&nbsp;     content: 'Cached system message part',

&nbsp;     providerOptions: {

&nbsp;       anthropic: { cacheControl: { type: 'ephemeral' } },

&nbsp;     },

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'system',

&nbsp;     content: 'Uncached system message part',

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: 'User prompt',

&nbsp;   },

&nbsp; ],

});

```

Cache control for tools:

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-3-5-haiku-latest'),

&nbsp; tools: {

&nbsp;   cityAttractions: tool({

&nbsp;     inputSchema: z.object({ city: z.string() }),

&nbsp;     providerOptions: {

&nbsp;       anthropic: {

&nbsp;         cacheControl: { type: 'ephemeral' },

&nbsp;       },

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: 'User prompt',

&nbsp;   },

&nbsp; ],

});

```

\#### Longer cache TTL

Anthropic also supports a longer 1-hour cache duration.

Here's an example:

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-3-5-haiku-latest'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'Long cached message',

&nbsp;         providerOptions: {

&nbsp;           anthropic: {

&nbsp;             cacheControl: { type: 'ephemeral', ttl: '1h' },

&nbsp;           },

&nbsp;         },

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

\#### Limitations

The minimum cacheable prompt length is:

\- 4096 tokens for Claude Opus 4.5

\- 1024 tokens for Claude Opus 4.1, Claude Opus 4, Claude Sonnet 4.5, Claude Sonnet 4, Claude Sonnet 3.7, and Claude Opus 3

\- 4096 tokens for Claude Haiku 4.5

\- 2048 tokens for Claude Haiku 3.5 and Claude Haiku 3

Shorter prompts cannot be cached, even if marked with `cacheControl`. Any requests to cache fewer than this number of tokens will be processed without caching.

For more on prompt caching with Anthropic, see \[Anthropic's Cache Control documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching).

<Note type="warning">

&nbsp; Because the `UIMessage` type (used by AI SDK UI hooks like `useChat`) does not

&nbsp; support the `providerOptions` property, you can use `convertToModelMessages`

&nbsp; first before passing the messages to functions like `generateText` or

&nbsp; `streamText`. For more details on `providerOptions` usage, see

&nbsp; \[here](/docs/foundations/prompts#provider-options).

</Note>

\### Bash Tool

The Bash Tool allows running bash commands. Here's how to create and use it:

```ts

const bashTool = anthropic.tools.bash\_20241022({

&nbsp; execute: async ({ command, restart }) => {

&nbsp;   // Implement your bash command execution logic here

&nbsp;   // Return the result of the command execution

&nbsp; },

});

```

Parameters:

\- `command` (string): The bash command to run. Required unless the tool is being restarted.

\- `restart` (boolean, optional): Specifying true will restart this tool.

<Note>Only certain Claude versions are supported.</Note>

\### Memory Tool

The \[Memory Tool](https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool) allows Claude to use a local memory, e.g. in the filesystem.

Here's how to create it:

```ts

const memory = anthropic.tools.memory\_20250818({

&nbsp; execute: async action => {

&nbsp;   // Implement your memory command execution logic here

&nbsp;   // Return the result of the command execution

&nbsp; },

});

```

<Note>Only certain Claude versions are supported.</Note>

\### Text Editor Tool

The Text Editor Tool provides functionality for viewing and editing text files.

```ts

const tools = {

&nbsp; str\_replace\_based\_edit\_tool: anthropic.tools.textEditor\_20250728({

&nbsp;   maxCharacters: 10000, // optional

&nbsp;   async execute({ command, path, old\_str, new\_str }) {

&nbsp;     // ...

&nbsp;   },

&nbsp; }),

} satisfies ToolSet;

```

<Note>

&nbsp; Different models support different versions of the tool. For Claude Sonnet 3.5

&nbsp; and 3.7 you need to use older tool versions.

</Note>

Parameters:

\- `command` ('view' | 'create' | 'str_replace' | 'insert' | 'undo_edit'): The command to run. Note: `undo\_edit` is only available in Claude 3.5 Sonnet and earlier models.

\- `path` (string): Absolute path to file or directory, e.g. `/repo/file.py` or `/repo`.

\- `file\_text` (string, optional): Required for `create` command, with the content of the file to be created.

\- `insert\_line` (number, optional): Required for `insert` command. The line number after which to insert the new string.

\- `new\_str` (string, optional): New string for `str\_replace` or `insert` commands.

\- `old\_str` (string, optional): Required for `str\_replace` command, containing the string to replace.

\- `view\_range` (number\[], optional): Optional for `view` command to specify line range to show.

\### Computer Tool

The Computer Tool enables control of keyboard and mouse actions on a computer:

```ts

const computerTool = anthropic.tools.computer\_20241022({

&nbsp; displayWidthPx: 1920,

&nbsp; displayHeightPx: 1080,

&nbsp; displayNumber: 0, // Optional, for X11 environments



&nbsp; execute: async ({ action, coordinate, text }) => {

&nbsp;   // Implement your computer control logic here

&nbsp;   // Return the result of the action



&nbsp;   // Example code:

&nbsp;   switch (action) {

&nbsp;     case 'screenshot': {

&nbsp;       // multipart result:

&nbsp;       return {

&nbsp;         type: 'image',

&nbsp;         data: fs

&nbsp;           .readFileSync('./data/screenshot-editor.png')

&nbsp;           .toString('base64'),

&nbsp;       };

&nbsp;     }

&nbsp;     default: {

&nbsp;       console.log('Action:', action);

&nbsp;       console.log('Coordinate:', coordinate);

&nbsp;       console.log('Text:', text);

&nbsp;       return `executed ${action}`;

&nbsp;     }

&nbsp;   }

&nbsp; },



&nbsp; // map to tool result content for LLM consumption:

&nbsp; toModelOutput({ output }) {

&nbsp;   return typeof output === 'string'

&nbsp;     ? \[{ type: 'text', text: output }]

&nbsp;     : \[{ type: 'image', data: output.data, mediaType: 'image/png' }];

&nbsp; },

});

```

Parameters:

\- `action` ('key' | 'type' | 'mouse_move' | 'left_click' | 'left_click_drag' | 'right_click' | 'middle_click' | 'double_click' | 'screenshot' | 'cursor_position'): The action to perform.

\- `coordinate` (number\[], optional): Required for `mouse\_move` and `left\_click\_drag` actions. Specifies the (x, y) coordinates.

\- `text` (string, optional): Required for `type` and `key` actions.

These tools can be used in conjunction with the `sonnet-3-5-sonnet-20240620` model to enable more complex interactions and tasks.

\### Web Search Tool

Anthropic provides a provider-defined web search tool that gives Claude direct access to real-time web content, allowing it to answer questions with up-to-date information beyond its knowledge cutoff.

You can enable web search using the provider-defined web search tool:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const webSearchTool = anthropic.tools.webSearch\_20250305({

&nbsp; maxUses: 5,

});



const result = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'What are the latest developments in AI?',

&nbsp; tools: {

&nbsp;   web\_search: webSearchTool,

&nbsp; },

});

```

<Note>

&nbsp; Web search must be enabled in your organization's \[Console

&nbsp; settings](https://console.anthropic.com/settings/privacy).

</Note>

\#### Configuration Options

The web search tool supports several configuration options:

\- \*\*maxUses\*\* \_number\_

&nbsp; Maximum number of web searches Claude can perform during the conversation.

\- \*\*allowedDomains\*\* \_string\[]\_

&nbsp; Optional list of domains that Claude is allowed to search. If provided, searches will be restricted to these domains.

\- \*\*blockedDomains\*\* \_string\[]\_

&nbsp; Optional list of domains that Claude should avoid when searching.

\- \*\*userLocation\*\* \_object\_

&nbsp; Optional user location information to provide geographically relevant search results.

```ts

const webSearchTool = anthropic.tools.webSearch\_20250305({

&nbsp; maxUses: 3,

&nbsp; allowedDomains: \['techcrunch.com', 'wired.com'],

&nbsp; blockedDomains: \['example-spam-site.com'],

&nbsp; userLocation: {

&nbsp;   type: 'approximate',

&nbsp;   country: 'US',

&nbsp;   region: 'California',

&nbsp;   city: 'San Francisco',

&nbsp;   timezone: 'America/Los\_Angeles',

&nbsp; },

});



const result = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'Find local news about technology',

&nbsp; tools: {

&nbsp;   web\_search: webSearchTool,

&nbsp; },

});

```

\### Web Fetch Tool

Anthropic provides a provider-defined web fetch tool that allows Claude to retrieve content from specific URLs. This is useful when you want Claude to analyze or reference content from a particular webpage or document.

You can enable web fetch using the provider-defined web fetch tool:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-0'),

&nbsp; prompt:

&nbsp;   'What is this page about? https://en.wikipedia.org/wiki/Maglemosian\_culture',

&nbsp; tools: {

&nbsp;   web\_fetch: anthropic.tools.webFetch\_20250910({ maxUses: 1 }),

&nbsp; },

});

```

\### Tool Search

Anthropic provides provider-defined tool search tools that enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. Instead of loading all tool definitions into the context window upfront, Claude searches your tool catalog and loads only the tools it needs.

There are two variants:

\- \*\*BM25 Search\*\* - Uses natural language queries to find tools

\- \*\*Regex Search\*\* - Uses regex patterns (Python `re.search()` syntax) to find tools

\#### Basic Usage

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText, tool } from 'ai';

import { z } from 'zod';



const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-5'),

&nbsp; prompt: 'What is the weather in San Francisco?',

&nbsp; tools: {

&nbsp;   toolSearch: anthropic.tools.toolSearchBm25\_20251119(),



&nbsp;   get\_weather: tool({

&nbsp;     description: 'Get the current weather at a specific location',

&nbsp;     inputSchema: z.object({

&nbsp;       location: z.string().describe('The city and state'),

&nbsp;     }),

&nbsp;     execute: async ({ location }) => ({

&nbsp;       location,

&nbsp;       temperature: 72,

&nbsp;       condition: 'Sunny',

&nbsp;     }),

&nbsp;     // Defer tool here - Claude discovers these via the tool search tool

&nbsp;     providerOptions: {

&nbsp;       anthropic: { deferLoading: true },

&nbsp;     },

&nbsp;   }),

&nbsp; },

});

```

\#### Using Regex Search

For more precise tool matching, you can use the regex variant:

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-5'),

&nbsp; prompt: 'Get the weather data',

&nbsp; tools: {

&nbsp;   toolSearch: anthropic.tools.toolSearchRegex\_20251119(),

&nbsp;   // ... deferred tools

&nbsp; },

});

```

Claude will construct regex patterns like `weather|temperature|forecast` to find matching tools.

\### MCP Connectors

Anthropic supports connecting to \[MCP servers](https://docs.claude.com/en/docs/agents-and-tools/mcp-connector) as part of their execution.

You can enable this feature with the `mcpServers` provider option:

```ts

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-5'),

&nbsp; prompt: `Call the echo tool with "hello world". what does it respond with back?`,

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     mcpServers: \[

&nbsp;       {

&nbsp;         type: 'url',

&nbsp;         name: 'echo',

&nbsp;         url: 'https://echo.mcp.inevitable.fyi/mcp',

&nbsp;         // optional: authorization token

&nbsp;         authorizationToken: mcpAuthToken,

&nbsp;         // optional: tool configuration

&nbsp;         toolConfiguration: {

&nbsp;           enabled: true,

&nbsp;           allowedTools: \['echo'],

&nbsp;         },

&nbsp;       },

&nbsp;     ],

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});

```

The tool calls and results are dynamic, i.e. the input and output schemas are not known.

\#### Configuration Options

The web fetch tool supports several configuration options:

\- \*\*maxUses\*\* \_number\_

&nbsp; The maxUses parameter limits the number of web fetches performed.

\- \*\*allowedDomains\*\* \_string\[]\_

&nbsp; Only fetch from these domains.

\- \*\*blockedDomains\*\* \_string\[]\_

&nbsp; Never fetch from these domains.

\- \*\*citations\*\* \_object\_

&nbsp; Unlike web search where citations are always enabled, citations are optional for web fetch. Set `"citations": {"enabled": true}` to enable Claude to cite specific passages from fetched documents.

\- \*\*maxContentTokens\*\* \_number\_

&nbsp; The maxContentTokens parameter limits the amount of content that will be included in the context.

\#### Error Handling

Web search errors are handled differently depending on whether you're using streaming or non-streaming:

\*\*Non-streaming (`generateText`, `generateObject`):\*\*

Web search errors throw exceptions that you can catch:

```ts

try {

&nbsp; const result = await generateText({

&nbsp;   model: anthropic('claude-opus-4-20250514'),

&nbsp;   prompt: 'Search for something',

&nbsp;   tools: {

&nbsp;     web\_search: webSearchTool,

&nbsp;   },

&nbsp; });

} catch (error) {

&nbsp; if (error.message.includes('Web search failed')) {

&nbsp;   console.log('Search error:', error.message);

&nbsp;   // Handle search error appropriately

&nbsp; }

}

```

\*\*Streaming (`streamText`, `streamObject`):\*\*

Web search errors are delivered as error parts in the stream:

```ts

const result = await streamText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'Search for something',

&nbsp; tools: {

&nbsp;   web\_search: webSearchTool,

&nbsp; },

});



for await (const part of result.textStream) {

&nbsp; if (part.type === 'error') {

&nbsp;   console.log('Search error:', part.error);

&nbsp;   // Handle search error appropriately

&nbsp; }

}

```

\## Code Execution

Anthropic provides a provider-defined code execution tool that gives Claude direct access to a real Python environment allowing it to execute code to inform its responses.

You can enable code execution using the provider-defined code execution tool:

```ts

import { anthropic } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const codeExecutionTool = anthropic.tools.codeExecution\_20250825();



const result = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt:

&nbsp;   'Calculate the mean and standard deviation of \[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]',

&nbsp; tools: {

&nbsp;   code\_execution: codeExecutionTool,

&nbsp; },

});

```

\#### Error Handling

Code execution errors are handled differently depending on whether you're using streaming or non-streaming:

\*\*Non-streaming (`generateText`, `generateObject`):\*\*

Code execution errors are delivered as tool result parts in the response:

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'Execute some Python script',

&nbsp; tools: {

&nbsp;   code\_execution: codeExecutionTool,

&nbsp; },

});



const toolErrors = result.content?.filter(

&nbsp; content => content.type === 'tool-error',

);



toolErrors?.forEach(error => {

&nbsp; console.error('Tool execution error:', {

&nbsp;   toolName: error.toolName,

&nbsp;   toolCallId: error.toolCallId,

&nbsp;   error: error.error,

&nbsp; });

});

```

\*\*Streaming (`streamText`, `streamObject`):\*\*

Code execution errors are delivered as error parts in the stream:

```ts

const result = await streamText({

&nbsp; model: anthropic('claude-opus-4-20250514'),

&nbsp; prompt: 'Execute some Python script',

&nbsp; tools: {

&nbsp;   code\_execution: codeExecutionTool,

&nbsp; },

});

for await (const part of result.textStream) {

&nbsp; if (part.type === 'error') {

&nbsp;   console.log('Code execution error:', part.error);

&nbsp;   // Handle code execution error appropriately

&nbsp; }

}

```

\### Programmatic Tool Calling

\[Programmatic Tool Calling](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/programmatic-tool-calling) allows Claude to write code that calls your tools programmatically within a code execution container, rather than requiring round trips through the model for each tool invocation. This reduces latency for multi-tool workflows and decreases token consumption.

To enable programmatic tool calling, use the `allowedCallers` provider option on tools that you want to be callable from within code execution:

```ts highlight="13-17"

import {

&nbsp; anthropic,

&nbsp; forwardAnthropicContainerIdFromLastStep,

} from '@ai-sdk/anthropic';

import { generateText, tool, stepCountIs } from 'ai';

import { z } from 'zod';



const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-5'),

&nbsp; stopWhen: stepCountIs(10),

&nbsp; prompt:

&nbsp;   'Get the weather for Tokyo, Sydney, and London, then calculate the average temperature.',

&nbsp; tools: {

&nbsp;   code\_execution: anthropic.tools.codeExecution\_20250825(),



&nbsp;   getWeather: tool({

&nbsp;     description: 'Get current weather data for a city.',

&nbsp;     inputSchema: z.object({

&nbsp;       city: z.string().describe('Name of the city'),

&nbsp;     }),

&nbsp;     execute: async ({ city }) => {

&nbsp;       // Your weather API implementation

&nbsp;       return { temp: 22, condition: 'Sunny' };

&nbsp;     },

&nbsp;     // Enable this tool to be called from within code execution

&nbsp;     providerOptions: {

&nbsp;       anthropic: {

&nbsp;         allowedCallers: \['code\_execution\_20250825'],

&nbsp;       },

&nbsp;     },

&nbsp;   }),

&nbsp; },



&nbsp; // Propagate container ID between steps for code execution continuity

&nbsp; prepareStep: forwardAnthropicContainerIdFromLastStep,

});

```

In this flow:

1\. Claude writes Python code that calls your `getWeather` tool multiple times in parallel

2\. The SDK automatically executes your tool and returns results to the code execution container

3\. Claude processes the results in code and generates the final response

<Note>

&nbsp; Programmatic tool calling requires `claude-sonnet-4-5` or `claude-opus-4-5`

&nbsp; models and uses the `code\_execution\_20250825` tool.

</Note>

\#### Container Persistence

When using programmatic tool calling across multiple steps, you need to preserve the container ID between steps using `prepareStep`. You can use the `forwardAnthropicContainerIdFromLastStep` helper function to do this automatically. The container ID is available in `providerMetadata.anthropic.container.id` after each step completes.

\## Agent Skills

\[Anthropic Agent Skills](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview) enable Claude to perform specialized tasks like document processing (PPTX, DOCX, PDF, XLSX) and data analysis. Skills run in a sandboxed container and require the code execution tool to be enabled.

\### Using Built-in Skills

Anthropic provides several built-in skills:

\- \*\*pptx\*\* - Create and edit PowerPoint presentations

\- \*\*docx\*\* - Create and edit Word documents

\- \*\*pdf\*\* - Process and analyze PDF files

\- \*\*xlsx\*\* - Work with Excel spreadsheets

To use skills, you need to:

1\. Enable the code execution tool

2\. Specify the container with skills in `providerOptions`

```ts highlight="4,9-17,19-23"

import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-5'),

&nbsp; tools: {

&nbsp;   code\_execution: anthropic.tools.codeExecution\_20250825(),

&nbsp; },

&nbsp; prompt: 'Create a presentation about renewable energy with 5 slides',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     container: {

&nbsp;       skills: \[

&nbsp;         {

&nbsp;           type: 'anthropic',

&nbsp;           skillId: 'pptx',

&nbsp;           version: 'latest', // optional

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});

```

\### Custom Skills

You can also use custom skills by specifying `type: 'custom'`:

```ts highlight="9-11"

const result = await generateText({

&nbsp; model: anthropic('claude-sonnet-4-5'),

&nbsp; tools: {

&nbsp;   code\_execution: anthropic.tools.codeExecution\_20250825(),

&nbsp; },

&nbsp; prompt: 'Use my custom skill to process this data',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     container: {

&nbsp;       skills: \[

&nbsp;         {

&nbsp;           type: 'custom',

&nbsp;           skillId: 'my-custom-skill-id',

&nbsp;           version: '1.0', // optional

&nbsp;         },

&nbsp;       ],

&nbsp;     },

&nbsp;   } satisfies AnthropicProviderOptions,

&nbsp; },

});

```

<Note>

&nbsp; Skills use progressive context loading and execute within a sandboxed

&nbsp; container with code execution capabilities.

</Note>

\### PDF support

Anthropic Sonnet `claude-3-5-sonnet-20241022` supports reading PDF files.

You can pass PDF files as part of the message content using the `file` type:

Option 1: URL-based PDF document

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-3-5-sonnet-20241022'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model according to this document?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: new URL(

&nbsp;           'https://github.com/vercel/ai/blob/main/examples/ai-functions/data/ai.pdf?raw=true',

&nbsp;         ),

&nbsp;         mimeType: 'application/pdf',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

Option 2: Base64-encoded PDF document

```ts

const result = await generateText({

&nbsp; model: anthropic('claude-3-5-sonnet-20241022'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model according to this document?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: fs.readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

The model will have access to the contents of the PDF file and

respond to questions about it.

The PDF file should be passed using the `data` field,

and the `mediaType` should be set to `'application/pdf'`.

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Computer Use | Web Search | Tool Search |

| -------------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `claude-opus-4-5` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `claude-haiku-4-5` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | |

| `claude-sonnet-4-5` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `claude-opus-4-1` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | |

| `claude-opus-4-0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | |

| `claude-sonnet-4-0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | |

| `claude-3-7-sonnet-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | |

| `claude-3-5-haiku-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | |

<Note>

&nbsp; The table above lists popular models. Please see the \[Anthropic

&nbsp; docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list

&nbsp; of available models. The table above lists popular models. You can also pass

&nbsp; any available provider model ID as a string if needed.

</Note>

---

title: Amazon Bedrock

description: Learn how to use the Amazon Bedrock provider.

---

\# Amazon Bedrock Provider

The Amazon Bedrock provider for the \[AI SDK](/docs) contains language model support for the \[Amazon Bedrock](https://aws.amazon.com/bedrock) APIs.

\## Setup

The Bedrock provider is available in the `@ai-sdk/amazon-bedrock` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/amazon-bedrock" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/amazon-bedrock" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/amazon-bedrock" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/amazon-bedrock" dark />

&nbsp; </Tab>

</Tabs>

\### Prerequisites

Access to Amazon Bedrock foundation models isn't granted by default. In order to gain access to a foundation model, an IAM user with sufficient permissions needs to request access to it through the console. Once access is provided to a model, it is available for all users in the account.

See the \[Model Access Docs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) for more information.

\### Authentication

\#### Using IAM Access Key and Secret Key

\*\*Step 1: Creating AWS Access Key and Secret Key\*\*

To get started, you'll need to create an AWS access key and secret key. Here's how:

\*\*Login to AWS Management Console\*\*

\- Go to the \[AWS Management Console](https://console.aws.amazon.com/) and log in with your AWS account credentials.

\*\*Create an IAM User\*\*

\- Navigate to the \[IAM dashboard](https://console.aws.amazon.com/iam/home) and click on "Users" in the left-hand navigation menu.

\- Click on "Create user" and fill in the required details to create a new IAM user.

\- Make sure to select "Programmatic access" as the access type.

\- The user account needs the `AmazonBedrockFullAccess` policy attached to it.

\*\*Create Access Key\*\*

\- Click on the "Security credentials" tab and then click on "Create access key".

\- Click "Create access key" to generate a new access key pair.

\- Download the `.csv` file containing the access key ID and secret access key.

\*\*Step 2: Configuring the Access Key and Secret Key\*\*

Within your project add a `.env` file if you don't already have one. This file will be used to set the access key and secret key as environment variables. Add the following lines to the `.env` file:

```makefile

AWS\_ACCESS\_KEY\_ID=YOUR\_ACCESS\_KEY\_ID

AWS\_SECRET\_ACCESS\_KEY=YOUR\_SECRET\_ACCESS\_KEY

AWS\_REGION=YOUR\_REGION

```

<Note>

&nbsp; Many frameworks such as \[Next.js](https://nextjs.org/) load the `.env` file

&nbsp; automatically. If you're using a different framework, you may need to load the

&nbsp; `.env` file manually using a package like

&nbsp; \[`dotenv`](https://github.com/motdotla/dotenv).

</Note>

Remember to replace `YOUR\_ACCESS\_KEY\_ID`, `YOUR\_SECRET\_ACCESS\_KEY`, and `YOUR\_REGION` with the actual values from your AWS account.

\#### Using AWS SDK Credentials Chain (instance profiles, instance roles, ECS roles, EKS Service Accounts, etc.)

When using AWS SDK, the SDK will automatically use the credentials chain to determine the credentials to use. This includes instance profiles, instance roles, ECS roles, EKS Service Accounts, etc. A similar behavior is possible using the AI SDK by not specifying the `accessKeyId` and `secretAccessKey`, `sessionToken` properties in the provider settings and instead passing a `credentialProvider` property.

\_Usage:\_

`@aws-sdk/credential-providers` package provides a set of credential providers that can be used to create a credential provider chain.

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @aws-sdk/credential-providers" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @aws-sdk/credential-providers" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @aws-sdk/credential-providers" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @aws-sdk/credential-providers" dark />

&nbsp; </Tab>

</Tabs>

```ts

import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';

import { fromNodeProviderChain } from '@aws-sdk/credential-providers';



const bedrock = createAmazonBedrock({

&nbsp; region: 'us-east-1',

&nbsp; credentialProvider: fromNodeProviderChain(),

});

```

\## Provider Instance

You can import the default provider instance `bedrock` from `@ai-sdk/amazon-bedrock`:

```ts
import { bedrock } from "@ai-sdk/amazon-bedrock";
```

If you need a customized setup, you can import `createAmazonBedrock` from `@ai-sdk/amazon-bedrock` and create a provider instance with your settings:

```ts

import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';



const bedrock = createAmazonBedrock({

&nbsp; region: 'us-east-1',

&nbsp; accessKeyId: 'xxxxxxxxx',

&nbsp; secretAccessKey: 'xxxxxxxxx',

&nbsp; sessionToken: 'xxxxxxxxx',

});

```

<Note>

&nbsp; The credentials settings fall back to environment variable defaults described

&nbsp; below. These may be set by your serverless environment without your awareness,

&nbsp; which can lead to merged/conflicting credential values and provider errors

&nbsp; around failed authentication. If you're experiencing issues be sure you are

&nbsp; explicitly specifying all settings (even if `undefined`) to avoid any

&nbsp; defaults.

</Note>

You can use the following optional settings to customize the Amazon Bedrock provider instance:

\- \*\*region\*\* \_string\_

&nbsp; The AWS region that you want to use for the API calls.

&nbsp; It uses the `AWS\_REGION` environment variable by default.

\- \*\*accessKeyId\*\* \_string\_

&nbsp; The AWS access key ID that you want to use for the API calls.

&nbsp; It uses the `AWS\_ACCESS\_KEY\_ID` environment variable by default.

\- \*\*secretAccessKey\*\* \_string\_

&nbsp; The AWS secret access key that you want to use for the API calls.

&nbsp; It uses the `AWS\_SECRET\_ACCESS\_KEY` environment variable by default.

\- \*\*sessionToken\*\* \_string\_

&nbsp; Optional. The AWS session token that you want to use for the API calls.

&nbsp; It uses the `AWS\_SESSION\_TOKEN` environment variable by default.

\- \*\*credentialProvider\*\* \_() =\&gt; Promise\&lt;\&#123; accessKeyId: string; secretAccessKey: string; sessionToken?: string; \&#125;\&gt;\_

&nbsp; Optional. The AWS credential provider chain that you want to use for the API calls.

&nbsp; It uses the specified credentials by default.

\## Language Models

You can create models that call the Bedrock API using the provider instance.

The first argument is the model id, e.g. `meta.llama3-70b-instruct-v1:0`.

```ts
const model = bedrock("meta.llama3-70b-instruct-v1:0");
```

Amazon Bedrock models also support some model specific provider options that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them in the `providerOptions` argument:

```ts

const model = bedrock('anthropic.claude-3-sonnet-20240229-v1:0');



await generateText({

&nbsp; model,

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     additionalModelRequestFields: { top\_k: 350 },

&nbsp;   },

&nbsp; },

});

```

Documentation for additional settings based on the selected model can be found within the \[Amazon Bedrock Inference Parameter Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).

You can use Amazon Bedrock language models to generate text with the `generateText` function:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: bedrock('meta.llama3-70b-instruct-v1:0'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Amazon Bedrock language models can also be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

\### File Inputs

<Note type="warning">

&nbsp; Amazon Bedrock supports file inputs in combination with specific models, e.g.

&nbsp; `anthropic.claude-3-haiku-20240307-v1:0`.

</Note>

The Amazon Bedrock provider supports file inputs, e.g. PDF files.

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: bedrock('anthropic.claude-3-haiku-20240307-v1:0'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'Describe the pdf in detail.' },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

\### Guardrails

You can use the `bedrock` provider options to utilize \[Amazon Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/):

```ts

const result = await generateText({

&nbsp; model: bedrock('anthropic.claude-3-sonnet-20240229-v1:0'),

&nbsp; prompt: 'Write a story about space exploration.',

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     guardrailConfig: {

&nbsp;       guardrailIdentifier: '1abcd2ef34gh',

&nbsp;       guardrailVersion: '1',

&nbsp;       trace: 'enabled' as const,

&nbsp;       streamProcessingMode: 'async',

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

Tracing information will be returned in the provider metadata if you have tracing enabled.

```ts

if (result.providerMetadata?.bedrock.trace) {

&nbsp; // ...

}

```

See the \[Amazon Bedrock Guardrails documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) for more information.

\### Citations

Amazon Bedrock supports citations for document-based inputs across compatible models. When enabled:

\- Some models can read documents with visual understanding, not just extracting text

\- Models can cite specific parts of documents you provide, making it easier to trace information back to its source (Not Supported Yet)

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateObject } from 'ai';

import { z } from 'zod';

import fs from 'fs';



const result = await generateObject({

&nbsp; model: bedrock('apac.anthropic.claude-sonnet-4-20250514-v1:0'),

&nbsp; schema: z.object({

&nbsp;   summary: z.string().describe('Summary of the PDF document'),

&nbsp;   keyPoints: z.array(z.string()).describe('Key points from the PDF'),

&nbsp; }),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'Summarize this PDF and provide key points.',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: readFileSync('./document.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;         providerOptions: {

&nbsp;           bedrock: {

&nbsp;             citations: { enabled: true },

&nbsp;           },

&nbsp;         },

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});



console.log('Response:', result.object);

```

\### Cache Points

<Note>

&nbsp; Amazon Bedrock prompt caching is currently in preview release. To request

&nbsp; access, visit the \[Amazon Bedrock prompt caching

&nbsp; page](https://aws.amazon.com/bedrock/prompt-caching/).

</Note>

In messages, you can use the `providerOptions` property to set cache points. Set the `bedrock` property in the `providerOptions` object to `{ cachePoint: { type: 'default' } }` to create a cache point.

Cache usage information is returned in the `providerMetadata` object`. See examples below.

<Note>

&nbsp; Cache points have model-specific token minimums and limits. For example,

&nbsp; Claude 3.5 Sonnet v2 requires at least 1,024 tokens for a cache point and

&nbsp; allows up to 4 cache points. See the \[Amazon Bedrock prompt caching

&nbsp; documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)

&nbsp; for details on supported models, regions, and limits.

</Note>

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



const cyberpunkAnalysis =

&nbsp; '... literary analysis of cyberpunk themes and concepts ...';



const result = await generateText({

&nbsp; model: bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'system',

&nbsp;     content: `You are an expert on William Gibson's cyberpunk literature and themes. You have access to the following academic analysis: ${cyberpunkAnalysis}`,

&nbsp;     providerOptions: {

&nbsp;       bedrock: { cachePoint: { type: 'default' } },

&nbsp;     },

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content:

&nbsp;       'What are the key cyberpunk themes that Gibson explores in Neuromancer?',

&nbsp;   },

&nbsp; ],

});



console.log(result.text);

console.log(result.providerMetadata?.bedrock?.usage);

// Shows cache read/write token usage, e.g.:

// {

//   cacheReadInputTokens: 1337,

//   cacheWriteInputTokens: 42,

// }

```

Cache points also work with streaming responses:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { streamText } from 'ai';



const cyberpunkAnalysis =

&nbsp; '... literary analysis of cyberpunk themes and concepts ...';



const result = streamText({

&nbsp; model: bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'assistant',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'You are an expert on cyberpunk literature.' },

&nbsp;       { type: 'text', text: `Academic analysis: ${cyberpunkAnalysis}` },

&nbsp;     ],

&nbsp;     providerOptions: { bedrock: { cachePoint: { type: 'default' } } },

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content:

&nbsp;       'How does Gibson explore the relationship between humanity and technology?',

&nbsp;   },

&nbsp; ],

});



for await (const textPart of result.textStream) {

&nbsp; process.stdout.write(textPart);

}



console.log(

&nbsp; 'Cache token usage:',

&nbsp; (await result.providerMetadata)?.bedrock?.usage,

);

// Shows cache read/write token usage, e.g.:

// {

//   cacheReadInputTokens: 1337,

//   cacheWriteInputTokens: 42,

// }

```

\## Reasoning

Amazon Bedrock supports model creator-specific reasoning features:

\- Anthropic (e.g. `claude-3-7-sonnet-20250219`): enable via the `reasoningConfig` provider option and specifying a thinking budget in tokens (minimum: `1024`, maximum: `64000`).

\- Amazon (e.g. `us.amazon.nova-2-lite-v1:0`): enable via the `reasoningConfig` provider option and specifying a maximum reasoning effort level (`'low' | 'medium' | 'high'`).

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



// Anthropic example

const anthropicResult = await generateText({

&nbsp; model: bedrock('us.anthropclaude-3-7-sonnet-20250219-v1:0'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     reasoningConfig: { type: 'enabled', budgetTokens: 1024 },

&nbsp;   },

&nbsp; },

});



console.log(anthropicResult.reasoningText); // reasoning text

console.log(anthropicResult.text); // text response



// Nova 2 example

const amazonResult = await generateText({

&nbsp; model: bedrock('us.amazon.nova-2-lite-v1:0'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     reasoningConfig: { type: 'enabled', maxReasoningEffort: 'medium' },

&nbsp;   },

&nbsp; },

});



console.log(amazonResult.reasoningText); // reasoning text

console.log(amazonResult.text); // text response

```

See \[AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details

on how to integrate reasoning into your chatbot.

\## Extended Context Window

Claude Sonnet 4 models on Amazon Bedrock support an extended context window of up to 1 million tokens when using the `context-1m-2025-08-07` beta feature.

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: bedrock('us.anthropic.claude-sonnet-4-20250514-v1:0'),

&nbsp; prompt: 'analyze this large document...',

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     anthropicBeta: \['context-1m-2025-08-07'],

&nbsp;   },

&nbsp; },

});

```

\## Computer Use

Via Anthropic, Amazon Bedrock provides three provider-defined tools that can be used to interact with external systems:

1\. \*\*Bash Tool\*\*: Allows running bash commands.

2\. \*\*Text Editor Tool\*\*: Provides functionality for viewing and editing text files.

3\. \*\*Computer Tool\*\*: Enables control of keyboard and mouse actions on a computer.

They are available via the `tools` property of the provider instance.

\### Bash Tool

The Bash Tool allows running bash commands. Here's how to create and use it:

```ts

const bashTool = anthropic.tools.bash\_20241022({

&nbsp; execute: async ({ command, restart }) => {

&nbsp;   // Implement your bash command execution logic here

&nbsp;   // Return the result of the command execution

&nbsp; },

});

```

Parameters:

\- `command` (string): The bash command to run. Required unless the tool is being restarted.

\- `restart` (boolean, optional): Specifying true will restart this tool.

\### Text Editor Tool

The Text Editor Tool provides functionality for viewing and editing text files.

\*\*For Claude 4 models (Opus \& Sonnet):\*\*

```ts

const textEditorTool = anthropic.tools.textEditor\_20250429({

&nbsp; execute: async ({

&nbsp;   command,

&nbsp;   path,

&nbsp;   file\_text,

&nbsp;   insert\_line,

&nbsp;   new\_str,

&nbsp;   old\_str,

&nbsp;   view\_range,

&nbsp; }) => {

&nbsp;   // Implement your text editing logic here

&nbsp;   // Return the result of the text editing operation

&nbsp; },

});

```

\*\*For Claude 3.5 Sonnet and earlier models:\*\*

```ts

const textEditorTool = anthropic.tools.textEditor\_20241022({

&nbsp; execute: async ({

&nbsp;   command,

&nbsp;   path,

&nbsp;   file\_text,

&nbsp;   insert\_line,

&nbsp;   new\_str,

&nbsp;   old\_str,

&nbsp;   view\_range,

&nbsp; }) => {

&nbsp;   // Implement your text editing logic here

&nbsp;   // Return the result of the text editing operation

&nbsp; },

});

```

Parameters:

\- `command` ('view' | 'create' | 'str_replace' | 'insert' | 'undo_edit'): The command to run. Note: `undo\_edit` is only available in Claude 3.5 Sonnet and earlier models.

\- `path` (string): Absolute path to file or directory, e.g. `/repo/file.py` or `/repo`.

\- `file\_text` (string, optional): Required for `create` command, with the content of the file to be created.

\- `insert\_line` (number, optional): Required for `insert` command. The line number after which to insert the new string.

\- `new\_str` (string, optional): New string for `str\_replace` or `insert` commands.

\- `old\_str` (string, optional): Required for `str\_replace` command, containing the string to replace.

\- `view\_range` (number\[], optional): Optional for `view` command to specify line range to show.

When using the Text Editor Tool, make sure to name the key in the tools object correctly:

\- \*\*Claude 4 models\*\*: Use `str\_replace\_based\_edit\_tool`

\- \*\*Claude 3.5 Sonnet and earlier\*\*: Use `str\_replace\_editor`

```ts

// For Claude 4 models

const response = await generateText({

&nbsp; model: bedrock('us.anthropic.claude-sonnet-4-20250514-v1:0'),

&nbsp; prompt:

&nbsp;   "Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal",

&nbsp; tools: {

&nbsp;   str\_replace\_based\_edit\_tool: textEditorTool, // Claude 4 tool name

&nbsp; },

});



// For Claude 3.5 Sonnet and earlier

const response = await generateText({

&nbsp; model: bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),

&nbsp; prompt:

&nbsp;   "Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal",

&nbsp; tools: {

&nbsp;   str\_replace\_editor: textEditorTool, // Earlier models tool name

&nbsp; },

});

```

\### Computer Tool

The Computer Tool enables control of keyboard and mouse actions on a computer:

```ts

const computerTool = anthropic.tools.computer\_20241022({

&nbsp; displayWidthPx: 1920,

&nbsp; displayHeightPx: 1080,

&nbsp; displayNumber: 0, // Optional, for X11 environments



&nbsp; execute: async ({ action, coordinate, text }) => {

&nbsp;   // Implement your computer control logic here

&nbsp;   // Return the result of the action



&nbsp;   // Example code:

&nbsp;   switch (action) {

&nbsp;     case 'screenshot': {

&nbsp;       // multipart result:

&nbsp;       return {

&nbsp;         type: 'image',

&nbsp;         data: fs

&nbsp;           .readFileSync('./data/screenshot-editor.png')

&nbsp;           .toString('base64'),

&nbsp;       };

&nbsp;     }

&nbsp;     default: {

&nbsp;       console.log('Action:', action);

&nbsp;       console.log('Coordinate:', coordinate);

&nbsp;       console.log('Text:', text);

&nbsp;       return `executed ${action}`;

&nbsp;     }

&nbsp;   }

&nbsp; },



&nbsp; // map to tool result content for LLM consumption:

&nbsp; toModelOutput({ output }) {

&nbsp;   return typeof output === 'string'

&nbsp;     ? \[{ type: 'text', text: output }]

&nbsp;     : \[{ type: 'image', data: output.data, mediaType: 'image/png' }];

&nbsp; },

});

```

Parameters:

\- `action` ('key' | 'type' | 'mouse_move' | 'left_click' | 'left_click_drag' | 'right_click' | 'middle_click' | 'double_click' | 'screenshot' | 'cursor_position'): The action to perform.

\- `coordinate` (number\[], optional): Required for `mouse\_move` and `left\_click\_drag` actions. Specifies the (x, y) coordinates.

\- `text` (string, optional): Required for `type` and `key` actions.

These tools can be used in conjunction with the `anthropic.claude-3-5-sonnet-20240620-v1:0` model to enable more complex interactions and tasks.

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ---------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `amazon.titan-tg1-large` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `amazon.titan-text-express-v1` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `amazon.titan-text-lite-v1` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `us.amazon.nova-premier-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.amazon.nova-pro-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.amazon.nova-lite-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.amazon.nova-micro-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-haiku-4-5-20251001-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-sonnet-4-20250514-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-sonnet-4-5-20250929-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-opus-4-20250514-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-opus-4-1-20250805-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-3-7-sonnet-20250219-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-3-5-sonnet-20241022-v2:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-3-5-sonnet-20240620-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-3-5-haiku-20241022-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `anthropic.claude-3-opus-20240229-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-3-sonnet-20240229-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-3-haiku-20240307-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-sonnet-4-20250514-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-sonnet-4-5-20250929-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-opus-4-20250514-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-opus-4-1-20250805-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-7-sonnet-20250219-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-5-sonnet-20241022-v2:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-5-sonnet-20240620-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-5-haiku-20241022-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `us.anthropic.claude-3-sonnet-20240229-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-opus-20240229-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-haiku-20240307-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `anthropic.claude-v2` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `anthropic.claude-v2:1` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `anthropic.claude-instant-v1` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `cohere.command-text-v14` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `cohere.command-light-text-v14` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `cohere.command-r-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `cohere.command-r-plus-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `us.deepseek.r1-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta.llama3-8b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-70b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-1-8b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-1-70b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-1-405b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-2-1b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-2-3b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-2-11b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta.llama3-2-90b-instruct-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `us.meta.llama3-2-1b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama3-2-3b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama3-2-11b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama3-2-90b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama3-1-8b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama3-1-70b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama3-3-70b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama4-scout-17b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.meta.llama4-maverick-17b-instruct-v1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistral.mistral-7b-instruct-v0:2` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `mistral.mixtral-8x7b-instruct-v0:1` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `mistral.mistral-large-2402-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `mistral.mistral-small-2402-v1:0` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `us.mistral.pixtral-large-2502-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `openai.gpt-oss-120b-1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `openai.gpt-oss-20b-1:0` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Amazon Bedrock

&nbsp; docs](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html)

&nbsp; for a full list of available models. You can also pass any available provider

&nbsp; model ID as a string if needed.

</Note>

\## Embedding Models

You can create models that call the Bedrock API \[Bedrock API](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html)

using the `.embedding()` factory method.

```ts
const model = bedrock.embedding("amazon.titan-embed-text-v1");
```

Bedrock Titan embedding model amazon.titan-embed-text-v2:0 supports several additional settings.

You can pass them as an options argument:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { embed } from 'ai';



const model = bedrock.embedding('amazon.titan-embed-text-v2:0');



const { embedding } = await embed({

&nbsp; model,

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     dimensions: 512, // optional, number of dimensions for the embedding

&nbsp;     normalize: true, // optional, normalize the output embeddings

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Bedrock Titan embedding models:

\- \*\*dimensions\*\*: \_number\_

&nbsp; The number of dimensions the output embeddings should have. The following values are accepted: 1024 (default), 512, 256.

\- \*\*normalize\*\* \_boolean\_

&nbsp; Flag indicating whether or not to normalize the output embeddings. Defaults to true.

\### Model Capabilities

| Model | Default Dimensions | Custom Dimensions |

| ------------------------------ | ------------------ | ------------------- |

| `amazon.titan-embed-text-v1` | 1536 | <Cross size={18} /> |

| `amazon.titan-embed-text-v2:0` | 1024 | <Check size={18} /> |

| `cohere.embed-english-v3` | 1024 | <Cross size={18} /> |

| `cohere.embed-multilingual-v3` | 1024 | <Cross size={18} /> |

<Note>

&nbsp; Cohere embedding models on Bedrock require an <code>input_type</code>. Set it

&nbsp; via

&nbsp; <code>providerOptions.bedrock.inputType</code> (defaults to{' '}

&nbsp; <code>search_query</code>).

</Note>

\## Reranking Models

You can create models that call the \[Bedrock Rerank API](https://docs.aws.amazon.com/bedrock/latest/userguide/rerank-api.html)

using the `.reranking()` factory method.

```ts
const model = bedrock.reranking("cohere.rerank-v3-5:0");
```

You can use Amazon Bedrock reranking models to rerank documents with the `rerank` function:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { rerank } from 'ai';



const documents = \[

&nbsp; 'sunny day at the beach',

&nbsp; 'rainy afternoon in the city',

&nbsp; 'snowy night in the mountains',

];



const { ranking } = await rerank({

&nbsp; model: bedrock.reranking('cohere.rerank-v3-5:0'),

&nbsp; documents,

&nbsp; query: 'talk about rain',

&nbsp; topN: 2,

});



console.log(ranking);

// \[

//   { originalIndex: 1, score: 0.9, document: 'rainy afternoon in the city' },

//   { originalIndex: 0, score: 0.3, document: 'sunny day at the beach' }

// ]

```

Amazon Bedrock reranking models support additional provider options that can be passed via `providerOptions.bedrock`:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: bedrock.reranking('cohere.rerank-v3-5:0'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     nextToken: 'pagination\_token\_here',

&nbsp;   },

&nbsp; },

});

```

The following provider options are available:

\- \*\*nextToken\*\* \_string\_

&nbsp; Token for pagination of results.

\- \*\*additionalModelRequestFields\*\* \_Record\&lt;string, unknown\&gt;\_

&nbsp; Additional model-specific request fields.

\### Model Capabilities

| Model |

| ---------------------- |

| `amazon.rerank-v1:0` |

| `cohere.rerank-v3-5:0` |

\## Image Models

You can create models that call the Bedrock API \[Bedrock API](https://docs.aws.amazon.com/nova/latest/userguide/image-generation.html)

using the `.image()` factory method.

For more on the Amazon Nova Canvas image model, see the \[Nova Canvas

Overview](https://docs.aws.amazon.com/ai/responsible-ai/nova-canvas/overview.html).

<Note>

&nbsp; The `amazon.nova-canvas-v1:0` model is available in the `us-east-1`,

&nbsp; `eu-west-1`, and `ap-northeast-1` regions.

</Note>

```ts
const model = bedrock.image("amazon.nova-canvas-v1:0");
```

You can then generate images with the `generateImage` function:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: 'A beautiful sunset over a calm ocean',

&nbsp; size: '512x512',

&nbsp; seed: 42,

});

```

You can also pass the `providerOptions` object to the `generateImage` function to customize the generation behavior:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: 'A beautiful sunset over a calm ocean',

&nbsp; size: '512x512',

&nbsp; seed: 42,

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     quality: 'premium',

&nbsp;     negativeText: 'blurry, low quality',

&nbsp;     cfgScale: 7.5,

&nbsp;     style: 'PHOTOREALISM',

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Amazon Nova Canvas:

\- \*\*quality\*\* \_string\_

&nbsp; The quality level for image generation. Accepts `'standard'` or `'premium'`.

\- \*\*negativeText\*\* \_string\_

&nbsp; Text describing what you don't want in the generated image.

\- \*\*cfgScale\*\* \_number\_

&nbsp; Controls how closely the generated image adheres to the prompt. Higher values result in images that are more closely aligned to the prompt.

\- \*\*style\*\* \_string\_

&nbsp; Predefined visual style for image generation.

&nbsp; Accepts one of:

&nbsp; `3D\_ANIMATED\_FAMILY\_FILM` · `DESIGN\_SKETCH` · `FLAT\_VECTOR\_ILLUSTRATION` ·

&nbsp; `GRAPHIC\_NOVEL\_ILLUSTRATION` · `MAXIMALISM` · `MIDCENTURY\_RETRO` ·

&nbsp; `PHOTOREALISM` · `SOFT\_DIGITAL\_PAINTING`.

Documentation for additional settings can be found within the \[Amazon Bedrock

User Guide for Amazon Nova

Documentation](https://docs.aws.amazon.com/nova/latest/userguide/image-gen-req-resp-structure.html).

\### Image Editing

Amazon Nova Canvas supports several image editing task types. When you provide input images via `prompt.images`, the model automatically detects the appropriate editing mode, or you can explicitly specify the `taskType` in provider options.

\#### Image Variation

Create variations of an existing image while maintaining its core characteristics:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: {

&nbsp;   text: 'Modernize the style, photo-realistic, 8k, hdr',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     taskType: 'IMAGE\_VARIATION',

&nbsp;     similarityStrength: 0.7, // 0-1, higher = closer to original

&nbsp;     negativeText: 'bad quality, low resolution',

&nbsp;   },

&nbsp; },

});

```

\- \*\*similarityStrength\*\* \_number\_

&nbsp; Controls how similar the output is to the input image. Values range from 0 to 1, where higher values produce results closer to the original.

\#### Inpainting

Edit specific parts of an image. You can define the area to modify using either a mask image or a text prompt:

\*\*Using a mask prompt (text-based selection):\*\*

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: {

&nbsp;   text: 'a cute corgi dog in the same style',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     maskPrompt: 'cat', // Describe what to replace

&nbsp;   },

&nbsp; },

&nbsp; seed: 42,

});

```

\*\*Using a mask image:\*\*

```ts

const image = readFileSync('./input-image.png');

const mask = readFileSync('./mask.png'); // White pixels = area to change



const { images } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: {

&nbsp;   text: 'A sunlit indoor lounge area with a pool containing a flamingo',

&nbsp;   images: \[image],

&nbsp;   mask: mask,

&nbsp; },

});

```

\- \*\*maskPrompt\*\* \_string\_

&nbsp; A text description of the area to modify. The model will automatically identify and mask the described region.

\#### Outpainting

Extend an image beyond its original boundaries:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: {

&nbsp;   text: 'A beautiful sunset landscape with mountains',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     taskType: 'OUTPAINTING',

&nbsp;     maskPrompt: 'background',

&nbsp;     outPaintingMode: 'DEFAULT', // or 'PRECISE'

&nbsp;   },

&nbsp; },

});

```

\- \*\*outPaintingMode\*\* \_string\_

&nbsp; Controls how the outpainting is performed. Accepts `'DEFAULT'` or `'PRECISE'`.

\#### Background Removal

Remove the background from an image:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: {

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   bedrock: {

&nbsp;     taskType: 'BACKGROUND\_REMOVAL',

&nbsp;   },

&nbsp; },

});

```

<Note>

&nbsp; Background removal does not require a text prompt - only the input image is

&nbsp; needed.

</Note>

\#### Image Editing Provider Options

The following additional provider options are available for image editing:

\- \*\*taskType\*\* \_string\_

&nbsp; Explicitly set the editing task type. Accepts `'TEXT\_IMAGE'` (default for text-only), `'IMAGE\_VARIATION'`, `'INPAINTING'`, `'OUTPAINTING'`, or `'BACKGROUND\_REMOVAL'`. When images are provided without an explicit taskType, the model defaults to `'IMAGE\_VARIATION'` (or `'INPAINTING'` if a mask is provided).

\- \*\*maskPrompt\*\* \_string\_

&nbsp; Text description of the area to modify (for inpainting/outpainting). Alternative to providing a mask image.

\- \*\*similarityStrength\*\* \_number\_

&nbsp; For `IMAGE\_VARIATION`: Controls similarity to the original (0-1).

\- \*\*outPaintingMode\*\* \_string\_

&nbsp; For `OUTPAINTING`: Controls the outpainting behavior (`'DEFAULT'` or `'PRECISE'`).

\### Image Model Settings

You can customize the generation behavior with optional options:

```ts

await generateImage({

&nbsp; model: bedrock.image('amazon.nova-canvas-v1:0'),

&nbsp; prompt: 'A beautiful sunset over a calm ocean',

&nbsp; size: '512x512',

&nbsp; seed: 42,

&nbsp; maxImagesPerCall: 1, // Maximum number of images to generate per API call

});

```

\- \*\*maxImagesPerCall\*\* \_number\_

&nbsp; Override the maximum number of images generated per API call. Default can vary

&nbsp; by model, with 5 as a common default.

\### Model Capabilities

The Amazon Nova Canvas model supports custom sizes with constraints as follows:

\- Each side must be between 320-4096 pixels, inclusive.

\- Each side must be evenly divisible by 16.

\- The aspect ratio must be between 1:4 and 4:1. That is, one side can't be more than 4 times longer than the other side.

\- The total pixel count must be less than 4,194,304.

For more, see \[Image generation access and

usage](https://docs.aws.amazon.com/nova/latest/userguide/image-gen-access.html).

| Model | Sizes |

| ------------------------- | ----------------------------------------------------------------------------------------------------- |

| `amazon.nova-canvas-v1:0` | Custom sizes: 320-4096px per side (must be divisible by 16), aspect ratio 1:4 to 4:1, max 4.2M pixels |

\## Response Headers

The Amazon Bedrock provider will return the response headers associated with

network requests made of the Bedrock servers.

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: bedrock('meta.llama3-70b-instruct-v1:0'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});



console.log(result.response.headers);

```

Below is sample output where you can see the `x-amzn-requestid` header. This can

be useful for correlating Bedrock API calls with requests made by the AI SDK:

```js highlight="6"

{

&nbsp; connection: 'keep-alive',

&nbsp; 'content-length': '2399',

&nbsp; 'content-type': 'application/json',

&nbsp; date: 'Fri, 07 Feb 2025 04:28:30 GMT',

&nbsp; 'x-amzn-requestid': 'c9f3ace4-dd5d-49e5-9807-39aedfa47c8e'

}

```

This information is also available with `streamText`:

```ts

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: bedrock('meta.llama3-70b-instruct-v1:0'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

for await (const textPart of result.textStream) {

&nbsp; process.stdout.write(textPart);

}

console.log('Response headers:', (await result.response).headers);

```

With sample output as:

```js highlight="6"

{

&nbsp; connection: 'keep-alive',

&nbsp; 'content-type': 'application/vnd.amazon.eventstream',

&nbsp; date: 'Fri, 07 Feb 2025 04:33:37 GMT',

&nbsp; 'transfer-encoding': 'chunked',

&nbsp; 'x-amzn-requestid': 'a976e3fc-0e45-4241-9954-b9bdd80ab407'

}

```

\## Bedrock Anthropic Provider Usage

The Bedrock Anthropic provider offers support for Anthropic's Claude models through Amazon Bedrock's native InvokeModel API. This provides full feature parity with the \[Anthropic API](https://platform.claude.com/docs/en/build-with-claude/overview), including features that may not be available through the Converse API (such as `stop\_sequence` in streaming responses).

For more information on Claude models available on Amazon Bedrock, see \[Claude on Amazon Bedrock](https://platform.claude.com/docs/en/build-with-claude/claude-on-amazon-bedrock).

\### Provider Instance

You can import the default provider instance `bedrockAnthropic` from `@ai-sdk/amazon-bedrock/anthropic`:

```typescript
import { bedrockAnthropic } from "@ai-sdk/amazon-bedrock/anthropic";
```

If you need a customized setup, you can import `createBedrockAnthropic` from `@ai-sdk/amazon-bedrock/anthropic` and create a provider instance with your settings:

```typescript

import { createBedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';



const bedrockAnthropic = createBedrockAnthropic({

&nbsp; region: 'us-east-1', // optional

&nbsp; accessKeyId: 'xxxxxxxxx', // optional

&nbsp; secretAccessKey: 'xxxxxxxxx', // optional

&nbsp; sessionToken: 'xxxxxxxxx', // optional

});

```

\#### Provider Settings

You can use the following optional settings to customize the Bedrock Anthropic provider instance:

\- \*\*region\*\* \_string\_

&nbsp; The AWS region that you want to use for the API calls.

&nbsp; It uses the `AWS\_REGION` environment variable by default.

\- \*\*accessKeyId\*\* \_string\_

&nbsp; The AWS access key ID that you want to use for the API calls.

&nbsp; It uses the `AWS\_ACCESS\_KEY\_ID` environment variable by default.

\- \*\*secretAccessKey\*\* \_string\_

&nbsp; The AWS secret access key that you want to use for the API calls.

&nbsp; It uses the `AWS\_SECRET\_ACCESS\_KEY` environment variable by default.

\- \*\*sessionToken\*\* \_string\_

&nbsp; Optional. The AWS session token that you want to use for the API calls.

&nbsp; It uses the `AWS\_SESSION\_TOKEN` environment variable by default.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key for authenticating requests using Bearer token authentication.

&nbsp; When provided, this will be used instead of AWS SigV4 authentication.

&nbsp; It uses the `AWS\_BEARER\_TOKEN\_BEDROCK` environment variable by default.

\- \*\*baseURL\*\* \_string\_

&nbsp; Base URL for the Bedrock API calls.

&nbsp; Useful for custom endpoints or proxy configurations.

\- \*\*headers\*\* \_Resolvable\&lt;Record\&lt;string, string | undefined\&gt;\&gt;\_

&nbsp; Headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\- \*\*credentialProvider\*\* \_() => PromiseLike\&lt;BedrockCredentials\&gt;\_

&nbsp; The AWS credential provider to use for the Bedrock provider to get dynamic

&nbsp; credentials similar to the AWS SDK. Setting a provider here will cause its

&nbsp; credential values to be used instead of the `accessKeyId`, `secretAccessKey`,

&nbsp; and `sessionToken` settings.

\### Language Models

You can create models that call the \[Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages\_post) using the provider instance.

The first argument is the model id, e.g. `us.anthropic.claude-3-5-sonnet-20241022-v2:0`.

```ts
const model = bedrockAnthropic("us.anthropic.claude-3-5-sonnet-20241022-v2:0");
```

You can use Bedrock Anthropic language models to generate text with the `generateText` function:

```ts

import { bedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: bedrockAnthropic('us.anthropic.claude-3-5-sonnet-20241022-v2:0'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

\### Cache Control

In the messages and message parts, you can use the `providerOptions` property to set cache control breakpoints.

You need to set the `anthropic` property in the `providerOptions` object to `{ cacheControl: { type: 'ephemeral' } }` to set a cache control breakpoint.

```ts

import { bedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: bedrockAnthropic('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'system',

&nbsp;     content: 'You are an expert assistant.',

&nbsp;     providerOptions: {

&nbsp;       anthropic: { cacheControl: { type: 'ephemeral' } },

&nbsp;     },

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: 'Explain quantum computing.',

&nbsp;   },

&nbsp; ],

});

```

<Note>

&nbsp; Cache control requires a minimum of 1024 tokens before the cache checkpoint.

&nbsp; See the \[Amazon Bedrock prompt caching

&nbsp; documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)

&nbsp; for details on supported models and limits.

</Note>

\### Computer Use

The Bedrock Anthropic provider supports Anthropic's computer use tools:

1\. \*\*Bash Tool\*\*: Allows running bash commands.

2\. \*\*Text Editor Tool\*\*: Provides functionality for viewing and editing text files.

3\. \*\*Computer Tool\*\*: Enables control of keyboard and mouse actions on a computer.

They are available via the `tools` property of the provider instance.

<Note>

&nbsp; Computer use tools require Claude 3.7 Sonnet or newer models. Claude 3.5

&nbsp; Sonnet v2 does not support these tools.

</Note>

\#### Bash Tool

```ts

import { bedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';

import { generateText, stepCountIs } from 'ai';



const result = await generateText({

&nbsp; model: bedrockAnthropic('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),

&nbsp; tools: {

&nbsp;   bash: bedrockAnthropic.tools.bash\_20241022({

&nbsp;     execute: async ({ command }) => {

&nbsp;       // Implement your bash command execution logic here

&nbsp;       return \[{ type: 'text', text: `Executed: ${command}` }];

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'List the files in my directory.',

&nbsp; stopWhen: stepCountIs(2),

});

```

\#### Text Editor Tool

```ts

import { bedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';

import { generateText, stepCountIs } from 'ai';



const result = await generateText({

&nbsp; model: bedrockAnthropic('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),

&nbsp; tools: {

&nbsp;   str\_replace\_editor: bedrockAnthropic.tools.textEditor\_20241022({

&nbsp;     execute: async ({ command, path, old\_str, new\_str }) => {

&nbsp;       // Implement your text editing logic here

&nbsp;       return 'File updated successfully';

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'Update my README file.',

&nbsp; stopWhen: stepCountIs(5),

});

```

\#### Computer Tool

```ts

import { bedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';

import { generateText, stepCountIs } from 'ai';

import fs from 'fs';



const result = await generateText({

&nbsp; model: bedrockAnthropic('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),

&nbsp; tools: {

&nbsp;   computer: bedrockAnthropic.tools.computer\_20241022({

&nbsp;     displayWidthPx: 1024,

&nbsp;     displayHeightPx: 768,

&nbsp;     execute: async ({ action, coordinate, text }) => {

&nbsp;       if (action === 'screenshot') {

&nbsp;         return {

&nbsp;           type: 'image',

&nbsp;           data: fs.readFileSync('./screenshot.png').toString('base64'),

&nbsp;         };

&nbsp;       }

&nbsp;       return `executed ${action}`;

&nbsp;     },

&nbsp;     toModelOutput({ output }) {

&nbsp;       return {

&nbsp;         type: 'content',

&nbsp;         value: \[

&nbsp;           typeof output === 'string'

&nbsp;             ? { type: 'text', text: output }

&nbsp;             : {

&nbsp;                 type: 'image-data',

&nbsp;                 data: output.data,

&nbsp;                 mediaType: 'image/png',

&nbsp;               },

&nbsp;         ],

&nbsp;       };

&nbsp;     },

&nbsp;   }),

&nbsp; },

&nbsp; prompt: 'Take a screenshot.',

&nbsp; stopWhen: stepCountIs(3),

});

```

\### Reasoning

Anthropic has reasoning support for Claude 3.7 and Claude 4 models on Bedrock, including:

\- `us.anthropic.claude-opus-4-5-20251101-v1:0`

\- `us.anthropic.claude-sonnet-4-5-20250929-v1:0`

\- `us.anthropic.claude-opus-4-20250514-v1:0`

\- `us.anthropic.claude-sonnet-4-20250514-v1:0`

\- `us.anthropic.claude-opus-4-1-20250805-v1:0`

\- `us.anthropic.claude-haiku-4-5-20251001-v1:0`

\- `us.anthropic.claude-3-7-sonnet-20250219-v1:0`

You can enable it using the `thinking` provider option and specifying a thinking budget in tokens.

```ts

import { bedrockAnthropic } from '@ai-sdk/amazon-bedrock/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: bedrockAnthropic('us.anthropic.claude-sonnet-4-5-20250929-v1:0'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;   },

&nbsp; },

});



console.log(reasoningText); // reasoning text

console.log(reasoning); // reasoning details including redacted reasoning

console.log(text); // text response

```

See \[AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details

on how to integrate reasoning into your chatbot.

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Computer Use | Reasoning |

| ---------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `us.anthropic.claude-opus-4-5-20251101-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-sonnet-4-5-20250929-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-opus-4-20250514-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-sonnet-4-20250514-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-opus-4-1-20250805-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-haiku-4-5-20251001-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-7-sonnet-20250219-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `us.anthropic.claude-3-5-sonnet-20241022-v2:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `us.anthropic.claude-3-5-haiku-20241022-v1:0` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The Bedrock Anthropic provider uses the native InvokeModel API and supports

&nbsp; all features available in the Anthropic API, except for the Files API and MCP

&nbsp; Connector which are not supported on Bedrock.

</Note>

\## Migrating to `@ai-sdk/amazon-bedrock` 2.x

The Amazon Bedrock provider was rewritten in version 2.x to remove the

dependency on the `@aws-sdk/client-bedrock-runtime` package.

The `bedrockOptions` provider setting previously available has been removed. If

you were using the `bedrockOptions` object, you should now use the `region`,

`accessKeyId`, `secretAccessKey`, and `sessionToken` settings directly instead.

Note that you may need to set all of these explicitly, e.g. even if you're not

using `sessionToken`, set it to `undefined`. If you're running in a serverless

environment, there may be default environment variables set by your containing

environment that the Amazon Bedrock provider will then pick up and could

conflict with the ones you're intending to use.

---

title: Groq

description: Learn how to use Groq.

---

\# Groq Provider

The \[Groq](https://groq.com/) provider contains language model support for the Groq API.

\## Setup

The Groq provider is available via the `@ai-sdk/groq` module.

You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/groq" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/groq" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/groq" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/groq" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `groq` from `@ai-sdk/groq`:

```ts
import { groq } from "@ai-sdk/groq";
```

If you need a customized setup, you can import `createGroq` from `@ai-sdk/groq`

and create a provider instance with your settings:

```ts

import { createGroq } from '@ai-sdk/groq';



const groq = createGroq({

&nbsp; // custom settings

});

```

You can use the following optional settings to customize the Groq provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.groq.com/openai/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `GROQ\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create \[Groq models](https://console.groq.com/docs/models) using a provider instance.

The first argument is the model id, e.g. `gemma2-9b-it`.

```ts
const model = groq("gemma2-9b-it");
```

\### Reasoning Models

Groq offers several reasoning models such as `qwen-qwq-32b` and `deepseek-r1-distill-llama-70b`.

You can configure how the reasoning is exposed in the generated text by using the `reasoningFormat` option.

It supports the options `parsed`, `hidden`, and `raw`.

```ts

import { groq } from '@ai-sdk/groq';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: groq('qwen/qwen3-32b'),

&nbsp; providerOptions: {

&nbsp;   groq: {

&nbsp;     reasoningFormat: 'parsed',

&nbsp;     reasoningEffort: 'default',

&nbsp;     parallelToolCalls: true, // Enable parallel function calling (default: true)

&nbsp;     user: 'user-123', // Unique identifier for end-user (optional)

&nbsp;     serviceTier: 'flex', // Use flex tier for higher throughput (optional)

&nbsp;   },

&nbsp; },

&nbsp; prompt: 'How many "r"s are in the word "strawberry"?',

});

```

The following optional provider options are available for Groq language models:

\- \*\*reasoningFormat\*\* \_'parsed' | 'raw' | 'hidden'\_

&nbsp; Controls how reasoning is exposed in the generated text. Only supported by reasoning models like `qwen-qwq-32b` and `deepseek-r1-distill-\*` models.

&nbsp; For a complete list of reasoning models and their capabilities, see \[Groq's reasoning models documentation](https://console.groq.com/docs/reasoning).

\- \*\*reasoningEffort\*\* \_'low' | 'meduim' | 'high' | 'none' | 'default'\_

&nbsp; Controls the level of effort the model will put into reasoning.

&nbsp; - `qwen/qwen3-32b`

&nbsp; - Supported values:

&nbsp; - `none`: Disable reasoning. The model will not use any reasoning tokens.

&nbsp; - `default`: Enable reasoning.

&nbsp; - `gpt-oss20b/gpt-oss120b`

&nbsp; - Supported values:

&nbsp; - `low`: Use a low level of reasoning effort.

&nbsp; - `medium`: Use a medium level of reasoning effort.

&nbsp; - `high`: Use a high level of reasoning effort.

&nbsp; Defaults to `default` for `qwen/qwen3-32b.`

\- \*\*structuredOutputs\*\* \_boolean\_

&nbsp; Whether to use structured outputs.

&nbsp; Defaults to `true`.

&nbsp; When enabled, object generation will use the `json\_schema` format instead of `json\_object` format, providing more reliable structured outputs.

\- \*\*strictJsonSchema\*\* \_boolean\_

&nbsp; Whether to use strict JSON schema validation. When `true`, the model uses constrained decoding to guarantee schema compliance.

&nbsp; Defaults to `true`.

&nbsp; Only used when `structuredOutputs` is enabled and a schema is provided. See \[Groq's Structured Outputs documentation](https://console.groq.com/docs/structured-outputs) for details on strict mode limitations.

\- \*\*parallelToolCalls\*\* \_boolean\_

&nbsp; Whether to enable parallel function calling during tool use. Defaults to `true`.

\- \*\*user\*\* \_string\_

&nbsp; A unique identifier representing your end-user, which can help with monitoring and abuse detection.

\- \*\*serviceTier\*\* \_'on_demand' | 'flex' | 'auto'\_

&nbsp; Service tier for the request. Defaults to `'on\_demand'`.

&nbsp; - `'on\_demand'`: Default tier with consistent performance and fairness

&nbsp; - `'flex'`: Higher throughput tier (10x rate limits) optimized for workloads that can handle occasional request failures

&nbsp; - `'auto'`: Uses on_demand rate limits first, then falls back to flex tier if exceeded

&nbsp; For more details about service tiers and their benefits, see \[Groq's Flex Processing documentation](https://console.groq.com/docs/flex-processing).

<Note>Only Groq reasoning models support the `reasoningFormat` option.</Note>

\#### Structured Outputs

Structured outputs are enabled by default for Groq models.

You can disable them by setting the `structuredOutputs` option to `false`.

```ts

import { groq } from '@ai-sdk/groq';

import { generateObject } from 'ai';

import { z } from 'zod';



const result = await generateObject({

&nbsp; model: groq('moonshotai/kimi-k2-instruct-0905'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.string()),

&nbsp;     instructions: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a simple pasta recipe.',

});



console.log(JSON.stringify(result.object, null, 2));

```

You can disable structured outputs for models that don't support them:

```ts highlight="9"

import { groq } from '@ai-sdk/groq';

import { generateObject } from 'ai';

import { z } from 'zod';



const result = await generateObject({

&nbsp; model: groq('gemma2-9b-it'),

&nbsp; providerOptions: {

&nbsp;   groq: {

&nbsp;     structuredOutputs: false,

&nbsp;   },

&nbsp; },

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.string()),

&nbsp;     instructions: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a simple pasta recipe in JSON format.',

});



console.log(JSON.stringify(result.object, null, 2));

```

<Note type="warning">

&nbsp; Structured outputs are only supported by newer Groq models like

&nbsp; `moonshotai/kimi-k2-instruct-0905`. For unsupported models, you can disable

&nbsp; structured outputs by setting `structuredOutputs: false`. When disabled, Groq

&nbsp; uses the `json\_object` format which requires the word "JSON" to be included in

&nbsp; your messages.

</Note>

\### Example

You can use Groq language models to generate text with the `generateText` function:

```ts

import { groq } from '@ai-sdk/groq';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: groq('gemma2-9b-it'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

\### Image Input

Groq's multi-modal models like `meta-llama/llama-4-scout-17b-16e-instruct` support image inputs. You can include images in your messages using either URLs or base64-encoded data:

```ts

import { groq } from '@ai-sdk/groq';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: groq('meta-llama/llama-4-scout-17b-16e-instruct'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'What do you see in this image?' },

&nbsp;       {

&nbsp;         type: 'image',

&nbsp;         image: 'https://example.com/image.jpg',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

You can also use base64-encoded images:

```ts

import { groq } from '@ai-sdk/groq';

import { generateText } from 'ai';

import { readFileSync } from 'fs';



const imageData = readFileSync('path/to/image.jpg', 'base64');



const { text } = await generateText({

&nbsp; model: groq('meta-llama/llama-4-scout-17b-16e-instruct'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'Describe this image in detail.' },

&nbsp;       {

&nbsp;         type: 'image',

&nbsp;         image: `data:image/jpeg;base64,${imageData}`,

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ----------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `gemma2-9b-it` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `llama-3.1-8b-instant` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `llama-3.3-70b-versatile` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/llama-guard-4-12b` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `deepseek-r1-distill-llama-70b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/llama-4-maverick-17b-128e-instruct` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/llama-4-scout-17b-16e-instruct` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/llama-prompt-guard-2-22m` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta-llama/llama-prompt-guard-2-86m` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `moonshotai/kimi-k2-instruct-0905` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `qwen/qwen3-32b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `llama-guard-3-8b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `llama3-70b-8192` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `llama3-8b-8192` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mixtral-8x7b-32768` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `qwen-qwq-32b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `qwen-2.5-32b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-r1-distill-qwen-32b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `openai/gpt-oss-20b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `openai/gpt-oss-120b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; The tables above list the most commonly used models. Please see the \[Groq

&nbsp; docs](https://console.groq.com/docs/models) for a complete list of available

&nbsp; models. You can also pass any available provider model ID as a string if

&nbsp; needed.

</Note>

\## Browser Search Tool

Groq provides a browser search tool that offers interactive web browsing capabilities. Unlike traditional web search, browser search navigates websites interactively, providing more detailed and comprehensive results.

\### Supported Models

Browser search is only available for these specific models:

\- `openai/gpt-oss-20b`

\- `openai/gpt-oss-120b`

<Note type="warning">

&nbsp; Browser search will only work with the supported models listed above. Using it

&nbsp; with other models will generate a warning and the tool will be ignored.

</Note>

\### Basic Usage

```ts

import { groq } from '@ai-sdk/groq';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: groq('openai/gpt-oss-120b'), // Must use supported model

&nbsp; prompt:

&nbsp;   'What are the latest developments in AI? Please search for recent news.',

&nbsp; tools: {

&nbsp;   browser\_search: groq.tools.browserSearch({}),

&nbsp; },

&nbsp; toolChoice: 'required', // Ensure the tool is used

});



console.log(result.text);

```

\### Streaming Example

```ts

import { groq } from '@ai-sdk/groq';

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: groq('openai/gpt-oss-120b'),

&nbsp; prompt: 'Search for the latest tech news and summarize it.',

&nbsp; tools: {

&nbsp;   browser\_search: groq.tools.browserSearch({}),

&nbsp; },

&nbsp; toolChoice: 'required',

});



for await (const delta of result.fullStream) {

&nbsp; if (delta.type === 'text-delta') {

&nbsp;   process.stdout.write(delta.text);

&nbsp; }

}

```

\### Key Features

\- \*\*Interactive Browsing\*\*: Navigates websites like a human user

\- \*\*Comprehensive Results\*\*: More detailed than traditional search snippets

\- \*\*Server-side Execution\*\*: Runs on Groq's infrastructure, no setup required

\- \*\*Powered by Exa\*\*: Uses Exa search engine for optimal results

\- \*\*Currently Free\*\*: Available at no additional charge during beta

\### Best Practices

\- Use `toolChoice: 'required'` to ensure the browser search is activated

\- Only supported on `openai/gpt-oss-20b` and `openai/gpt-oss-120b` models

\- The tool works automatically - no configuration parameters needed

\- Server-side execution means no additional API keys or setup required

\### Model Validation

The provider automatically validates model compatibility:

```ts

// ✅ Supported - will work

const result = await generateText({

&nbsp; model: groq('openai/gpt-oss-120b'),

&nbsp; tools: { browser\_search: groq.tools.browserSearch({}) },

});



// ❌ Unsupported - will show warning and ignore tool

const result = await generateText({

&nbsp; model: groq('gemma2-9b-it'),

&nbsp; tools: { browser\_search: groq.tools.browserSearch({}) },

});

// Warning: "Browser search is only supported on models: openai/gpt-oss-20b, openai/gpt-oss-120b"

```

<Note>

&nbsp; For more details about browser search capabilities and limitations, see the

&nbsp; \[Groq Browser Search

&nbsp; Documentation](https://console.groq.com/docs/browser-search).

</Note>

\## Transcription Models

You can create models that call the \[Groq transcription API](https://console.groq.com/docs/speech-to-text)

using the `.transcription()` factory method.

The first argument is the model id e.g. `whisper-large-v3`.

```ts
const model = groq.transcription("whisper-large-v3");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format will improve accuracy and latency.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { groq } from '@ai-sdk/groq';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: groq.transcription('whisper-large-v3'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { groq: { language: 'en' } },

});

```

The following provider options are available:

\- \*\*timestampGranularities\*\* \_string\[]\_

&nbsp; The granularity of the timestamps in the transcription.

&nbsp; Defaults to `\['segment']`.

&nbsp; Possible values are `\['word']`, `\['segment']`, and `\['word', 'segment']`.

&nbsp; Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

&nbsp; \*\*Important:\*\* Requires `responseFormat` to be set to `'verbose\_json'`.

\- \*\*responseFormat\*\* \_string\_

&nbsp; The format of the response. Set to `'verbose\_json'` to receive timestamps for audio segments and enable `timestampGranularities`.

&nbsp; Set to `'text'` to return only the transcribed text.

&nbsp; Optional.

\- \*\*language\*\* \_string\_

&nbsp; The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency.

&nbsp; Optional.

\- \*\*prompt\*\* \_string\_

&nbsp; An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.

&nbsp; Optional.

\- \*\*temperature\*\* \_number\_

&nbsp; The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.

&nbsp; Defaults to 0.

&nbsp; Optional.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| ------------------------ | ------------------- | ------------------- | ------------------- | ------------------- |

| `whisper-large-v3` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `whisper-large-v3-turbo` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

---

title: Fal

description: Learn how to use Fal AI models with the AI SDK.

---

\# Fal Provider

\[Fal AI](https://fal.ai/) provides a generative media platform for developers with lightning-fast inference capabilities. Their platform offers optimized performance for running diffusion models, with speeds up to 4x faster than alternatives.

\## Setup

The Fal provider is available via the `@ai-sdk/fal` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/fal" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/fal" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/fal" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/fal" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `fal` from `@ai-sdk/fal`:

```ts
import { fal } from "@ai-sdk/fal";
```

If you need a customized setup, you can import `createFal` and create a provider instance with your settings:

```ts

import { createFal } from '@ai-sdk/fal';



const fal = createFal({

&nbsp; apiKey: 'your-api-key', // optional, defaults to FAL\_API\_KEY environment variable, falling back to FAL\_KEY

&nbsp; baseURL: 'custom-url', // optional

&nbsp; headers: {

&nbsp;   /\* custom headers \*/

&nbsp; }, // optional

});

```

You can use the following optional settings to customize the Fal provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://fal.run`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `FAL\_API\_KEY` environment variable, falling back to `FAL\_KEY`.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Image Models

You can create Fal image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

\### Basic Usage

```ts

import { fal } from '@ai-sdk/fal';

import { generateImage } from 'ai';

import fs from 'fs';



const { image, providerMetadata } = await generateImage({

&nbsp; model: fal.image('fal-ai/flux/dev'),

&nbsp; prompt: 'A serene mountain landscape at sunset',

});



const filename = `image-${Date.now()}.png`;

fs.writeFileSync(filename, image.uint8Array);

console.log(`Image saved to ${filename}`);

```

Fal image models may return additional information for the images and the request.

Here are some examples of properties that may be set for each image

```js

providerMetadata.fal.images\[0].nsfw; // boolean, image is not safe for work

providerMetadata.fal.images\[0].width; // number, image width

providerMetadata.fal.images\[0].height; // number, image height

providerMetadata.fal.images\[0].content\_type; // string, mime type of the image

```

\### Model Capabilities

Fal offers many models optimized for different use cases. Here are a few popular examples. For a full list of models, see the \[Fal AI Search Page](https://fal.ai/explore/search).

| Model | Description |

| ---------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |

| `fal-ai/flux/dev` | FLUX.1 \[dev] model for high-quality image generation |

| `fal-ai/flux-pro/kontext` | FLUX.1 Kontext \[pro] handles both text and reference images as inputs, enabling targeted edits and complex transformations |

| `fal-ai/flux-pro/kontext/max` | FLUX.1 Kontext \[max] with improved prompt adherence and typography generation |

| `fal-ai/flux-lora` | Super fast endpoint for FLUX.1 with LoRA support |

| `fal-ai/ideogram/character` | Generate consistent character appearances across multiple images. Maintain facial features, proportions, and distinctive traits |

| `fal-ai/qwen-image` | Qwen-Image foundation model with significant advances in complex text rendering and precise image editing |

| `fal-ai/omnigen-v2` | Unified image generation model for Image Editing, Personalized Image Generation, Virtual Try-On, Multi Person Generation and more |

| `fal-ai/bytedance/dreamina/v3.1/text-to-image` | Dreamina showcases superior picture effects with improvements in aesthetics, precise and diverse styles, and rich details |

| `fal-ai/recraft/v3/text-to-image` | SOTA in image generation with vector art and brand style capabilities |

| `fal-ai/wan/v2.2-a14b/text-to-image` | High-resolution, photorealistic images with fine-grained detail |

Fal models support the following aspect ratios:

\- 1:1 (square HD)

\- 16:9 (landscape)

\- 9:16 (portrait)

\- 4:3 (landscape)

\- 3:4 (portrait)

\- 16:10 (1280x800)

\- 10:16 (800x1280)

\- 21:9 (2560x1080)

\- 9:21 (1080x2560)

Key features of Fal models include:

\- Up to 4x faster inference speeds compared to alternatives

\- Optimized by the Fal Inference Engine™

\- Support for real-time infrastructure

\- Cost-effective scaling with pay-per-use pricing

\- LoRA training capabilities for model personalization

\#### Modify Image

Transform existing images using text prompts.

```ts

await generateImage({

&nbsp; model: fal.image('fal-ai/flux-pro/kontext/max'),

&nbsp; prompt: {

&nbsp;   text: 'Put a donut next to the flour.',

&nbsp;   images: \[

&nbsp;     'https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf\_output.png',

&nbsp;   ],

&nbsp; },

});

```

Images can also be passed as base64-encoded string, a `Uint8Array`, an `ArrayBuffer`, or a `Buffer`.

A mask can be passed as well

```ts

await generateImage({

&nbsp; model: fal.image('fal-ai/flux-pro/kontext/max'),

&nbsp; prompt: {

&nbsp;   text: 'Put a donut next to the flour.',

&nbsp;   images: \[imageBuffer],

&nbsp;   mask: maskBuffer,

&nbsp; },

});

```

\### Provider Options

Fal image models support flexible provider options through the `providerOptions.fal` object. You can pass any parameters supported by the specific Fal model's API. Common options include:

\- \*\*imageUrl\*\* - Reference image URL for image-to-image generation

\- \*\*strength\*\* - Controls how much the output differs from the input image

\- \*\*guidanceScale\*\* - Controls adherence to the prompt (range: 1-20)

\- \*\*numInferenceSteps\*\* - Number of denoising steps (range: 1-50)

\- \*\*enableSafetyChecker\*\* - Enable/disable safety filtering

\- \*\*outputFormat\*\* - Output format: 'jpeg' or 'png'

\- \*\*syncMode\*\* - Wait for completion before returning response

\- \*\*acceleration\*\* - Speed of generation: 'none', 'regular', or 'high'

\- \*\*safetyTolerance\*\* - Content safety filtering level (1-6, where 1 is strictest)

<Note type="warning">

&nbsp; \*\*Deprecation Notice\*\*: snake_case parameter names (e.g., `image\_url`,

&nbsp; `guidance\_scale`) are deprecated and will be removed in `@ai-sdk/fal` v2.0.

&nbsp; Please use camelCase names (e.g., `imageUrl`, `guidanceScale`) instead.

</Note>

Refer to the \[Fal AI model documentation](https://fal.ai/models) for model-specific parameters.

\### Advanced Features

Fal's platform offers several advanced capabilities:

\- \*\*Private Model Inference\*\*: Run your own diffusion transformer models with up to 50% faster inference

\- \*\*LoRA Training\*\*: Train and personalize models in under 5 minutes

\- \*\*Real-time Infrastructure\*\*: Enable new user experiences with fast inference times

\- \*\*Scalable Architecture\*\*: Scale to thousands of GPUs when needed

For more details about Fal's capabilities and features, visit the \[Fal AI documentation](https://fal.ai/docs).

\## Transcription Models

You can create models that call the \[Fal transcription API](https://docs.fal.ai/guides/convert-speech-to-text)

using the `.transcription()` factory method.

The first argument is the model id without the `fal-ai/` prefix e.g. `wizper`.

```ts
const model = fal.transcription("wizper");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `batchSize` option will increase the number of audio chunks processed in parallel.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { fal } from '@ai-sdk/fal';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: fal.transcription('wizper'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { fal: { batchSize: 10 } },

});

```

The following provider options are available:

\- \*\*language\*\* \_string\_

&nbsp; Language of the audio file. If set to null, the language will be automatically detected.

&nbsp; Accepts ISO language codes like 'en', 'fr', 'zh', etc.

&nbsp; Optional.

\- \*\*diarize\*\* \_boolean\_

&nbsp; Whether to diarize the audio file (identify different speakers).

&nbsp; Defaults to true.

&nbsp; Optional.

\- \*\*chunkLevel\*\* \_string\_

&nbsp; Level of the chunks to return. Either 'segment' or 'word'.

&nbsp; Default value: "segment"

&nbsp; Optional.

\- \*\*version\*\* \_string\_

&nbsp; Version of the model to use. All models are Whisper large variants.

&nbsp; Default value: "3"

&nbsp; Optional.

\- \*\*batchSize\*\* \_number\_

&nbsp; Batch size for processing.

&nbsp; Default value: 64

&nbsp; Optional.

\- \*\*numSpeakers\*\* \_number\_

&nbsp; Number of speakers in the audio file. If not provided, the number of speakers will be automatically detected.

&nbsp; Optional.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| --------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `whisper` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `wizper` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

\## Speech Models

You can create models that call Fal text-to-speech endpoints using the `.speech()` factory method.

\### Basic Usage

```ts

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { fal } from '@ai-sdk/fal';



const result = await generateSpeech({

&nbsp; model: fal.speech('fal-ai/minimax/speech-02-hd'),

&nbsp; text: 'Hello from the AI SDK!',

});

```

\### Model Capabilities

| Model | Description |

| ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| `fal-ai/minimax/voice-clone` | Clone a voice from a sample audio and generate speech from text prompts |

| `fal-ai/minimax/voice-design` | Design a personalized voice from a text description and generate speech from text prompts |

| `fal-ai/dia-tts/voice-clone` | Clone dialog voices from a sample audio and generate dialogs from text prompts |

| `fal-ai/minimax/speech-02-hd` | Generate speech from text prompts and different voices |

| `fal-ai/minimax/speech-02-turbo` | Generate fast speech from text prompts and different voices |

| `fal-ai/dia-tts` | Directly generates realistic dialogue from transcripts with audio conditioning for emotion control. Produces natural nonverbals like laughter and throat clearing |

| `resemble-ai/chatterboxhd/text-to-speech` | Generate expressive, natural speech with Resemble AI's Chatterbox. Features unique emotion control, instant voice cloning from short audio, and built-in watermarking |

\### Provider Options

Pass provider-specific options via `providerOptions.fal` depending on the model:

\- \*\*voice_setting\*\* \_object\_

&nbsp; - `voice\_id` (string): predefined voice ID

&nbsp; - `speed` (number): 0.5–2.0

&nbsp; - `vol` (number): 0–10

&nbsp; - `pitch` (number): -12–12

&nbsp; - `emotion` (enum): happy | sad | angry | fearful | disgusted | surprised | neutral

&nbsp; - `english\_normalization` (boolean)

\- \*\*audio_setting\*\* \_object\_

&nbsp; Audio configuration settings specific to the model.

\- \*\*language_boost\*\* \_enum\_

&nbsp; Chinese | Chinese,Yue | English | Arabic | Russian | Spanish | French | Portuguese | German | Turkish | Dutch | Ukrainian | Vietnamese | Indonesian | Japanese | Italian | Korean | Thai | Polish | Romanian | Greek | Czech | Finnish | Hindi | auto

\- \*\*pronunciation_dict\*\* \_object\_

&nbsp; Custom pronunciation dictionary for specific words.

Model-specific parameters (e.g., `audio\_url`, `prompt`, `preview\_text`, `ref\_audio\_url`, `ref\_text`) can be passed directly under `providerOptions.fal` and will be forwarded to the Fal API.

---

title: AssemblyAI

description: Learn how to use the AssemblyAI provider for the AI SDK.

---

\# AssemblyAI Provider

The \[AssemblyAI](https://assemblyai.com/) provider contains language model support for the AssemblyAI transcription API.

\## Setup

The AssemblyAI provider is available in the `@ai-sdk/assemblyai` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/assemblyai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/assemblyai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/assemblyai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/assemblyai" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `assemblyai` from `@ai-sdk/assemblyai`:

```ts
import { assemblyai } from "@ai-sdk/assemblyai";
```

If you need a customized setup, you can import `createAssemblyAI` from `@ai-sdk/assemblyai` and create a provider instance with your settings:

```ts

import { createAssemblyAI } from '@ai-sdk/assemblyai';



const assemblyai = createAssemblyAI({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the AssemblyAI provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `ASSEMBLYAI\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Transcription Models

You can create models that call the \[AssemblyAI transcription API](https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file/typescript)

using the `.transcription()` factory method.

The first argument is the model id e.g. `best`.

```ts
const model = assemblyai.transcription("best");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `contentSafety` option will enable content safety filtering.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { assemblyai } from '@ai-sdk/assemblyai';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: assemblyai.transcription('best'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { assemblyai: { contentSafety: true } },

});

```

The following provider options are available:

\- \*\*audioEndAt\*\* \_number\_

&nbsp; End time of the audio in milliseconds.

&nbsp; Optional.

\- \*\*audioStartFrom\*\* \_number\_

&nbsp; Start time of the audio in milliseconds.

&nbsp; Optional.

\- \*\*autoChapters\*\* \_boolean\_

&nbsp; Whether to automatically generate chapters for the transcription.

&nbsp; Optional.

\- \*\*autoHighlights\*\* \_boolean\_

&nbsp; Whether to automatically generate highlights for the transcription.

&nbsp; Optional.

\- \*\*boostParam\*\* \_enum\_

&nbsp; Boost parameter for the transcription.

&nbsp; Allowed values: `'low'`, `'default'`, `'high'`.

&nbsp; Optional.

\- \*\*contentSafety\*\* \_boolean\_

&nbsp; Whether to enable content safety filtering.

&nbsp; Optional.

\- \*\*contentSafetyConfidence\*\* \_number\_

&nbsp; Confidence threshold for content safety filtering (25-100).

&nbsp; Optional.

\- \*\*customSpelling\*\* \_array of objects\_

&nbsp; Custom spelling rules for the transcription.

&nbsp; Each object has `from` (array of strings) and `to` (string) properties.

&nbsp; Optional.

\- \*\*disfluencies\*\* \_boolean\_

&nbsp; Whether to include disfluencies (um, uh, etc.) in the transcription.

&nbsp; Optional.

\- \*\*entityDetection\*\* \_boolean\_

&nbsp; Whether to detect entities in the transcription.

&nbsp; Optional.

\- \*\*filterProfanity\*\* \_boolean\_

&nbsp; Whether to filter profanity in the transcription.

&nbsp; Optional.

\- \*\*formatText\*\* \_boolean\_

&nbsp; Whether to format the text in the transcription.

&nbsp; Optional.

\- \*\*iabCategories\*\* \_boolean\_

&nbsp; Whether to include IAB categories in the transcription.

&nbsp; Optional.

\- \*\*languageCode\*\* \_string\_

&nbsp; Language code for the audio.

&nbsp; Supports numerous ISO-639-1 and ISO-639-3 language codes.

&nbsp; Optional.

\- \*\*languageConfidenceThreshold\*\* \_number\_

&nbsp; Confidence threshold for language detection.

&nbsp; Optional.

\- \*\*languageDetection\*\* \_boolean\_

&nbsp; Whether to enable language detection.

&nbsp; Optional.

\- \*\*multichannel\*\* \_boolean\_

&nbsp; Whether to process multiple audio channels separately.

&nbsp; Optional.

\- \*\*punctuate\*\* \_boolean\_

&nbsp; Whether to add punctuation to the transcription.

&nbsp; Optional.

\- \*\*redactPii\*\* \_boolean\_

&nbsp; Whether to redact personally identifiable information.

&nbsp; Optional.

\- \*\*redactPiiAudio\*\* \_boolean\_

&nbsp; Whether to redact PII in the audio file.

&nbsp; Optional.

\- \*\*redactPiiAudioQuality\*\* \_enum\_

&nbsp; Quality of the redacted audio file.

&nbsp; Allowed values: `'mp3'`, `'wav'`.

&nbsp; Optional.

\- \*\*redactPiiPolicies\*\* \_array of enums\_

&nbsp; Policies for PII redaction, specifying which types of information to redact.

&nbsp; Supports numerous types like `'person\_name'`, `'phone\_number'`, etc.

&nbsp; Optional.

\- \*\*redactPiiSub\*\* \_enum\_

&nbsp; Substitution method for redacted PII.

&nbsp; Allowed values: `'entity\_name'`, `'hash'`.

&nbsp; Optional.

\- \*\*sentimentAnalysis\*\* \_boolean\_

&nbsp; Whether to perform sentiment analysis on the transcription.

&nbsp; Optional.

\- \*\*speakerLabels\*\* \_boolean\_

&nbsp; Whether to label different speakers in the transcription.

&nbsp; Optional.

\- \*\*speakersExpected\*\* \_number\_

&nbsp; Expected number of speakers in the audio.

&nbsp; Optional.

\- \*\*speechThreshold\*\* \_number\_

&nbsp; Threshold for speech detection (0-1).

&nbsp; Optional.

\- \*\*summarization\*\* \_boolean\_

&nbsp; Whether to generate a summary of the transcription.

&nbsp; Optional.

\- \*\*summaryModel\*\* \_enum\_

&nbsp; Model to use for summarization.

&nbsp; Allowed values: `'informative'`, `'conversational'`, `'catchy'`.

&nbsp; Optional.

\- \*\*summaryType\*\* \_enum\_

&nbsp; Type of summary to generate.

&nbsp; Allowed values: `'bullets'`, `'bullets\_verbose'`, `'gist'`, `'headline'`, `'paragraph'`.

&nbsp; Optional.

\- \*\*topics\*\* \_array of strings\_

&nbsp; List of topics to detect in the transcription.

&nbsp; Optional.

\- \*\*webhookAuthHeaderName\*\* \_string\_

&nbsp; Name of the authentication header for webhook requests.

&nbsp; Optional.

\- \*\*webhookAuthHeaderValue\*\* \_string\_

&nbsp; Value of the authentication header for webhook requests.

&nbsp; Optional.

\- \*\*webhookUrl\*\* \_string\_

&nbsp; URL to send webhook notifications to.

&nbsp; Optional.

\- \*\*wordBoost\*\* \_array of strings\_

&nbsp; List of words to boost in the transcription.

&nbsp; Optional.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| ------ | ------------------- | ------------------- | ------------------- | ------------------- |

| `best` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `nano` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

---

title: DeepInfra

description: Learn how to use DeepInfra's models with the AI SDK.

---

\# DeepInfra Provider

The \[DeepInfra](https://deepinfra.com) provider contains support for state-of-the-art models through the DeepInfra API, including Llama 3, Mixtral, Qwen, and many other popular open-source models.

\## Setup

The DeepInfra provider is available via the `@ai-sdk/deepinfra` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/deepinfra" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/deepinfra" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/deepinfra" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/deepinfra" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `deepinfra` from `@ai-sdk/deepinfra`:

```ts
import { deepinfra } from "@ai-sdk/deepinfra";
```

If you need a customized setup, you can import `createDeepInfra` from `@ai-sdk/deepinfra` and create a provider instance with your settings:

```ts

import { createDeepInfra } from '@ai-sdk/deepinfra';



const deepinfra = createDeepInfra({

&nbsp; apiKey: process.env.DEEPINFRA\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the DeepInfra provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.deepinfra.com/v1`.

&nbsp; Note: Language models and embeddings use OpenAI-compatible endpoints at `{baseURL}/openai`,

&nbsp; while image models use `{baseURL}/inference`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `DEEPINFRA\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create language models using a provider instance. The first argument is the model ID, for example:

```ts

import { deepinfra } from '@ai-sdk/deepinfra';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

DeepInfra language models can also be used in the `streamText` function (see \[AI SDK Core](/docs/ai-sdk-core)).

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| --------------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta-llama/Llama-4-Scout-17B-16E-Instruct` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta-llama/Llama-3.3-70B-Instruct-Turbo` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Llama-3.3-70B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Meta-Llama-3.1-405B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Meta-Llama-3.1-70B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `meta-llama/Meta-Llama-3.1-8B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Llama-3.2-11B-Vision-Instruct` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta-llama/Llama-3.2-90B-Vision-Instruct` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `mistralai/Mixtral-8x7B-Instruct-v0.1` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `deepseek-ai/DeepSeek-V3` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-ai/DeepSeek-R1` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `deepseek-ai/DeepSeek-R1-Distill-Llama-70B` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `deepseek-ai/DeepSeek-R1-Turbo` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `nvidia/Llama-3.1-Nemotron-70B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `Qwen/Qwen2-7B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `Qwen/Qwen2.5-72B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `Qwen/Qwen2.5-Coder-32B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `Qwen/QwQ-32B-Preview` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `google/codegemma-7b-it` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `google/gemma-2-9b-it` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `microsoft/WizardLM-2-8x22B` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[DeepInfra

&nbsp; docs](https://deepinfra.com) for a full list of available models. You can also

&nbsp; pass any available provider model ID as a string if needed.

</Note>

\## Image Models

You can create DeepInfra image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

```ts

import { deepinfra } from '@ai-sdk/deepinfra';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: deepinfra.image('stabilityai/sd3.5'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; aspectRatio: '16:9',

});

```

<Note>

&nbsp; Model support for `size` and `aspectRatio` parameters varies by model. Please

&nbsp; check the individual model documentation on \[DeepInfra's models

&nbsp; page](https://deepinfra.com/models/text-to-image) for supported options and

&nbsp; additional parameters.

</Note>

\### Model-specific options

You can pass model-specific parameters using the `providerOptions.deepinfra` field:

```ts

import { deepinfra } from '@ai-sdk/deepinfra';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: deepinfra.image('stabilityai/sd3.5'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; aspectRatio: '16:9',

&nbsp; providerOptions: {

&nbsp;   deepinfra: {

&nbsp;     num\_inference\_steps: 30, // Control the number of denoising steps (1-50)

&nbsp;   },

&nbsp; },

});

```

\### Image Editing

DeepInfra supports image editing through models like `Qwen/Qwen-Image-Edit`. Pass input images via `prompt.images` to transform or edit existing images.

\#### Basic Image Editing

Transform an existing image using text prompts:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: deepinfra.image('Qwen/Qwen-Image-Edit'),

&nbsp; prompt: {

&nbsp;   text: 'Turn the cat into a golden retriever dog',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; size: '1024x1024',

});

```

\#### Inpainting with Mask

Edit specific parts of an image using a mask. Transparent areas in the mask indicate where the image should be edited:

```ts

const image = readFileSync('./input-image.png');

const mask = readFileSync('./mask.png');



const { images } = await generateImage({

&nbsp; model: deepinfra.image('Qwen/Qwen-Image-Edit'),

&nbsp; prompt: {

&nbsp;   text: 'A sunlit indoor lounge area with a pool containing a flamingo',

&nbsp;   images: \[image],

&nbsp;   mask: mask,

&nbsp; },

});

```

\#### Multi-Image Combining

Combine multiple reference images into a single output:

```ts

const cat = readFileSync('./cat.png');

const dog = readFileSync('./dog.png');



const { images } = await generateImage({

&nbsp; model: deepinfra.image('Qwen/Qwen-Image-Edit'),

&nbsp; prompt: {

&nbsp;   text: 'Create a scene with both animals together, playing as friends',

&nbsp;   images: \[cat, dog],

&nbsp; },

});

```

<Note>

&nbsp; Input images can be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`, or

&nbsp; base64-encoded strings. DeepInfra uses an OpenAI-compatible image editing API

&nbsp; at `https://api.deepinfra.com/v1/openai/images/edits`.

</Note>

\### Model Capabilities

For models supporting aspect ratios, the following ratios are typically supported:

`1:1 (default), 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21`

For models supporting size parameters, dimensions must typically be:

\- Multiples of 32

\- Width and height between 256 and 1440 pixels

\- Default size is 1024x1024

| Model | Dimensions Specification | Notes |

| ---------------------------------- | ------------------------ | -------------------------------------------------------- |

| `stabilityai/sd3.5` | Aspect Ratio | Premium quality base model, 8B parameters |

| `black-forest-labs/FLUX-1.1-pro` | Size | Latest state-of-art model with superior prompt following |

| `black-forest-labs/FLUX-1-schnell` | Size | Fast generation in 1-4 steps |

| `black-forest-labs/FLUX-1-dev` | Size | Optimized for anatomical accuracy |

| `black-forest-labs/FLUX-pro` | Size | Flagship Flux model |

| `stabilityai/sd3.5-medium` | Aspect Ratio | Balanced 2.5B parameter model |

| `stabilityai/sdxl-turbo` | Aspect Ratio | Optimized for fast generation |

For more details and pricing information, see the \[DeepInfra text-to-image models page](https://deepinfra.com/models/text-to-image).

\## Embedding Models

You can create DeepInfra embedding models using the `.embedding()` factory method.

For more on embedding models with the AI SDK see \[embed()](/docs/reference/ai-sdk-core/embed).

```ts

import { deepinfra } from '@ai-sdk/deepinfra';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: deepinfra.embedding('BAAI/bge-large-en-v1.5'),

&nbsp; value: 'sunny day at the beach',

});

```

\### Model Capabilities

| Model | Dimensions | Max Tokens |

| ----------------------------------------------------- | ---------- | ---------- |

| `BAAI/bge-base-en-v1.5` | 768 | 512 |

| `BAAI/bge-large-en-v1.5` | 1024 | 512 |

| `BAAI/bge-m3` | 1024 | 8192 |

| `intfloat/e5-base-v2` | 768 | 512 |

| `intfloat/e5-large-v2` | 1024 | 512 |

| `intfloat/multilingual-e5-large` | 1024 | 512 |

| `sentence-transformers/all-MiniLM-L12-v2` | 384 | 256 |

| `sentence-transformers/all-MiniLM-L6-v2` | 384 | 256 |

| `sentence-transformers/all-mpnet-base-v2` | 768 | 384 |

| `sentence-transformers/clip-ViT-B-32` | 512 | 77 |

| `sentence-transformers/clip-ViT-B-32-multilingual-v1` | 512 | 77 |

| `sentence-transformers/multi-qa-mpnet-base-dot-v1` | 768 | 512 |

| `sentence-transformers/paraphrase-MiniLM-L6-v2` | 384 | 128 |

| `shibing624/text2vec-base-chinese` | 768 | 512 |

| `thenlper/gte-base` | 768 | 512 |

| `thenlper/gte-large` | 1024 | 512 |

<Note>

&nbsp; For a complete list of available embedding models, see the \[DeepInfra

&nbsp; embeddings page](https://deepinfra.com/models/embeddings).

</Note>

---

title: Deepgram

description: Learn how to use the Deepgram provider for the AI SDK.

---

\# Deepgram Provider

The \[Deepgram](https://deepgram.com/) provider contains language model support for the Deepgram transcription API.

\## Setup

The Deepgram provider is available in the `@ai-sdk/deepgram` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/deepgram" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/deepgram" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/deepgram" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/deepgram" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `deepgram` from `@ai-sdk/deepgram`:

```ts
import { deepgram } from "@ai-sdk/deepgram";
```

If you need a customized setup, you can import `createDeepgram` from `@ai-sdk/deepgram` and create a provider instance with your settings:

```ts

import { createDeepgram } from '@ai-sdk/deepgram';



const deepgram = createDeepgram({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the Deepgram provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `DEEPGRAM\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Transcription Models

You can create models that call the \[Deepgram transcription API](https://developers.deepgram.com/docs/pre-recorded-audio)

using the `.transcription()` factory method.

The first argument is the model id e.g. `nova-3`.

```ts
const model = deepgram.transcription("nova-3");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `summarize` option will enable summaries for sections of content.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { deepgram } from '@ai-sdk/deepgram';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: deepgram.transcription('nova-3'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { deepgram: { summarize: true } },

});

```

The following provider options are available:

\- \*\*language\*\* \_string\_

&nbsp; Language code for the audio.

&nbsp; Supports numerous ISO-639-1 and ISO-639-3 language codes.

&nbsp; Optional.

\- \*\*smartFormat\*\* \_boolean\_

&nbsp; Whether to apply smart formatting to the transcription.

&nbsp; Optional.

\- \*\*punctuate\*\* \_boolean\_

&nbsp; Whether to add punctuation to the transcription.

&nbsp; Optional.

\- \*\*paragraphs\*\* \_boolean\_

&nbsp; Whether to format the transcription into paragraphs.

&nbsp; Optional.

\- \*\*summarize\*\* \_enum | boolean\_

&nbsp; Whether to generate a summary of the transcription.

&nbsp; Allowed values: `'v2'`, `false`.

&nbsp; Optional.

\- \*\*topics\*\* \_boolean\_

&nbsp; Whether to detect topics in the transcription.

&nbsp; Optional.

\- \*\*intents\*\* \_boolean\_

&nbsp; Whether to detect intents in the transcription.

&nbsp; Optional.

\- \*\*sentiment\*\* \_boolean\_

&nbsp; Whether to perform sentiment analysis on the transcription.

&nbsp; Optional.

\- \*\*detectEntities\*\* \_boolean\_

&nbsp; Whether to detect entities in the transcription.

&nbsp; Optional.

\- \*\*redact\*\* \_string | array of strings\_

&nbsp; Specifies what content to redact from the transcription.

&nbsp; Optional.

\- \*\*replace\*\* \_string\_

&nbsp; Replacement string for redacted content.

&nbsp; Optional.

\- \*\*search\*\* \_string\_

&nbsp; Search term to find in the transcription.

&nbsp; Optional.

\- \*\*keyterm\*\* \_string\_

&nbsp; Key terms to identify in the transcription.

&nbsp; Optional.

\- \*\*diarize\*\* \_boolean\_

&nbsp; Whether to identify different speakers in the transcription.

&nbsp; Defaults to `true`.

&nbsp; Optional.

\- \*\*utterances\*\* \_boolean\_

&nbsp; Whether to segment the transcription into utterances.

&nbsp; Optional.

\- \*\*uttSplit\*\* \_number\_

&nbsp; Threshold for splitting utterances.

&nbsp; Optional.

\- \*\*fillerWords\*\* \_boolean\_

&nbsp; Whether to include filler words (um, uh, etc.) in the transcription.

&nbsp; Optional.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| -------------------------------------------------------------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `nova-3` (+ \[variants](https://developers.deepgram.com/docs/models-languages-overview#nova-3)) | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `nova-2` (+ \[variants](https://developers.deepgram.com/docs/models-languages-overview#nova-2)) | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `nova` (+ \[variants](https://developers.deepgram.com/docs/models-languages-overview#nova)) | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `enhanced` (+ \[variants](https://developers.deepgram.com/docs/models-languages-overview#enhanced)) | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `base` (+ \[variants](https://developers.deepgram.com/docs/models-languages-overview#base)) | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

---

title: Black Forest Labs

description: Learn how to use Black Forest Labs models with the AI SDK.

---

\# Black Forest Labs Provider

\[Black Forest Labs](https://bfl.ai/) provides a generative image platform for developers with FLUX-based models. Their platform offers fast, high quality, and in-context image generation and editing with precise and coherent results.

\## Setup

The Black Forest Labs provider is available via the `@ai-sdk/black-forest-labs` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/black-forest-labs" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/black-forest-labs" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/black-forest-labs" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/black-forest-labs" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `blackForestLabs` from `@ai-sdk/black-forest-labs`:

```ts
import { blackForestLabs } from "@ai-sdk/black-forest-labs";
```

If you need a customized setup, you can import `createBlackForestLabs` and create a provider instance with your settings:

```ts

import { createBlackForestLabs } from '@ai-sdk/black-forest-labs';



const blackForestLabs = createBlackForestLabs({

&nbsp; apiKey: 'your-api-key', // optional, defaults to BFL\_API\_KEY environment variable

&nbsp; baseURL: 'custom-url', // optional

&nbsp; headers: {

&nbsp;   /\* custom headers \*/

&nbsp; }, // optional

});

```

You can use the following optional settings to customize the Black Forest Labs provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use a regional endpoint.

&nbsp; The default prefix is `https://api.bfl.ai/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `x-key` header.

&nbsp; It defaults to the `BFL\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Image Models

You can create Black Forest Labs image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

\### Basic Usage

```ts

import { writeFileSync } from 'node:fs';

import { blackForestLabs } from '@ai-sdk/black-forest-labs';

import { generateImage } from 'ai';



const { image, providerMetadata } = await generateImage({

&nbsp; model: blackForestLabs.image('flux-pro-1.1'),

&nbsp; prompt: 'A serene mountain landscape at sunset',

});



const filename = `image-${Date.now()}.png`;

writeFileSync(filename, image.uint8Array);

console.log(`Image saved to ${filename}`);

```

\### Model Capabilities

Black Forest Labs offers many models optimized for different use cases. Here are a few popular examples. For a full list of models, see the \[Black Forest Labs Models Page](https://bfl.ai/models).

| Model | Description |

| -------------------- | -------------------------------------------------------------------------------------------------------------------------- |

| `flux-kontext-pro` | FLUX.1 Kontext \[pro] handles both text and reference images as inputs, enabling targeted edits and complex transformations |

| `flux-kontext-max` | FLUX.1 Kontext \[max] with improved prompt adherence and typography generation |

| `flux-pro-1.1-ultra` | Ultra-fast, ultra high-resolution image creation |

| `flux-pro-1.1` | Fast, high-quality image generation from text. |

Black Forest Labs models support aspect ratios from 3:7 (portrait) to 7:3 (landscape).

\### Image Editing

Black Forest Labs Kontext models support powerful image editing capabilities using reference images. Pass input images via `prompt.images` to transform, combine, or edit existing images.

\#### Single Image Editing

Transform an existing image using text prompts:

```ts

import {

&nbsp; blackForestLabs,

&nbsp; BlackForestLabsImageProviderOptions,

} from '@ai-sdk/black-forest-labs';

import { generateImage } from 'ai';



const { images } = await generateImage({

&nbsp; model: blackForestLabs.image('flux-kontext-pro'),

&nbsp; prompt: {

&nbsp;   text: 'A baby elephant with a shirt that has the logo from the input image.',

&nbsp;   images: \[

&nbsp;     'https://www.google.com/images/branding/googlelogo/1x/googlelogo\_color\_272x92dp.png',

&nbsp;   ],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   blackForestLabs: {

&nbsp;     width: 1024,

&nbsp;     height: 768,

&nbsp;   } satisfies BlackForestLabsImageProviderOptions,

&nbsp; },

});

```

\#### Multi-Reference Editing

Combine multiple reference images for complex transformations. Black Forest Labs supports up to 10 input images:

```ts

import { blackForestLabs } from '@ai-sdk/black-forest-labs';

import { generateImage } from 'ai';



const { images } = await generateImage({

&nbsp; model: blackForestLabs.image('flux-kontext-pro'),

&nbsp; prompt: {

&nbsp;   text: 'Combine the style of image 1 with the subject of image 2',

&nbsp;   images: \[

&nbsp;     'https://example.com/style-reference.jpg',

&nbsp;     'https://example.com/subject-reference.jpg',

&nbsp;   ],

&nbsp; },

});

```

<Note>

&nbsp; Input images can be provided as URLs or base64-encoded strings. They support

&nbsp; up to 20MB or 20 megapixels per image.

</Note>

\### Provider Options

Black Forest Labs image models support flexible provider options through the `providerOptions.blackForestLabs` object. The supported parameters depend on the used model ID:

\- \*\*width\*\* \_number\_ - Output width in pixels (256–1920). When set, this overrides any width derived from `size`.

\- \*\*height\*\* \_number\_ - Output height in pixels (256–1920). When set, this overrides any height derived from `size`.

\- \*\*outputFormat\*\* \_string\_ - Desired format of the output image (`"jpeg"` or `"png"`).

\- \*\*steps\*\* \_number\_ - Number of inference steps. Higher values may improve quality but increase generation time.

\- \*\*guidance\*\* \_number\_ - Guidance scale for generation. Higher values follow the prompt more closely.

\- \*\*imagePrompt\*\* \_string\_ - Base64-encoded image to use as additional visual context for generation.

\- \*\*imagePromptStrength\*\* \_number\_ - Strength of the image prompt influence on generation (0.0 to 1.0).

\- \*\*promptUpsampling\*\* \_boolean\_ - If true, performs upsampling on the prompt.

\- \*\*raw\*\* \_boolean\_ - Enable raw mode for more natural, authentic aesthetics.

\- \*\*safetyTolerance\*\* \_number\_ - Moderation level for inputs and outputs (0 = most strict, 6 = more permissive).

\- \*\*pollIntervalMillis\*\* \_number\_ - Interval in milliseconds between polling attempts (default 500ms).

\- \*\*pollTimeoutMillis\*\* \_number\_ - Overall timeout in milliseconds for polling before timing out (default 60s).

\- \*\*webhookUrl\*\* \_string\_ - URL for asynchronous completion notification. Must be a valid HTTP/HTTPS URL.

\- \*\*webhookSecret\*\* \_string\_ - Secret for webhook signature verification, sent in the `X-Webhook-Secret` header.

<Note>

&nbsp; To pass reference images for editing, use `prompt.images` instead of provider

&nbsp; options. This supports up to 10 images as URLs or base64-encoded strings.

</Note>

\### Regional Endpoints

By default, requests are sent to `https://api.bfl.ai/v1`. You can select a \[regional endpoint](https://docs.bfl.ai/api\_integration/integration\_guidelines#regional-endpoints) by setting `baseURL` when creating the provider instance:

```ts

import { createBlackForestLabs } from '@ai-sdk/black-forest-labs';



const blackForestLabs = createBlackForestLabs({

&nbsp; baseURL: 'https://api.eu.bfl.ai/v1', // or https://api.us.bfl.ai/v1

});

```

---

title: Gladia

description: Learn how to use the Gladia provider for the AI SDK.

---

\# Gladia Provider

The \[Gladia](https://gladia.io/) provider contains language model support for the Gladia transcription API.

\## Setup

The Gladia provider is available in the `@ai-sdk/gladia` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/gladia" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/gladia" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/gladia" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/gladia" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `gladia` from `@ai-sdk/gladia`:

```ts
import { gladia } from "@ai-sdk/gladia";
```

If you need a customized setup, you can import `createGladia` from `@ai-sdk/gladia` and create a provider instance with your settings:

```ts

import { createGladia } from '@ai-sdk/gladia';



const gladia = createGladia({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the Gladia provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `GLADIA\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Transcription Models

You can create models that call the \[Gladia transcription API](https://docs.gladia.io/chapters/pre-recorded-stt/getting-started)

using the `.transcription()` factory method.

```ts
const model = gladia.transcription();
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `summarize` option will enable summaries for sections of content.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { gladia } from '@ai-sdk/gladia';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: gladia.transcription(),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { gladia: { summarize: true } },

});

```

<Note>

&nbsp; Gladia does not have various models, so you can omit the standard `model` id

&nbsp; parameter.

</Note>

The following provider options are available:

\- \*\*contextPrompt\*\* \_string\_

&nbsp; Context to feed the transcription model with for possible better accuracy.

&nbsp; Optional.

\- \*\*customVocabulary\*\* \_boolean | any\[]\_

&nbsp; Custom vocabulary to improve transcription accuracy.

&nbsp; Optional.

\- \*\*customVocabularyConfig\*\* \_object\_

&nbsp; Configuration for custom vocabulary.

&nbsp; Optional.

&nbsp; - \*\*vocabulary\*\* \_Array\&lt;string | \\{ value: string, intensity?: number, pronunciations?: string\[], language?: string \\}\&gt;\_

&nbsp; - \*\*defaultIntensity\*\* \_number\_

\- \*\*detectLanguage\*\* \_boolean\_

&nbsp; Whether to automatically detect the language.

&nbsp; Optional.

\- \*\*enableCodeSwitching\*\* \_boolean\_

&nbsp; Enable code switching for multilingual audio.

&nbsp; Optional.

\- \*\*codeSwitchingConfig\*\* \_object\_

&nbsp; Configuration for code switching.

&nbsp; Optional.

&nbsp; - \*\*languages\*\* \_string\[]\_

\- \*\*language\*\* \_string\_

&nbsp; Specify the language of the audio.

&nbsp; Optional.

\- \*\*callback\*\* \_boolean\_

&nbsp; Enable callback when transcription is complete.

&nbsp; Optional.

\- \*\*callbackConfig\*\* \_object\_

&nbsp; Configuration for callback.

&nbsp; Optional.

&nbsp; - \*\*url\*\* \_string\_

&nbsp; - \*\*method\*\* \_'POST' | 'PUT'\_

\- \*\*subtitles\*\* \_boolean\_

&nbsp; Generate subtitles from the transcription.

&nbsp; Optional.

\- \*\*subtitlesConfig\*\* \_object\_

&nbsp; Configuration for subtitles.

&nbsp; Optional.

&nbsp; - \*\*formats\*\* \_Array\&lt;'srt' | 'vtt'\&gt;\_

&nbsp; - \*\*minimumDuration\*\* \_number\_

&nbsp; - \*\*maximumDuration\*\* \_number\_

&nbsp; - \*\*maximumCharactersPerRow\*\* \_number\_

&nbsp; - \*\*maximumRowsPerCaption\*\* \_number\_

&nbsp; - \*\*style\*\* \_'default' | 'compliance'\_

\- \*\*diarization\*\* \_boolean\_

&nbsp; Enable speaker diarization.

&nbsp; Defaults to `true`.

&nbsp; Optional.

\- \*\*diarizationConfig\*\* \_object\_

&nbsp; Configuration for diarization.

&nbsp; Optional.

&nbsp; - \*\*numberOfSpeakers\*\* \_number\_

&nbsp; - \*\*minSpeakers\*\* \_number\_

&nbsp; - \*\*maxSpeakers\*\* \_number\_

&nbsp; - \*\*enhanced\*\* \_boolean\_

\- \*\*translation\*\* \_boolean\_

&nbsp; Enable translation of the transcription.

&nbsp; Optional.

\- \*\*translationConfig\*\* \_object\_

&nbsp; Configuration for translation.

&nbsp; Optional.

&nbsp; - \*\*targetLanguages\*\* \_string\[]\_

&nbsp; - \*\*model\*\* \_'base' | 'enhanced'\_

&nbsp; - \*\*matchOriginalUtterances\*\* \_boolean\_

\- \*\*summarization\*\* \_boolean\_

&nbsp; Enable summarization of the transcription.

&nbsp; Optional.

\- \*\*summarizationConfig\*\* \_object\_

&nbsp; Configuration for summarization.

&nbsp; Optional.

&nbsp; - \*\*type\*\* \_'general' | 'bullet_points' | 'concise'\_

\- \*\*moderation\*\* \_boolean\_

&nbsp; Enable content moderation.

&nbsp; Optional.

\- \*\*namedEntityRecognition\*\* \_boolean\_

&nbsp; Enable named entity recognition.

&nbsp; Optional.

\- \*\*chapterization\*\* \_boolean\_

&nbsp; Enable chapterization of the transcription.

&nbsp; Optional.

\- \*\*nameConsistency\*\* \_boolean\_

&nbsp; Enable name consistency in the transcription.

&nbsp; Optional.

\- \*\*customSpelling\*\* \_boolean\_

&nbsp; Enable custom spelling.

&nbsp; Optional.

\- \*\*customSpellingConfig\*\* \_object\_

&nbsp; Configuration for custom spelling.

&nbsp; Optional.

&nbsp; - \*\*spellingDictionary\*\* \_Record\&lt;string, string\[]\&gt;\_

\- \*\*structuredDataExtraction\*\* \_boolean\_

&nbsp; Enable structured data extraction.

&nbsp; Optional.

\- \*\*structuredDataExtractionConfig\*\* \_object\_

&nbsp; Configuration for structured data extraction.

&nbsp; Optional.

&nbsp; - \*\*classes\*\* \_string\[]\_

\- \*\*sentimentAnalysis\*\* \_boolean\_

&nbsp; Enable sentiment analysis.

&nbsp; Optional.

\- \*\*audioToLlm\*\* \_boolean\_

&nbsp; Enable audio to LLM processing.

&nbsp; Optional.

\- \*\*audioToLlmConfig\*\* \_object\_

&nbsp; Configuration for audio to LLM.

&nbsp; Optional.

&nbsp; - \*\*prompts\*\* \_string\[]\_

\- \*\*customMetadata\*\* \_Record\&lt;string, any\&gt;\_

&nbsp; Custom metadata to include with the request.

&nbsp; Optional.

\- \*\*sentences\*\* \_boolean\_

&nbsp; Enable sentence detection.

&nbsp; Optional.

\- \*\*displayMode\*\* \_boolean\_

&nbsp; Enable display mode.

&nbsp; Optional.

\- \*\*punctuationEnhanced\*\* \_boolean\_

&nbsp; Enable enhanced punctuation.

&nbsp; Optional.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| --------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `Default` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

---

title: LMNT

description: Learn how to use the LMNT provider for the AI SDK.

---

\# LMNT Provider

The \[LMNT](https://lmnt.com/) provider contains language model support for the LMNT transcription API.

\## Setup

The LMNT provider is available in the `@ai-sdk/lmnt` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/lmnt" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/lmnt" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/lmnt" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/lmnt" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `lmnt` from `@ai-sdk/lmnt`:

```ts
import { lmnt } from "@ai-sdk/lmnt";
```

If you need a customized setup, you can import `createLMNT` from `@ai-sdk/lmnt` and create a provider instance with your settings:

```ts

import { createLMNT } from '@ai-sdk/lmnt';



const lmnt = createLMNT({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the LMNT provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `LMNT\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Speech Models

You can create models that call the \[LMNT speech API](https://docs.lmnt.com/api-reference/speech/synthesize-speech-bytes)

using the `.speech()` factory method.

The first argument is the model id e.g. `aurora`.

```ts
const model = lmnt.speech("aurora");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying a voice to use for the generated audio.

```ts highlight="6"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { lmnt } from '@ai-sdk/lmnt';



const result = await generateSpeech({

&nbsp; model: lmnt.speech('aurora'),

&nbsp; text: 'Hello, world!',

&nbsp; language: 'en', // Standardized language parameter

});

```

\### Provider Options

The LMNT provider accepts the following options:

\- \*\*model\*\* \_'aurora' | 'blizzard'\_

&nbsp; The LMNT model to use. Defaults to `'aurora'`.

\- \*\*language\*\* \_'auto' | 'en' | 'es' | 'pt' | 'fr' | 'de' | 'zh' | 'ko' | 'hi' | 'ja' | 'ru' | 'it' | 'tr'\_

&nbsp; The language to use for speech synthesis. Defaults to `'auto'`.

\- \*\*format\*\* \_'aac' | 'mp3' | 'mulaw' | 'raw' | 'wav'\_

&nbsp; The audio format to return. Defaults to `'mp3'`.

\- \*\*sampleRate\*\* \_number\_

&nbsp; The sample rate of the audio in Hz. Defaults to `24000`.

\- \*\*speed\*\* \_number\_

&nbsp; The speed of the speech. Must be between 0.25 and 2. Defaults to `1`.

\- \*\*seed\*\* \_number\_

&nbsp; An optional seed for deterministic generation.

\- \*\*conversational\*\* \_boolean\_

&nbsp; Whether to use a conversational style. Defaults to `false`.

\- \*\*length\*\* \_number\_

&nbsp; Maximum length of the audio in seconds. Maximum value is 300.

\- \*\*topP\*\* \_number\_

&nbsp; Top-p sampling parameter. Must be between 0 and 1. Defaults to `1`.

\- \*\*temperature\*\* \_number\_

&nbsp; Temperature parameter for sampling. Must be at least 0. Defaults to `1`.

\### Model Capabilities

| Model | Instructions |

| ---------- | ------------------- |

| `aurora` | <Check size={18} /> |

| `blizzard` | <Check size={18} /> |

---

title: Google Generative AI

description: Learn how to use Google Generative AI Provider.

---

\# Google Generative AI Provider

The \[Google Generative AI](https://ai.google.dev) provider contains language and embedding model support for

the \[Google Generative AI](https://ai.google.dev/api/rest) APIs.

\## Setup

The Google provider is available in the `@ai-sdk/google` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/google" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/google" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/google" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/google" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `google` from `@ai-sdk/google`:

```ts
import { google } from "@ai-sdk/google";
```

If you need a customized setup, you can import `createGoogleGenerativeAI` from `@ai-sdk/google` and create a provider instance with your settings:

```ts

import { createGoogleGenerativeAI } from '@ai-sdk/google';



const google = createGoogleGenerativeAI({

&nbsp; // custom settings

});

```

You can use the following optional settings to customize the Google Generative AI provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://generativelanguage.googleapis.com/v1beta`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `x-goog-api-key` header.

&nbsp; It defaults to the `GOOGLE\_GENERATIVE\_AI\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create models that call the \[Google Generative AI API](https://ai.google.dev/api/rest) using the provider instance.

The first argument is the model id, e.g. `gemini-2.5-flash`.

The models support tool calls and some have multi-modal capabilities.

```ts
const model = google("gemini-2.5-flash");
```

You can use Google Generative AI language models to generate text with the `generateText` function:

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Google Generative AI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

Google Generative AI also supports some model specific settings that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them as an options argument:

```ts

const model = google('gemini-2.5-flash');



await generateText({

&nbsp; model,

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     safetySettings: \[

&nbsp;       {

&nbsp;         category: 'HARM\_CATEGORY\_UNSPECIFIED',

&nbsp;         threshold: 'BLOCK\_LOW\_AND\_ABOVE',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Google Generative AI models:

\- \*\*cachedContent\*\* \_string\_

&nbsp; Optional. The name of the cached content used as context to serve the prediction.

&nbsp; Format: cachedContents/\\{cachedContent\\}

\- \*\*structuredOutputs\*\* \_boolean\_

&nbsp; Optional. Enable structured output. Default is true.

&nbsp; This is useful when the JSON Schema contains elements that are

&nbsp; not supported by the OpenAPI schema version that

&nbsp; Google Generative AI uses. You can use this to disable

&nbsp; structured outputs if you need to.

&nbsp; See \[Troubleshooting: Schema Limitations](#schema-limitations) for more details.

\- \*\*safetySettings\*\* \_Array\\<\\{ category: string; threshold: string \\}\\>\_

&nbsp; Optional. Safety settings for the model.

&nbsp; - \*\*category\*\* \_string\_

&nbsp; The category of the safety setting. Can be one of the following:

&nbsp; - `HARM\_CATEGORY\_HATE\_SPEECH`

&nbsp; - `HARM\_CATEGORY\_DANGEROUS\_CONTENT`

&nbsp; - `HARM\_CATEGORY\_HARASSMENT`

&nbsp; - `HARM\_CATEGORY\_SEXUALLY\_EXPLICIT`

&nbsp; - \*\*threshold\*\* \_string\_

&nbsp; The threshold of the safety setting. Can be one of the following:

&nbsp; - `HARM\_BLOCK\_THRESHOLD\_UNSPECIFIED`

&nbsp; - `BLOCK\_LOW\_AND\_ABOVE`

&nbsp; - `BLOCK\_MEDIUM\_AND\_ABOVE`

&nbsp; - `BLOCK\_ONLY\_HIGH`

&nbsp; - `BLOCK\_NONE`

\- \*\*responseModalities\*\* \_string\[]\_

&nbsp; The modalities to use for the response. The following modalities are supported: `TEXT`, `IMAGE`. When not defined or empty, the model defaults to returning only text.

\- \*\*thinkingConfig\*\* \_\\{ thinkingLevel?: 'minimal' | 'low' | 'medium' | 'high'; thinkingBudget?: number; includeThoughts?: boolean \\}\_

&nbsp; Optional. Configuration for the model's thinking process. Only supported by specific \[Google Generative AI models](https://ai.google.dev/gemini-api/docs/thinking).

&nbsp; - \*\*thinkingLevel\*\* \_'minimal' | 'low' | 'medium' | 'high'\_

&nbsp; Optional. Controls the thinking depth for Gemini 3 models. Gemini 3 Pro supports 'low' and 'high', while Gemini 3 Flash supports all four levels: 'minimal', 'low', 'medium', and 'high'. Only supported by Gemini 3 models (`gemini-3-pro-preview` and later).

&nbsp; - \*\*thinkingBudget\*\* \_number\_

&nbsp; Optional. Gives the model guidance on the number of thinking tokens it can use when generating a response. Setting it to 0 disables thinking, if the model supports it.

&nbsp; For more information about the possible value ranges for each model see \[Google Generative AI thinking documentation](https://ai.google.dev/gemini-api/docs/thinking#set-budget).

&nbsp; <Note>

&nbsp; This option is for Gemini 2.5 models. Gemini 3 models should use

&nbsp; `thinkingLevel` instead.

&nbsp; </Note>

&nbsp; - \*\*includeThoughts\*\* \_boolean\_

&nbsp; Optional. If set to true, thought summaries are returned, which are synthisized versions of the model's raw thoughts and offer insights into the model's internal reasoning process.

\- \*\*imageConfig\*\* \_\\{ aspectRatio: string \\}\_

&nbsp; Optional. Configuration for the models image generation. Only supported by specific \[Google Generative AI models](https://ai.google.dev/gemini-api/docs/image-generation).

&nbsp; - \*\*aspectRatio\*\* \_string\_

&nbsp; Model defaults to generate 1:1 squares, or to matching the output image size to that of your input image. Can be one of the following:

&nbsp; - 1:1

&nbsp; - 2:3

&nbsp; - 3:2

&nbsp; - 3:4

&nbsp; - 4:3

&nbsp; - 4:5

&nbsp; - 5:4

&nbsp; - 9:16

&nbsp; - 16:9

&nbsp; - 21:9

\### Thinking

The Gemini 2.5 and Gemini 3 series models use an internal "thinking process" that significantly improves their reasoning and multi-step planning abilities, making them highly effective for complex tasks such as coding, advanced mathematics, and data analysis. For more information see \[Google Generative AI thinking documentation](https://ai.google.dev/gemini-api/docs/thinking).

\#### Gemini 3 Models

For Gemini 3 models, use the `thinkingLevel` parameter to control the depth of reasoning:

```ts

import { google, GoogleGenerativeAIProviderOptions } from '@ai-sdk/google';

import { generateText } from 'ai';



const model = google('gemini-3-pro-preview');



const { text, reasoning } = await generateText({

&nbsp; model: model,

&nbsp; prompt: 'What is the sum of the first 10 prime numbers?',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     thinkingConfig: {

&nbsp;       thinkingLevel: 'high',

&nbsp;       includeThoughts: true,

&nbsp;     },

&nbsp;   } satisfies GoogleGenerativeAIProviderOptions,

&nbsp; },

});



console.log(text);



console.log(reasoning); // Reasoning summary

```

\#### Gemini 2.5 Models

For Gemini 2.5 models, use the `thinkingBudget` parameter to control the number of thinking tokens:

```ts

import { google, GoogleGenerativeAIProviderOptions } from '@ai-sdk/google';

import { generateText } from 'ai';



const model = google('gemini-2.5-flash');



const { text, reasoning } = await generateText({

&nbsp; model: model,

&nbsp; prompt: 'What is the sum of the first 10 prime numbers?',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     thinkingConfig: {

&nbsp;       thinkingBudget: 8192,

&nbsp;       includeThoughts: true,

&nbsp;     },

&nbsp;   } satisfies GoogleGenerativeAIProviderOptions,

&nbsp; },

});



console.log(text);



console.log(reasoning); // Reasoning summary

```

\### File Inputs

The Google Generative AI provider supports file inputs, e.g. PDF files.

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model according to this document?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: fs.readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

You can also use YouTube URLs directly:

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'Summarize this video',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: 'https://www.youtube.com/watch?v=dQw4w9WgXcQ',

&nbsp;         mediaType: 'video/mp4',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

<Note>

&nbsp; The AI SDK will automatically download URLs if you pass them as data, except

&nbsp; for `https://generativelanguage.googleapis.com/v1beta/files/` and YouTube

&nbsp; URLs. You can use the Google Generative AI Files API to upload larger files to

&nbsp; that location. YouTube URLs (public or unlisted videos) are supported directly

&nbsp; - you can specify one YouTube video URL per request.

</Note>

See \[File Parts](/docs/foundations/prompts#file-parts) for details on how to use files in prompts.

\### Cached Content

Google Generative AI supports both explicit and implicit caching to help reduce costs on repetitive content.

\#### Implicit Caching

Gemini 2.5 models automatically provide cache cost savings without needing to create an explicit cache. When you send requests that share common prefixes with previous requests, you'll receive a 75% token discount on cached content.

To maximize cache hits with implicit caching:

\- Keep content at the beginning of requests consistent

\- Add variable content (like user questions) at the end of prompts

\- Ensure requests meet minimum token requirements:

&nbsp; - Gemini 2.5 Flash: 1024 tokens minimum

&nbsp; - Gemini 2.5 Pro: 2048 tokens minimum

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



// Structure prompts with consistent content at the beginning

const baseContext =

&nbsp; 'You are a cooking assistant with expertise in Italian cuisine. Here are 1000 lasagna recipes for reference...';



const { text: veggieLasagna } = await generateText({

&nbsp; model: google('gemini-2.5-pro'),

&nbsp; prompt: `${baseContext}\\n\\nWrite a vegetarian lasagna recipe for 4 people.`,

});



// Second request with same prefix - eligible for cache hit

const { text: meatLasagna, providerMetadata } = await generateText({

&nbsp; model: google('gemini-2.5-pro'),

&nbsp; prompt: `${baseContext}\\n\\nWrite a meat lasagna recipe for 12 people.`,

});



// Check cached token count in usage metadata

console.log('Cached tokens:', providerMetadata.google?.usageMetadata);

// e.g.

// {

//   groundingMetadata: null,

//   safetyRatings: null,

//   usageMetadata: {

//     cachedContentTokenCount: 2027,

//     thoughtsTokenCount: 702,

//     promptTokenCount: 2152,

//     candidatesTokenCount: 710,

//     totalTokenCount: 3564

//   }

// }

```

<Note>

&nbsp; Usage metadata was added to `providerMetadata` in `@ai-sdk/google@1.2.23`. If

&nbsp; you are using an older version, usage metadata is available in the raw HTTP

&nbsp; `response` body returned as part of the return value from `generateText`.

</Note>

\#### Explicit Caching

For guaranteed cost savings, you can still use explicit caching with Gemini 2.5 and 2.0 models. See the \[models page](https://ai.google.dev/gemini-api/docs/models) to check if caching is supported for the used model:

```ts

import { google } from '@ai-sdk/google';

import { GoogleAICacheManager } from '@google/generative-ai/server';

import { generateText } from 'ai';



const cacheManager = new GoogleAICacheManager(

&nbsp; process.env.GOOGLE\_GENERATIVE\_AI\_API\_KEY,

);



const model = 'gemini-2.5-pro';



const { name: cachedContent } = await cacheManager.create({

&nbsp; model,

&nbsp; contents: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     parts: \[{ text: '1000 Lasagna Recipes...' }],

&nbsp;   },

&nbsp; ],

&nbsp; ttlSeconds: 60 \* 5,

});



const { text: veggieLasangaRecipe } = await generateText({

&nbsp; model: google(model),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     cachedContent,

&nbsp;   },

&nbsp; },

});



const { text: meatLasangaRecipe } = await generateText({

&nbsp; model: google(model),

&nbsp; prompt: 'Write a meat lasagna recipe for 12 people.',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     cachedContent,

&nbsp;   },

&nbsp; },

});

```

\### Code Execution

With \[Code Execution](https://ai.google.dev/gemini-api/docs/code-execution), certain models can generate and execute Python code to perform calculations, solve problems, or provide more accurate information.

You can enable code execution by adding the `code\_execution` tool to your request.

```ts

import { google } from '@ai-sdk/google';

import { googleTools } from '@ai-sdk/google/internal';

import { generateText } from 'ai';



const { text, toolCalls, toolResults } = await generateText({

&nbsp; model: google('gemini-2.5-pro'),

&nbsp; tools: { code\_execution: google.tools.codeExecution({}) },

&nbsp; prompt: 'Use python to calculate the 20th fibonacci number.',

});

```

The response will contain the tool calls and results from the code execution.

\### Google Search

With \[search grounding](https://ai.google.dev/gemini-api/docs/google-search),

the model has access to the latest information using Google search.

Google search can be used to provide answers around current events:

```ts highlight="8,17-20"

import { google } from '@ai-sdk/google';

import { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text, sources, providerMetadata } = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; tools: {

&nbsp;   google\_search: google.tools.googleSearch({}),

&nbsp; },

&nbsp; prompt:

&nbsp;   'List the top 5 San Francisco news from the past week.' +

&nbsp;   'You must include the date of each article.',

});



// access the grounding metadata. Casting to the provider metadata type

// is optional but provides autocomplete and type safety.

const metadata = providerMetadata?.google as

&nbsp; | GoogleGenerativeAIProviderMetadata

&nbsp; | undefined;

const groundingMetadata = metadata?.groundingMetadata;

const safetyRatings = metadata?.safetyRatings;

```

When Search Grounding is enabled, the model will include sources in the response.

Additionally, the grounding metadata includes detailed information about how search results were used to ground the model's response. Here are the available fields:

\- \*\*`webSearchQueries`\*\* (`string\[] | null`)

&nbsp; - Array of search queries used to retrieve information

&nbsp; - Example: `\["What's the weather in Chicago this weekend?"]`

\- \*\*`searchEntryPoint`\*\* (`{ renderedContent: string } | null`)

&nbsp; - Contains the main search result content used as an entry point

&nbsp; - The `renderedContent` field contains the formatted content

\- \*\*`groundingSupports`\*\* (Array of support objects | null)

&nbsp; - Contains details about how specific response parts are supported by search results

&nbsp; - Each support object includes:

&nbsp; - \*\*`segment`\*\*: Information about the grounded text segment

&nbsp; - `text`: The actual text segment

&nbsp; - `startIndex`: Starting position in the response

&nbsp; - `endIndex`: Ending position in the response

&nbsp; - \*\*`groundingChunkIndices`\*\*: References to supporting search result chunks

&nbsp; - \*\*`confidenceScores`\*\*: Confidence scores (0-1) for each supporting chunk

Example response:

```json

{

&nbsp; "groundingMetadata": {

&nbsp;   "webSearchQueries": \["What's the weather in Chicago this weekend?"],

&nbsp;   "searchEntryPoint": {

&nbsp;     "renderedContent": "..."

&nbsp;   },

&nbsp;   "groundingSupports": \[

&nbsp;     {

&nbsp;       "segment": {

&nbsp;         "startIndex": 0,

&nbsp;         "endIndex": 65,

&nbsp;         "text": "Chicago weather changes rapidly, so layers let you adjust easily."

&nbsp;       },

&nbsp;       "groundingChunkIndices": \[0],

&nbsp;       "confidenceScores": \[0.99]

&nbsp;     }

&nbsp;   ]

&nbsp; }

}

```

\### File Search

The \[File Search tool](https://ai.google.dev/gemini-api/docs/file-search) lets Gemini retrieve context from your own documents that you have indexed in File Search stores. Only Gemini 2.5 and Gemini 3 models support this feature.

```ts highlight="9-13"

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text, sources } = await generateText({

&nbsp; model: google('gemini-2.5-pro'),

&nbsp; tools: {

&nbsp;   file\_search: google.tools.fileSearch({

&nbsp;     fileSearchStoreNames: \[

&nbsp;       'projects/my-project/locations/us/fileSearchStores/my-store',

&nbsp;     ],

&nbsp;     metadataFilter: 'author = "Robert Graves"',

&nbsp;     topK: 8,

&nbsp;   }),

&nbsp; },

&nbsp; prompt: "Summarise the key themes of 'I, Claudius'.",

});

```

File Search responses include citations via the normal `sources` field and expose raw \[grounding metadata](#google-search) in `providerMetadata.google.groundingMetadata`.

\### URL Context

Google provides a provider-defined URL context tool.

The URL context tool allows you to provide specific URLs that you want the model to analyze directly in from the prompt.

```ts highlight="9,13-17"

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text, sources, providerMetadata } = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; prompt: `Based on the document: https://ai.google.dev/gemini-api/docs/url-context.

&nbsp;         Answer this question: How many links we can consume in one request?`,

&nbsp; tools: {

&nbsp;   url\_context: google.tools.urlContext({}),

&nbsp; },

});



const metadata = providerMetadata?.google as

&nbsp; | GoogleGenerativeAIProviderMetadata

&nbsp; | undefined;

const groundingMetadata = metadata?.groundingMetadata;

const urlContextMetadata = metadata?.urlContextMetadata;

```

The URL context metadata includes detailed information about how the model used the URL context to generate the response. Here are the available fields:

\- \*\*`urlMetadata`\*\* (`{ retrievedUrl: string; urlRetrievalStatus: string; }\[] | null`)

&nbsp; - Array of URL context metadata

&nbsp; - Each object includes:

&nbsp; - \*\*`retrievedUrl`\*\*: The URL of the context

&nbsp; - \*\*`urlRetrievalStatus`\*\*: The status of the URL retrieval

Example response:

```json

{

&nbsp; "urlMetadata": \[

&nbsp;   {

&nbsp;     "retrievedUrl": "https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai",

&nbsp;     "urlRetrievalStatus": "URL\_RETRIEVAL\_STATUS\_SUCCESS"

&nbsp;   }

&nbsp; ]

}

```

With the URL context tool, you will also get the `groundingMetadata`.

```json

"groundingMetadata": {

&nbsp;   "groundingChunks": \[

&nbsp;       {

&nbsp;           "web": {

&nbsp;               "uri": "https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai",

&nbsp;               "title": "Google Generative AI - AI SDK Providers"

&nbsp;           }

&nbsp;       }

&nbsp;   ],

&nbsp;   "groundingSupports": \[

&nbsp;       {

&nbsp;           "segment": {

&nbsp;               "startIndex": 67,

&nbsp;               "endIndex": 157,

&nbsp;               "text": "\*\*Installation\*\*: Install the `@ai-sdk/google` module using your preferred package manager"

&nbsp;           },

&nbsp;           "groundingChunkIndices": \[

&nbsp;               0

&nbsp;           ]

&nbsp;       },

&nbsp;   ]

}

```

<Note>You can add up to 20 URLs per request.</Note>

<Note>

&nbsp; The URL context tool is only supported for Gemini 2.0 Flash models and above.

&nbsp; Check the \[supported models for URL context

&nbsp; tool](https://ai.google.dev/gemini-api/docs/url-context#supported-models).

</Note>

\#### Combine URL Context with Search Grounding

You can combine the URL context tool with search grounding to provide the model with the latest information from the web.

```ts highlight="9-10"

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text, sources, providerMetadata } = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; prompt: `Based on this context: https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai, tell me how to use Gemini with AI SDK.

&nbsp;   Also, provide the latest news about AI SDK V5.`,

&nbsp; tools: {

&nbsp;   google\_search: google.tools.googleSearch({}),

&nbsp;   url\_context: google.tools.urlContext({}),

&nbsp; },

});



const metadata = providerMetadata?.google as

&nbsp; | GoogleGenerativeAIProviderMetadata

&nbsp; | undefined;

const groundingMetadata = metadata?.groundingMetadata;

const urlContextMetadata = metadata?.urlContextMetadata;

```

\### Google Maps Grounding

With \[Google Maps grounding](https://ai.google.dev/gemini-api/docs/maps-grounding),

the model has access to Google Maps data for location-aware responses. This enables providing local data and geospatial context, such as finding nearby restaurants.

```ts highlight="7-16"

import { google } from '@ai-sdk/google';

import { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text, sources, providerMetadata } = await generateText({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; tools: {

&nbsp;   google\_maps: google.tools.googleMaps({}),

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     retrievalConfig: {

&nbsp;       latLng: { latitude: 34.090199, longitude: -117.881081 },

&nbsp;     },

&nbsp;   },

&nbsp; },

&nbsp; prompt:

&nbsp;   'What are the best Italian restaurants within a 15-minute walk from here?',

});



const metadata = providerMetadata?.google as

&nbsp; | GoogleGenerativeAIProviderMetadata

&nbsp; | undefined;

const groundingMetadata = metadata?.groundingMetadata;

```

The optional `retrievalConfig.latLng` provider option provides location context for queries about nearby places. This configuration applies to any grounding tools that support location context, including Google Maps and Google Search.

When Google Maps grounding is enabled, the model's response will include sources pointing to Google Maps URLs. The grounding metadata includes `maps` chunks with place information:

```json

{

&nbsp; "groundingMetadata": {

&nbsp;   "groundingChunks": \[

&nbsp;     {

&nbsp;       "maps": {

&nbsp;         "uri": "https://maps.google.com/?cid=12345",

&nbsp;         "title": "Restaurant Name",

&nbsp;         "placeId": "places/ChIJ..."

&nbsp;       }

&nbsp;     }

&nbsp;   ]

&nbsp; }

}

```

<Note>Google Maps grounding is supported on Gemini 2.0 and newer models.</Note>

\### RAG Engine Grounding

With \[RAG Engine Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-vertexai-search#generate-content-using-gemini-api),

the model has access to your custom knowledge base using the Vertex RAG Engine.

This enables the model to provide answers based on your specific data sources and documents.

<Note>

&nbsp; RAG Engine Grounding is only supported with Vertex Gemini models. You must use

&nbsp; the Google Vertex provider (`@ai-sdk/google-vertex`) instead of the standard

&nbsp; Google provider (`@ai-sdk/google`) to use this feature.

</Note>

```ts highlight="8,17-20"

import { createVertex } from '@ai-sdk/google-vertex';

import { GoogleGenerativeAIProviderMetadata } from '@ai-sdk/google';

import { generateText } from 'ai';



const vertex = createVertex({

&nbsp; project: 'my-project',

&nbsp; location: 'us-central1',

});



const { text, sources, providerMetadata } = await generateText({

&nbsp; model: vertex('gemini-2.5-flash'),

&nbsp; tools: {

&nbsp;   vertex\_rag\_store: vertex.tools.vertexRagStore({

&nbsp;     ragCorpus:

&nbsp;       'projects/my-project/locations/us-central1/ragCorpora/my-rag-corpus',

&nbsp;     topK: 5,

&nbsp;   }),

&nbsp; },

&nbsp; prompt:

&nbsp;   'What are the key features of our product according to our documentation?',

});



// access the grounding metadata. Casting to the provider metadata type

// is optional but provides autocomplete and type safety.

const metadata = providerMetadata?.google as

&nbsp; | GoogleGenerativeAIProviderMetadata

&nbsp; | undefined;

const groundingMetadata = metadata?.groundingMetadata;

const safetyRatings = metadata?.safetyRatings;

```

When RAG Engine Grounding is enabled, the model will include sources from your RAG corpus in the response.

Additionally, the grounding metadata includes detailed information about how RAG results were used to ground the model's response. Here are the available fields:

\- \*\*`groundingChunks`\*\* (Array of chunk objects | null)

&nbsp; - Contains the retrieved context chunks from your RAG corpus

&nbsp; - Each chunk includes:

&nbsp; - \*\*`retrievedContext`\*\*: Information about the retrieved context

&nbsp; - `uri`: The URI or identifier of the source document

&nbsp; - `title`: The title of the source document (optional)

&nbsp; - `text`: The actual text content of the chunk

\- \*\*`groundingSupports`\*\* (Array of support objects | null)

&nbsp; - Contains details about how specific response parts are supported by RAG results

&nbsp; - Each support object includes:

&nbsp; - \*\*`segment`\*\*: Information about the grounded text segment

&nbsp; - `text`: The actual text segment

&nbsp; - `startIndex`: Starting position in the response

&nbsp; - `endIndex`: Ending position in the response

&nbsp; - \*\*`groundingChunkIndices`\*\*: References to supporting RAG result chunks

&nbsp; - \*\*`confidenceScores`\*\*: Confidence scores (0-1) for each supporting chunk

Example response:

```json

{

&nbsp; "groundingMetadata": {

&nbsp;   "groundingChunks": \[

&nbsp;     {

&nbsp;       "retrievedContext": {

&nbsp;         "uri": "gs://my-bucket/docs/product-guide.pdf",

&nbsp;         "title": "Product User Guide",

&nbsp;         "text": "Our product includes advanced AI capabilities, real-time processing, and enterprise-grade security features."

&nbsp;       }

&nbsp;     }

&nbsp;   ],

&nbsp;   "groundingSupports": \[

&nbsp;     {

&nbsp;       "segment": {

&nbsp;         "startIndex": 0,

&nbsp;         "endIndex": 45,

&nbsp;         "text": "Our product includes advanced AI capabilities and real-time processing."

&nbsp;       },

&nbsp;       "groundingChunkIndices": \[0],

&nbsp;       "confidenceScores": \[0.95]

&nbsp;     }

&nbsp;   ]

&nbsp; }

}

```

\#### Configuration Options

The `vertexRagStore` tool accepts the following configuration options:

\- \*\*`ragCorpus`\*\* (`string`, required)

&nbsp; - The RagCorpus resource name in the format: `projects/{project}/locations/{location}/ragCorpora/{rag\_corpus}`

&nbsp; - This identifies your specific RAG corpus to search against

\- \*\*`topK`\*\* (`number`, optional)

&nbsp; - The number of top contexts to retrieve from your RAG corpus

&nbsp; - Defaults to the corpus configuration if not specified

\### Image Outputs

Gemini models with image generation capabilities (`gemini-2.5-flash-image-preview`) support image generation. Images are exposed as files in the response.

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: google('gemini-2.5-flash-image-preview'),

&nbsp; prompt:

&nbsp;   'Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme',

});



for (const file of result.files) {

&nbsp; if (file.mediaType.startsWith('image/')) {

&nbsp;   console.log('Generated image:', file);

&nbsp; }

}

```

\### Safety Ratings

The safety ratings provide insight into the safety of the model's response.

See \[Google AI documentation on safety settings](https://ai.google.dev/gemini-api/docs/safety-settings).

Example response excerpt:

```json

{

&nbsp; "safetyRatings": \[

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_HATE\_SPEECH",

&nbsp;     "probability": "NEGLIGIBLE",

&nbsp;     "probabilityScore": 0.11027937,

&nbsp;     "severity": "HARM\_SEVERITY\_LOW",

&nbsp;     "severityScore": 0.28487435

&nbsp;   },

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_DANGEROUS\_CONTENT",

&nbsp;     "probability": "HIGH",

&nbsp;     "blocked": true,

&nbsp;     "probabilityScore": 0.95422274,

&nbsp;     "severity": "HARM\_SEVERITY\_MEDIUM",

&nbsp;     "severityScore": 0.43398145

&nbsp;   },

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_HARASSMENT",

&nbsp;     "probability": "NEGLIGIBLE",

&nbsp;     "probabilityScore": 0.11085559,

&nbsp;     "severity": "HARM\_SEVERITY\_NEGLIGIBLE",

&nbsp;     "severityScore": 0.19027223

&nbsp;   },

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_SEXUALLY\_EXPLICIT",

&nbsp;     "probability": "NEGLIGIBLE",

&nbsp;     "probabilityScore": 0.22901751,

&nbsp;     "severity": "HARM\_SEVERITY\_NEGLIGIBLE",

&nbsp;     "severityScore": 0.09089675

&nbsp;   }

&nbsp; ]

}

```

\### Troubleshooting

\#### Schema Limitations

The Google Generative AI API uses a subset of the OpenAPI 3.0 schema,

which does not support features such as unions.

The errors that you get in this case look like this:

`GenerateContentRequest.generation\_config.response\_schema.properties\[occupation].type: must be specified`

By default, structured outputs are enabled (and for tool calling they are required).

You can disable structured outputs for object generation as a workaround:

```ts highlight="3,8"

const { object } = await generateObject({

&nbsp; model: google('gemini-2.5-flash'),

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     structuredOutputs: false,

&nbsp;   },

&nbsp; },

&nbsp; schema: z.object({

&nbsp;   name: z.string(),

&nbsp;   age: z.number(),

&nbsp;   contact: z.union(\[

&nbsp;     z.object({

&nbsp;       type: z.literal('email'),

&nbsp;       value: z.string(),

&nbsp;     }),

&nbsp;     z.object({

&nbsp;       type: z.literal('phone'),

&nbsp;       value: z.string(),

&nbsp;     }),

&nbsp;   ]),

&nbsp; }),

&nbsp; prompt: 'Generate an example person for testing.',

});

```

The following Zod features are known to not work with Google Generative AI:

\- `z.union`

\- `z.record`

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming | Google Search | URL Context |

| ------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `gemini-3-pro-preview` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.5-pro` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.5-flash` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.5-flash-lite` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.5-flash-lite-preview-06-17` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.0-flash` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-1.5-pro` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `gemini-1.5-pro-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `gemini-1.5-flash` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `gemini-1.5-flash-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `gemini-1.5-flash-8b` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `gemini-1.5-flash-8b-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Google Generative AI

&nbsp; docs](https://ai.google.dev/gemini-api/docs/models/) for a full list of

&nbsp; available models. The table above lists popular models. You can also pass any

&nbsp; available provider model ID as a string if needed.

</Note>

\## Gemma Models

You can use \[Gemma models](https://deepmind.google/models/gemma/) with the Google Generative AI API.

Gemma models don't natively support the `systemInstruction` parameter, but the provider automatically handles system instructions by prepending them to the first user message. This allows you to use system instructions with Gemma models seamlessly:

```ts

import { google } from '@ai-sdk/google';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: google('gemma-3-27b-it'),

&nbsp; system: 'You are a helpful assistant that responds concisely.',

&nbsp; prompt: 'What is machine learning?',

});

```

The system instruction is automatically formatted and included in the conversation, so Gemma models can follow the guidance without any additional configuration.

\## Embedding Models

You can create models that call the \[Google Generative AI embeddings API](https://ai.google.dev/gemini-api/docs/embeddings)

using the `.embedding()` factory method.

```ts
const model = google.embedding("gemini-embedding-001");
```

The Google Generative AI provider sends API calls to the right endpoint based on the type of embedding:

\- \*\*Single embeddings\*\*: When embedding a single value with `embed()`, the provider uses the single `:embedContent` endpoint, which typically has higher rate limits compared to the batch endpoint.

\- \*\*Batch embeddings\*\*: When embedding multiple values with `embedMany()` or multiple values in `embed()`, the provider uses the `:batchEmbedContents` endpoint.

Google Generative AI embedding models support aditional settings. You can pass them as an options argument:

```ts

import { google } from '@ai-sdk/google';

import { embed } from 'ai';



const model = google.embedding('gemini-embedding-001');



const { embedding } = await embed({

&nbsp; model,

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     outputDimensionality: 512, // optional, number of dimensions for the embedding

&nbsp;     taskType: 'SEMANTIC\_SIMILARITY', // optional, specifies the task type for generating embeddings

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Google Generative AI embedding models:

\- \*\*outputDimensionality\*\*: \_number\_

&nbsp; Optional reduced dimension for the output embedding. If set, excessive values in the output embedding are truncated from the end.

\- \*\*taskType\*\*: \_string\_

&nbsp; Optional. Specifies the task type for generating embeddings. Supported task types include:

&nbsp; - `SEMANTIC\_SIMILARITY`: Optimized for text similarity.

&nbsp; - `CLASSIFICATION`: Optimized for text classification.

&nbsp; - `CLUSTERING`: Optimized for clustering texts based on similarity.

&nbsp; - `RETRIEVAL\_DOCUMENT`: Optimized for document retrieval.

&nbsp; - `RETRIEVAL\_QUERY`: Optimized for query-based retrieval.

&nbsp; - `QUESTION\_ANSWERING`: Optimized for answering questions.

&nbsp; - `FACT\_VERIFICATION`: Optimized for verifying factual information.

&nbsp; - `CODE\_RETRIEVAL\_QUERY`: Optimized for retrieving code blocks based on natural language queries.

\### Model Capabilities

| Model | Default Dimensions | Custom Dimensions |

| ---------------------- | ------------------ | ------------------- |

| `gemini-embedding-001` | 3072 | <Check size={18} /> |

| `text-embedding-004` | 768 | <Check size={18} /> |

\## Image Models

You can create \[Imagen](https://ai.google.dev/gemini-api/docs/imagen) models that call the Google Generative AI API using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

```ts

import { google } from '@ai-sdk/google';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: google.image('imagen-4.0-generate-001'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; aspectRatio: '16:9',

});

```

Further configuration can be done using Google provider options. You can validate the provider options using the `GoogleGenerativeAIImageProviderOptions` type.

```ts

import { google } from '@ai-sdk/google';

import { GoogleGenerativeAIImageProviderOptions } from '@ai-sdk/google';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: google.image('imagen-4.0-generate-001'),

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     personGeneration: 'dont\_allow',

&nbsp;   } satisfies GoogleGenerativeAIImageProviderOptions,

&nbsp; },

&nbsp; // ...

});

```

The following provider options are available:

\- \*\*personGeneration\*\* `allow\_adult` | `allow\_all` | `dont\_allow`

&nbsp; Whether to allow person generation. Defaults to `allow\_adult`.

<Note>

&nbsp; Imagen models do not support the `size` parameter. Use the `aspectRatio`

&nbsp; parameter instead.

</Note>

\#### Model Capabilities

| Model | Aspect Ratios |

| ------------------------- | ------------------------- |

| `imagen-4.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

---

title: Hume

description: Learn how to use the Hume provider for the AI SDK.

---

\# Hume Provider

The \[Hume](https://hume.ai/) provider contains language model support for the Hume transcription API.

\## Setup

The Hume provider is available in the `@ai-sdk/hume` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/hume" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/hume" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/hume" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/hume" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `hume` from `@ai-sdk/hume`:

```ts
import { hume } from "@ai-sdk/hume";
```

If you need a customized setup, you can import `createHume` from `@ai-sdk/hume` and create a provider instance with your settings:

```ts

import { createHume } from '@ai-sdk/hume';



const hume = createHume({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the Hume provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `HUME\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Speech Models

You can create models that call the \[Hume speech API](https://dev.hume.ai/docs/text-to-speech-tts/overview)

using the `.speech()` factory method.

```ts
const model = hume.speech();
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying a voice to use for the generated audio.

```ts highlight="6"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { hume } from '@ai-sdk/hume';



const result = await generateSpeech({

&nbsp; model: hume.speech(),

&nbsp; text: 'Hello, world!',

&nbsp; voice: 'd8ab67c6-953d-4bd8-9370-8fa53a0f1453',

&nbsp; providerOptions: { hume: {} },

});

```

The following provider options are available:

\- \*\*context\*\* \_object\_

&nbsp; Either:

&nbsp; - `{ generationId: string }` - A generation ID to use for context.

&nbsp; - `{ utterances: HumeUtterance\[] }` - An array of utterance objects for context.

\### Model Capabilities

| Model | Instructions |

| --------- | ------------------- |

| `default` | <Check size={18} /> |

---

title: Google Vertex AI

description: Learn how to use the Google Vertex AI provider.

---

\# Google Vertex Provider

The Google Vertex provider for the \[AI SDK](/docs) contains language model support for the \[Google Vertex AI](https://cloud.google.com/vertex-ai) APIs. This includes support for \[Google's Gemini models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) and \[Anthropic's Claude partner models](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude).

<Note>

&nbsp; The Google Vertex provider is compatible with both Node.js and Edge runtimes.

&nbsp; The Edge runtime is supported through the `@ai-sdk/google-vertex/edge`

&nbsp; sub-module. More details can be found in the \[Google Vertex Edge

&nbsp; Runtime](#google-vertex-edge-runtime) and \[Google Vertex Anthropic Edge

&nbsp; Runtime](#google-vertex-anthropic-edge-runtime) sections below.

</Note>

\## Setup

The Google Vertex and Google Vertex Anthropic providers are both available in the `@ai-sdk/google-vertex` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/google-vertex" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/google-vertex" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet

&nbsp; text="yarn add @ai-sdk/google-vertex @google-cloud/vertexai"

&nbsp; dark

&nbsp; />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/google-vertex" dark />

&nbsp; </Tab>

</Tabs>

\## Google Vertex Provider Usage

The Google Vertex provider instance is used to create model instances that call the Vertex AI API. The models available with this provider include \[Google's Gemini models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models). If you're looking to use \[Anthropic's Claude models](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude), see the \[Google Vertex Anthropic Provider](#google-vertex-anthropic-provider-usage) section below.

\### Provider Instance

You can import the default provider instance `vertex` from `@ai-sdk/google-vertex`:

```ts
import { vertex } from "@ai-sdk/google-vertex";
```

If you need a customized setup, you can import `createVertex` from `@ai-sdk/google-vertex` and create a provider instance with your settings:

```ts

import { createVertex } from '@ai-sdk/google-vertex';



const vertex = createVertex({

&nbsp; project: 'my-project', // optional

&nbsp; location: 'us-central1', // optional

});

```

Google Vertex supports multiple authentication methods depending on your runtime environment and requirements.

\#### Node.js Runtime

The Node.js runtime is the default runtime supported by the AI SDK. It supports all standard Google Cloud authentication options through the \[`google-auth-library`](https://github.com/googleapis/google-auth-library-nodejs?tab=readme-ov-file#ways-to-authenticate). Typical use involves setting a path to a json credentials file in the `GOOGLE\_APPLICATION\_CREDENTIALS` environment variable. The credentials file can be obtained from the \[Google Cloud Console](https://console.cloud.google.com/apis/credentials).

If you want to customize the Google authentication options you can pass them as options to the `createVertex` function, for example:

```ts

import { createVertex } from '@ai-sdk/google-vertex';



const vertex = createVertex({

&nbsp; googleAuthOptions: {

&nbsp;   credentials: {

&nbsp;     client\_email: 'my-email',

&nbsp;     private\_key: 'my-private-key',

&nbsp;   },

&nbsp; },

});

```

\##### Optional Provider Settings

You can use the following optional settings to customize the provider instance:

\- \*\*project\*\* \_string\_

&nbsp; The Google Cloud project ID that you want to use for the API calls.

&nbsp; It uses the `GOOGLE\_VERTEX\_PROJECT` environment variable by default.

\- \*\*location\*\* \_string\_

&nbsp; The Google Cloud location that you want to use for the API calls, e.g. `us-central1`.

&nbsp; It uses the `GOOGLE\_VERTEX\_LOCATION` environment variable by default.

\- \*\*googleAuthOptions\*\* \_object\_

&nbsp; Optional. The Authentication options used by the \[Google Auth Library](https://github.com/googleapis/google-auth-library-nodejs/). See also the \[GoogleAuthOptions](https://github.com/googleapis/google-auth-library-nodejs/blob/08978822e1b7b5961f0e355df51d738e012be392/src/auth/googleauth.ts#L87C18-L87C35) interface.

&nbsp; - \*\*authClient\*\* \_object\_

&nbsp; An `AuthClient` to use.

&nbsp; - \*\*keyFilename\*\* \_string\_

&nbsp; Path to a .json, .pem, or .p12 key file.

&nbsp; - \*\*keyFile\*\* \_string\_

&nbsp; Path to a .json, .pem, or .p12 key file.

&nbsp; - \*\*credentials\*\* \_object\_

&nbsp; Object containing client_email and private_key properties, or the external account client options.

&nbsp; - \*\*clientOptions\*\* \_object\_

&nbsp; Options object passed to the constructor of the client.

&nbsp; - \*\*scopes\*\* \_string | string\[]\_

&nbsp; Required scopes for the desired API request.

&nbsp; - \*\*projectId\*\* \_string\_

&nbsp; Your project ID.

&nbsp; - \*\*universeDomain\*\* \_string\_

&nbsp; The default service domain for a given Cloud universe.

\- \*\*headers\*\* \_Resolvable\&lt;Record\&lt;string, string | undefined\&gt;\&gt;\_

&nbsp; Headers to include in the requests. Can be provided in multiple formats:

&nbsp; - A record of header key-value pairs: `Record<string, string | undefined>`

&nbsp; - A function that returns headers: `() => Record<string, string | undefined>`

&nbsp; - An async function that returns headers: `async () => Record<string, string | undefined>`

&nbsp; - A promise that resolves to headers: `Promise<Record<string, string | undefined>>`

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\- \*\*baseURL\*\* \_string\_

&nbsp; Optional. Base URL for the Google Vertex API calls e.g. to use proxy servers. By default, it is constructed using the location and project:

&nbsp; `https://${location}-aiplatform.googleapis.com/v1/projects/${project}/locations/${location}/publishers/google`

<a id="google-vertex-edge-runtime"></a>

\#### Edge Runtime

Edge runtimes (like Vercel Edge Functions and Cloudflare Workers) are lightweight JavaScript environments that run closer to users at the network edge.

They only provide a subset of the standard Node.js APIs.

For example, direct file system access is not available, and many Node.js-specific libraries

(including the standard Google Auth library) are not compatible.

The Edge runtime version of the Google Vertex provider supports Google's \[Application Default Credentials](https://github.com/googleapis/google-auth-library-nodejs?tab=readme-ov-file#application-default-credentials) through environment variables. The values can be obtained from a json credentials file from the \[Google Cloud Console](https://console.cloud.google.com/apis/credentials).

You can import the default provider instance `vertex` from `@ai-sdk/google-vertex/edge`:

```ts
import { vertex } from "@ai-sdk/google-vertex/edge";
```

<Note>

&nbsp; The `/edge` sub-module is included in the `@ai-sdk/google-vertex` package, so

&nbsp; you don't need to install it separately. You must import from

&nbsp; `@ai-sdk/google-vertex/edge` to differentiate it from the Node.js provider.

</Note>

If you need a customized setup, you can import `createVertex` from `@ai-sdk/google-vertex/edge` and create a provider instance with your settings:

```ts

import { createVertex } from '@ai-sdk/google-vertex/edge';



const vertex = createVertex({

&nbsp; project: 'my-project', // optional

&nbsp; location: 'us-central1', // optional

});

```

For Edge runtime authentication, you'll need to set these environment variables from your Google Default Application Credentials JSON file:

\- `GOOGLE\_CLIENT\_EMAIL`

\- `GOOGLE\_PRIVATE\_KEY`

\- `GOOGLE\_PRIVATE\_KEY\_ID` (optional)

These values can be obtained from a service account JSON file from the \[Google Cloud Console](https://console.cloud.google.com/apis/credentials).

\##### Optional Provider Settings

You can use the following optional settings to customize the provider instance:

\- \*\*project\*\* \_string\_

&nbsp; The Google Cloud project ID that you want to use for the API calls.

&nbsp; It uses the `GOOGLE\_VERTEX\_PROJECT` environment variable by default.

\- \*\*location\*\* \_string\_

&nbsp; The Google Cloud location that you want to use for the API calls, e.g. `us-central1`.

&nbsp; It uses the `GOOGLE\_VERTEX\_LOCATION` environment variable by default.

\- \*\*googleCredentials\*\* \_object\_

&nbsp; Optional. The credentials used by the Edge provider for authentication. These credentials are typically set through environment variables and are derived from a service account JSON file.

&nbsp; - \*\*clientEmail\*\* \_string\_

&nbsp; The client email from the service account JSON file. Defaults to the contents of the `GOOGLE\_CLIENT\_EMAIL` environment variable.

&nbsp; - \*\*privateKey\*\* \_string\_

&nbsp; The private key from the service account JSON file. Defaults to the contents of the `GOOGLE\_PRIVATE\_KEY` environment variable.

&nbsp; - \*\*privateKeyId\*\* \_string\_

&nbsp; The private key ID from the service account JSON file (optional). Defaults to the contents of the `GOOGLE\_PRIVATE\_KEY\_ID` environment variable.

\- \*\*headers\*\* \_Resolvable\&lt;Record\&lt;string, string | undefined\&gt;\&gt;\_

&nbsp; Headers to include in the requests. Can be provided in multiple formats:

&nbsp; - A record of header key-value pairs: `Record<string, string | undefined>`

&nbsp; - A function that returns headers: `() => Record<string, string | undefined>`

&nbsp; - An async function that returns headers: `async () => Record<string, string | undefined>`

&nbsp; - A promise that resolves to headers: `Promise<Record<string, string | undefined>>`

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\#### Express Mode

Express mode provides a simplified authentication method using an API key instead of OAuth or service account credentials. When using express mode, the `project` and `location` settings are not required.

```ts

import { createVertex } from '@ai-sdk/google-vertex';



const vertex = createVertex({

&nbsp; apiKey: process.env.GOOGLE\_VERTEX\_API\_KEY,

});

```

\##### Optional Provider Settings

\- \*\*apiKey\*\* \_string\_

&nbsp; The API key for Google Vertex AI. When provided, the provider uses express mode with API key authentication instead of OAuth.

&nbsp; It uses the `GOOGLE\_VERTEX\_API\_KEY` environment variable by default.

\### Language Models

You can create models that call the Vertex API using the provider instance.

The first argument is the model id, e.g. `gemini-1.5-pro`.

```ts
const model = vertex("gemini-1.5-pro");
```

<Note>

&nbsp; If you are using \[your own

&nbsp; models](https://cloud.google.com/vertex-ai/docs/training-overview), the name

&nbsp; of your model needs to start with `projects/`.

</Note>

Google Vertex models support also some model specific settings that are not part

of the \[standard call settings](/docs/ai-sdk-core/settings). You can pass them as

an options argument:

```ts

const model = vertex('gemini-1.5-pro');



await generateText({

&nbsp; model,

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     safetySettings: \[

&nbsp;       {

&nbsp;         category: 'HARM\_CATEGORY\_UNSPECIFIED',

&nbsp;         threshold: 'BLOCK\_LOW\_AND\_ABOVE',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Google Vertex models:

\- \*\*structuredOutputs\*\* \_boolean\_

&nbsp; Optional. Enable structured output. Default is true.

&nbsp; This is useful when the JSON Schema contains elements that are

&nbsp; not supported by the OpenAPI schema version that

&nbsp; Google Vertex uses. You can use this to disable

&nbsp; structured outputs if you need to.

&nbsp; See \[Troubleshooting: Schema Limitations](#schema-limitations) for more details.

\- \*\*safetySettings\*\* \_Array\\<\\{ category: string; threshold: string \\}\\>\_

&nbsp; Optional. Safety settings for the model.

&nbsp; - \*\*category\*\* \_string\_

&nbsp; The category of the safety setting. Can be one of the following:

&nbsp; - `HARM\_CATEGORY\_UNSPECIFIED`

&nbsp; - `HARM\_CATEGORY\_HATE\_SPEECH`

&nbsp; - `HARM\_CATEGORY\_DANGEROUS\_CONTENT`

&nbsp; - `HARM\_CATEGORY\_HARASSMENT`

&nbsp; - `HARM\_CATEGORY\_SEXUALLY\_EXPLICIT`

&nbsp; - `HARM\_CATEGORY\_CIVIC\_INTEGRITY`

&nbsp; - \*\*threshold\*\* \_string\_

&nbsp; The threshold of the safety setting. Can be one of the following:

&nbsp; - `HARM\_BLOCK\_THRESHOLD\_UNSPECIFIED`

&nbsp; - `BLOCK\_LOW\_AND\_ABOVE`

&nbsp; - `BLOCK\_MEDIUM\_AND\_ABOVE`

&nbsp; - `BLOCK\_ONLY\_HIGH`

&nbsp; - `BLOCK\_NONE`

\- \*\*audioTimestamp\*\* \_boolean\_

&nbsp; Optional. Enables timestamp understanding for audio files. Defaults to false.

&nbsp; This is useful for generating transcripts with accurate timestamps.

&nbsp; Consult \[Google's Documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/audio-understanding) for usage details.

\- \*\*labels\*\* \_object\_

&nbsp; Optional. Defines labels used in billing reports.

&nbsp; Consult \[Google's Documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for usage details.

You can use Google Vertex language models to generate text with the `generateText` function:

```ts highlight="1,4"

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: vertex('gemini-1.5-pro'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Google Vertex language models can also be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

\#### Code Execution

With \[Code Execution](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution), certain Gemini models on Vertex AI can generate and execute Python code. This allows the model to perform calculations, data manipulation, and other programmatic tasks to enhance its responses.

You can enable code execution by adding the `code\_execution` tool to your request.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: vertex('gemini-2.5-pro'),

&nbsp; tools: { code\_execution: vertex.tools.codeExecution({}) },

&nbsp; prompt:

&nbsp;   'Use python to calculate 20th fibonacci number. Then find the nearest palindrome to it.',

});

```

The response will contain `tool-call` and `tool-result` parts for the executed code.

\#### URL Context

URL Context allows Gemini models to retrieve and analyze content from URLs. Supported models: Gemini 2.5 Flash-Lite, 2.5 Pro, 2.5 Flash, 2.0 Flash.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: vertex('gemini-2.5-pro'),

&nbsp; tools: { url\_context: vertex.tools.urlContext({}) },

&nbsp; prompt: 'What are the key points from https://example.com/article?',

});

```

\#### Google Search

Google Search enables Gemini models to access real-time web information. Supported models: Gemini 2.5 Flash-Lite, 2.5 Flash, 2.0 Flash, 2.5 Pro.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: vertex('gemini-2.5-pro'),

&nbsp; tools: { google\_search: vertex.tools.googleSearch({}) },

&nbsp; prompt: 'What are the latest developments in AI?',

});

```

\#### Enterprise Web Search

\[Enterprise Web Search](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/web-grounding-enterprise) provides grounding using a compliance-focused web index designed for highly-regulated industries such as finance, healthcare, and the public sector. Unlike standard Google Search grounding, Enterprise Web Search does not log customer data and supports VPC service controls. Supported models: Gemini 2.0 and newer.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: vertex('gemini-2.5-flash'),

&nbsp; tools: {

&nbsp;   enterprise\_web\_search: vertex.tools.enterpriseWebSearch({}),

&nbsp; },

&nbsp; prompt: 'What are the latest FDA regulations for clinical trials?',

});

```

\#### Google Maps

Google Maps grounding enables Gemini models to access Google Maps data for location-aware responses. Supported models: Gemini 2.5 Flash-Lite, 2.5 Flash, 2.0 Flash, 2.5 Pro, 3.0 Pro.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: vertex('gemini-2.5-flash'),

&nbsp; tools: {

&nbsp;   google\_maps: vertex.tools.googleMaps({}),

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     retrievalConfig: {

&nbsp;       latLng: { latitude: 34.090199, longitude: -117.881081 },

&nbsp;     },

&nbsp;   },

&nbsp; },

&nbsp; prompt: 'What are the best Italian restaurants nearby?',

});

```

The optional `retrievalConfig.latLng` provider option provides location context for queries about nearby places. This configuration applies to any grounding tools that support location context.

\#### Reasoning (Thinking Tokens)

Google Vertex AI, through its support for Gemini models, can also emit "thinking" tokens, representing the model's reasoning process. The AI SDK exposes these as reasoning information.

To enable thinking tokens for compatible Gemini models via Vertex, set `includeThoughts: true` in the `thinkingConfig` provider option. Since the Vertex provider uses the Google provider's underlying language model, these options are passed through `providerOptions.google`:

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { GoogleGenerativeAIProviderOptions } from '@ai-sdk/google'; // Note: importing from @ai-sdk/google

import { generateText, streamText } from 'ai';



// For generateText:

const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: vertex('gemini-2.0-flash-001'), // Or other supported model via Vertex

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     // Options are nested under 'google' for Vertex provider

&nbsp;     thinkingConfig: {

&nbsp;       includeThoughts: true,

&nbsp;       // thinkingBudget: 2048, // Optional

&nbsp;     },

&nbsp;   } satisfies GoogleGenerativeAIProviderOptions,

&nbsp; },

&nbsp; prompt: 'Explain quantum computing in simple terms.',

});



console.log('Reasoning:', reasoningText);

console.log('Reasoning Details:', reasoning);

console.log('Final Text:', text);



// For streamText:

const result = streamText({

&nbsp; model: vertex('gemini-2.0-flash-001'), // Or other supported model via Vertex

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     // Options are nested under 'google' for Vertex provider

&nbsp;     thinkingConfig: {

&nbsp;       includeThoughts: true,

&nbsp;       // thinkingBudget: 2048, // Optional

&nbsp;     },

&nbsp;   } satisfies GoogleGenerativeAIProviderOptions,

&nbsp; },

&nbsp; prompt: 'Explain quantum computing in simple terms.',

});



for await (const part of result.fullStream) {

&nbsp; if (part.type === 'reasoning') {

&nbsp;   process.stdout.write(`THOUGHT: ${part.textDelta}\\n`);

&nbsp; } else if (part.type === 'text-delta') {

&nbsp;   process.stdout.write(part.textDelta);

&nbsp; }

}

```

When `includeThoughts` is true, parts of the API response marked with `thought: true` will be processed as reasoning.

\- In `generateText`, these contribute to the `reasoningText` (string) and `reasoning` (array) fields.

\- In `streamText`, these are emitted as `reasoning` stream parts.

<Note>

&nbsp; Refer to the \[Google Vertex AI documentation on

&nbsp; "thinking"](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking)

&nbsp; for model compatibility and further details.

</Note>

\#### File Inputs

The Google Vertex provider supports file inputs, e.g. PDF files.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: vertex('gemini-1.5-pro'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model according to this document?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: fs.readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

<Note>

&nbsp; The AI SDK will automatically download URLs if you pass them as data, except

&nbsp; for `gs://` URLs. You can use the Google Cloud Storage API to upload larger

&nbsp; files to that location.

</Note>

See \[File Parts](/docs/foundations/prompts#file-parts) for details on how to use files in prompts.

\### Safety Ratings

The safety ratings provide insight into the safety of the model's response.

See \[Google Vertex AI documentation on configuring safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters).

Example response excerpt:

```json

{

&nbsp; "safetyRatings": \[

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_HATE\_SPEECH",

&nbsp;     "probability": "NEGLIGIBLE",

&nbsp;     "probabilityScore": 0.11027937,

&nbsp;     "severity": "HARM\_SEVERITY\_LOW",

&nbsp;     "severityScore": 0.28487435

&nbsp;   },

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_DANGEROUS\_CONTENT",

&nbsp;     "probability": "HIGH",

&nbsp;     "blocked": true,

&nbsp;     "probabilityScore": 0.95422274,

&nbsp;     "severity": "HARM\_SEVERITY\_MEDIUM",

&nbsp;     "severityScore": 0.43398145

&nbsp;   },

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_HARASSMENT",

&nbsp;     "probability": "NEGLIGIBLE",

&nbsp;     "probabilityScore": 0.11085559,

&nbsp;     "severity": "HARM\_SEVERITY\_NEGLIGIBLE",

&nbsp;     "severityScore": 0.19027223

&nbsp;   },

&nbsp;   {

&nbsp;     "category": "HARM\_CATEGORY\_SEXUALLY\_EXPLICIT",

&nbsp;     "probability": "NEGLIGIBLE",

&nbsp;     "probabilityScore": 0.22901751,

&nbsp;     "severity": "HARM\_SEVERITY\_NEGLIGIBLE",

&nbsp;     "severityScore": 0.09089675

&nbsp;   }

&nbsp; ]

}

```

For more details, see the \[Google Vertex AI documentation on grounding with Google Search](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#ground-to-search).

\### Troubleshooting

\#### Schema Limitations

The Google Vertex API uses a subset of the OpenAPI 3.0 schema,

which does not support features such as unions.

The errors that you get in this case look like this:

`GenerateContentRequest.generation\_config.response\_schema.properties\[occupation].type: must be specified`

By default, structured outputs are enabled (and for tool calling they are required).

You can disable structured outputs for object generation as a workaround:

```ts highlight="3,8"

const result = await generateObject({

&nbsp; model: vertex('gemini-1.5-pro'),

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     structuredOutputs: false,

&nbsp;   },

&nbsp; },

&nbsp; schema: z.object({

&nbsp;   name: z.string(),

&nbsp;   age: z.number(),

&nbsp;   contact: z.union(\[

&nbsp;     z.object({

&nbsp;       type: z.literal('email'),

&nbsp;       value: z.string(),

&nbsp;     }),

&nbsp;     z.object({

&nbsp;       type: z.literal('phone'),

&nbsp;       value: z.string(),

&nbsp;     }),

&nbsp;   ]),

&nbsp; }),

&nbsp; prompt: 'Generate an example person for testing.',

});

```

The following Zod features are known to not work with Google Vertex:

\- `z.union`

\- `z.record`

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ---------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `gemini-3-pro-preview` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.5-pro` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.5-flash` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-2.0-flash-001` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-1.5-flash` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gemini-1.5-pro` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Google Vertex AI

&nbsp; docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#supported-models)

&nbsp; for a full list of available models. The table above lists popular models. You

&nbsp; can also pass any available provider model ID as a string if needed.

</Note>

\### Embedding Models

You can create models that call the Google Vertex AI embeddings API using the `.embeddingModel()` factory method:

```ts
const model = vertex.embeddingModel("text-embedding-004");
```

Google Vertex AI embedding models support additional settings. You can pass them as an options argument:

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { embed } from 'ai';



const model = vertex.embeddingModel('text-embedding-004');



const { embedding } = await embed({

&nbsp; model,

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   google: {

&nbsp;     outputDimensionality: 512, // optional, number of dimensions for the embedding

&nbsp;     taskType: 'SEMANTIC\_SIMILARITY', // optional, specifies the task type for generating embeddings

&nbsp;     autoTruncate: false, // optional

&nbsp;   },

&nbsp; },

});

```

The following optional provider options are available for Google Vertex AI embedding models:

\- \*\*outputDimensionality\*\*: \_number\_

&nbsp; Optional reduced dimension for the output embedding. If set, excessive values in the output embedding are truncated from the end.

\- \*\*taskType\*\*: \_string\_

&nbsp; Optional. Specifies the task type for generating embeddings. Supported task types include:

&nbsp; - `SEMANTIC\_SIMILARITY`: Optimized for text similarity.

&nbsp; - `CLASSIFICATION`: Optimized for text classification.

&nbsp; - `CLUSTERING`: Optimized for clustering texts based on similarity.

&nbsp; - `RETRIEVAL\_DOCUMENT`: Optimized for document retrieval.

&nbsp; - `RETRIEVAL\_QUERY`: Optimized for query-based retrieval.

&nbsp; - `QUESTION\_ANSWERING`: Optimized for answering questions.

&nbsp; - `FACT\_VERIFICATION`: Optimized for verifying factual information.

&nbsp; - `CODE\_RETRIEVAL\_QUERY`: Optimized for retrieving code blocks based on natural language queries.

\- \*\*title\*\*: \_string\_

&nbsp; Optional. The title of the document being embedded. This helps the model produce better embeddings by providing additional context. Only valid when `taskType` is set to `'RETRIEVAL\_DOCUMENT'`.

\- \*\*autoTruncate\*\*: \_boolean\_

&nbsp; Optional. When set to `true`, input text will be truncated if it exceeds the maximum length. When set to `false`, an error is returned if the input text is too long. Defaults to `true`.

\#### Model Capabilities

| Model | Max Values Per Call | Parallel Calls |

| -------------------- | ------------------- | ------------------- |

| `text-embedding-004` | 2048 | <Check size={18} /> |

<Note>

&nbsp; The table above lists popular models. You can also pass any available provider

&nbsp; model ID as a string if needed.

</Note>

\### Image Models

You can create \[Imagen](https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview) models that call the \[Imagen on Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/image/generate-images)

using the `.image()` factory method. For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: vertex.image('imagen-4.0-generate-001'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; aspectRatio: '16:9',

});

```

Further configuration can be done using Google Vertex provider options. You can validate the provider options using the `GoogleVertexImageProviderOptions` type.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { GoogleVertexImageProviderOptions } from '@ai-sdk/google-vertex';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: vertex.image('imagen-4.0-generate-001'),

&nbsp; providerOptions: {

&nbsp;   vertex: {

&nbsp;     negativePrompt: 'pixelated, blurry, low-quality',

&nbsp;   } satisfies GoogleVertexImageProviderOptions,

&nbsp; },

&nbsp; // ...

});

```

The following provider options are available:

\- \*\*negativePrompt\*\* \_string\_

&nbsp; A description of what to discourage in the generated images.

\- \*\*personGeneration\*\* `allow\_adult` | `allow\_all` | `dont\_allow`

&nbsp; Whether to allow person generation. Defaults to `allow\_adult`.

\- \*\*safetySetting\*\* `block\_low\_and\_above` | `block\_medium\_and\_above` | `block\_only\_high` | `block\_none`

&nbsp; Whether to block unsafe content. Defaults to `block\_medium\_and\_above`.

\- \*\*addWatermark\*\* \_boolean\_

&nbsp; Whether to add an invisible watermark to the generated images. Defaults to `true`.

\- \*\*storageUri\*\* \_string\_

&nbsp; Cloud Storage URI to store the generated images.

<Note>

&nbsp; Imagen models do not support the `size` parameter. Use the `aspectRatio`

&nbsp; parameter instead.

</Note>

Additional information about the images can be retrieved using Google Vertex meta data.

```ts

import { vertex } from '@ai-sdk/google-vertex';

import { GoogleVertexImageProviderOptions } from '@ai-sdk/google-vertex';

import { generateImage } from 'ai';



const { image, providerMetadata } = await generateImage({

&nbsp; model: vertex.image('imagen-4.0-generate-001'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; aspectRatio: '16:9',

});



console.log(

&nbsp; `Revised prompt: ${providerMetadata.vertex.images\[0].revisedPrompt}`,

);

```

\#### Image Editing

Google Vertex Imagen models support image editing through inpainting, outpainting, and other edit modes. Pass input images via `prompt.images` and optionally a mask via `prompt.mask`.

<Note>

&nbsp; Image editing is supported by `imagen-3.0-capability-001`. The

&nbsp; `imagen-4.0-generate-001` model does not currently support editing operations.

</Note>

\##### Inpainting (Insert Objects)

Insert or replace objects in specific areas using a mask:

```ts

import {

&nbsp; vertex,

&nbsp; GoogleVertexImageProviderOptions,

} from '@ai-sdk/google-vertex';

import { generateImage } from 'ai';

import fs from 'fs';



const image = fs.readFileSync('./input-image.png');

const mask = fs.readFileSync('./mask.png'); // White = edit area



const { images } = await generateImage({

&nbsp; model: vertex.image('imagen-3.0-capability-001'),

&nbsp; prompt: {

&nbsp;   text: 'A sunlit indoor lounge area with a pool containing a flamingo',

&nbsp;   images: \[image],

&nbsp;   mask,

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   vertex: {

&nbsp;     edit: {

&nbsp;       baseSteps: 50,

&nbsp;       mode: 'EDIT\_MODE\_INPAINT\_INSERTION',

&nbsp;       maskMode: 'MASK\_MODE\_USER\_PROVIDED',

&nbsp;       maskDilation: 0.01,

&nbsp;     },

&nbsp;   } satisfies GoogleVertexImageProviderOptions,

&nbsp; },

});

```

\##### Outpainting (Extend Image)

Extend an image beyond its original boundaries:

```ts

import {

&nbsp; vertex,

&nbsp; GoogleVertexImageProviderOptions,

} from '@ai-sdk/google-vertex';

import { generateImage } from 'ai';

import fs from 'fs';



const image = fs.readFileSync('./input-image.png');

const mask = fs.readFileSync('./outpaint-mask.png'); // White = extend area



const { images } = await generateImage({

&nbsp; model: vertex.image('imagen-3.0-capability-001'),

&nbsp; prompt: {

&nbsp;   text: 'Extend the scene with more of the forest background',

&nbsp;   images: \[image],

&nbsp;   mask,

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   vertex: {

&nbsp;     edit: {

&nbsp;       baseSteps: 50,

&nbsp;       mode: 'EDIT\_MODE\_OUTPAINT',

&nbsp;       maskMode: 'MASK\_MODE\_USER\_PROVIDED',

&nbsp;     },

&nbsp;   } satisfies GoogleVertexImageProviderOptions,

&nbsp; },

});

```

\##### Edit Provider Options

The following options are available under `providerOptions.vertex.edit`:

\- \*\*mode\*\* - The edit mode to use:

&nbsp; - `EDIT\_MODE\_INPAINT\_INSERTION` - Insert objects into masked areas

&nbsp; - `EDIT\_MODE\_INPAINT\_REMOVAL` - Remove objects from masked areas

&nbsp; - `EDIT\_MODE\_OUTPAINT` - Extend image beyond boundaries

&nbsp; - `EDIT\_MODE\_CONTROLLED\_EDITING` - Controlled editing

&nbsp; - `EDIT\_MODE\_PRODUCT\_IMAGE` - Product image editing

&nbsp; - `EDIT\_MODE\_BGSWAP` - Background swap

\- \*\*baseSteps\*\* \_number\_ - Number of sampling steps (35-75). Higher values = better quality but slower.

\- \*\*maskMode\*\* - How to interpret the mask:

&nbsp; - `MASK\_MODE\_USER\_PROVIDED` - Use the provided mask directly

&nbsp; - `MASK\_MODE\_DEFAULT` - Default mask mode

&nbsp; - `MASK\_MODE\_DETECTION\_BOX` - Mask from detected bounding boxes

&nbsp; - `MASK\_MODE\_CLOTHING\_AREA` - Mask from clothing segmentation

&nbsp; - `MASK\_MODE\_PARSED\_PERSON` - Mask from person parsing

\- \*\*maskDilation\*\* \_number\_ - Percentage (0-1) to grow the mask. Recommended: 0.01.

<Note>

&nbsp; Input images must be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`, or

&nbsp; base64-encoded strings. URL-based images are not supported for Google Vertex

&nbsp; image editing.

</Note>

\#### Model Capabilities

| Model | Aspect Ratios |

| ------------------------------- | ------------------------- |

| `imagen-3.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| `imagen-3.0-generate-002` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| `imagen-3.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| `imagen-4.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| `imagen-4.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

| `imagen-4.0-ultra-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |

\## Google Vertex Anthropic Provider Usage

The Google Vertex Anthropic provider for the \[AI SDK](/docs) offers support for Anthropic's Claude models through the Google Vertex AI APIs. This section provides details on how to set up and use the Google Vertex Anthropic provider.

\### Provider Instance

You can import the default provider instance `vertexAnthropic` from `@ai-sdk/google-vertex/anthropic`:

```typescript
import { vertexAnthropic } from "@ai-sdk/google-vertex/anthropic";
```

If you need a customized setup, you can import `createVertexAnthropic` from `@ai-sdk/google-vertex/anthropic` and create a provider instance with your settings:

```typescript

import { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic';



const vertexAnthropic = createVertexAnthropic({

&nbsp; project: 'my-project', // optional

&nbsp; location: 'us-central1', // optional

});

```

\#### Node.js Runtime

For Node.js environments, the Google Vertex Anthropic provider supports all standard Google Cloud authentication options through the `google-auth-library`. You can customize the authentication options by passing them to the `createVertexAnthropic` function:

```typescript

import { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic';



const vertexAnthropic = createVertexAnthropic({

&nbsp; googleAuthOptions: {

&nbsp;   credentials: {

&nbsp;     client\_email: 'my-email',

&nbsp;     private\_key: 'my-private-key',

&nbsp;   },

&nbsp; },

});

```

\##### Optional Provider Settings

You can use the following optional settings to customize the Google Vertex Anthropic provider instance:

\- \*\*project\*\* \_string\_

&nbsp; The Google Cloud project ID that you want to use for the API calls.

&nbsp; It uses the `GOOGLE\_VERTEX\_PROJECT` environment variable by default.

\- \*\*location\*\* \_string\_

&nbsp; The Google Cloud location that you want to use for the API calls, e.g. `us-central1`.

&nbsp; It uses the `GOOGLE\_VERTEX\_LOCATION` environment variable by default.

\- \*\*googleAuthOptions\*\* \_object\_

&nbsp; Optional. The Authentication options used by the \[Google Auth Library](https://github.com/googleapis/google-auth-library-nodejs/). See also the \[GoogleAuthOptions](https://github.com/googleapis/google-auth-library-nodejs/blob/08978822e1b7b5961f0e355df51d738e012be392/src/auth/googleauth.ts#L87C18-L87C35) interface.

&nbsp; - \*\*authClient\*\* \_object\_

&nbsp; An `AuthClient` to use.

&nbsp; - \*\*keyFilename\*\* \_string\_

&nbsp; Path to a .json, .pem, or .p12 key file.

&nbsp; - \*\*keyFile\*\* \_string\_

&nbsp; Path to a .json, .pem, or .p12 key file.

&nbsp; - \*\*credentials\*\* \_object\_

&nbsp; Object containing client_email and private_key properties, or the external account client options.

&nbsp; - \*\*clientOptions\*\* \_object\_

&nbsp; Options object passed to the constructor of the client.

&nbsp; - \*\*scopes\*\* \_string | string\[]\_

&nbsp; Required scopes for the desired API request.

&nbsp; - \*\*projectId\*\* \_string\_

&nbsp; Your project ID.

&nbsp; - \*\*universeDomain\*\* \_string\_

&nbsp; The default service domain for a given Cloud universe.

\- \*\*headers\*\* \_Resolvable\&lt;Record\&lt;string, string | undefined\&gt;\&gt;\_

&nbsp; Headers to include in the requests. Can be provided in multiple formats:

&nbsp; - A record of header key-value pairs: `Record<string, string | undefined>`

&nbsp; - A function that returns headers: `() => Record<string, string | undefined>`

&nbsp; - An async function that returns headers: `async () => Record<string, string | undefined>`

&nbsp; - A promise that resolves to headers: `Promise<Record<string, string | undefined>>`

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

<a id="google-vertex-anthropic-edge-runtime"></a>

\#### Edge Runtime

Edge runtimes (like Vercel Edge Functions and Cloudflare Workers) are lightweight JavaScript environments that run closer to users at the network edge.

They only provide a subset of the standard Node.js APIs.

For example, direct file system access is not available, and many Node.js-specific libraries

(including the standard Google Auth library) are not compatible.

The Edge runtime version of the Google Vertex Anthropic provider supports Google's \[Application Default Credentials](https://github.com/googleapis/google-auth-library-nodejs?tab=readme-ov-file#application-default-credentials) through environment variables. The values can be obtained from a json credentials file from the \[Google Cloud Console](https://console.cloud.google.com/apis/credentials).

For Edge runtimes, you can import the provider instance from `@ai-sdk/google-vertex/anthropic/edge`:

```typescript
import { vertexAnthropic } from "@ai-sdk/google-vertex/anthropic/edge";
```

To customize the setup, use `createVertexAnthropic` from the same module:

```typescript

import { createVertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge';



const vertexAnthropic = createVertexAnthropic({

&nbsp; project: 'my-project', // optional

&nbsp; location: 'us-central1', // optional

});

```

For Edge runtime authentication, set these environment variables from your Google Default Application Credentials JSON file:

\- `GOOGLE\_CLIENT\_EMAIL`

\- `GOOGLE\_PRIVATE\_KEY`

\- `GOOGLE\_PRIVATE\_KEY\_ID` (optional)

\##### Optional Provider Settings

You can use the following optional settings to customize the provider instance:

\- \*\*project\*\* \_string\_

&nbsp; The Google Cloud project ID that you want to use for the API calls.

&nbsp; It uses the `GOOGLE\_VERTEX\_PROJECT` environment variable by default.

\- \*\*location\*\* \_string\_

&nbsp; The Google Cloud location that you want to use for the API calls, e.g. `us-central1`.

&nbsp; It uses the `GOOGLE\_VERTEX\_LOCATION` environment variable by default.

\- \*\*googleCredentials\*\* \_object\_

&nbsp; Optional. The credentials used by the Edge provider for authentication. These credentials are typically set through environment variables and are derived from a service account JSON file.

&nbsp; - \*\*clientEmail\*\* \_string\_

&nbsp; The client email from the service account JSON file. Defaults to the contents of the `GOOGLE\_CLIENT\_EMAIL` environment variable.

&nbsp; - \*\*privateKey\*\* \_string\_

&nbsp; The private key from the service account JSON file. Defaults to the contents of the `GOOGLE\_PRIVATE\_KEY` environment variable.

&nbsp; - \*\*privateKeyId\*\* \_string\_

&nbsp; The private key ID from the service account JSON file (optional). Defaults to the contents of the `GOOGLE\_PRIVATE\_KEY\_ID` environment variable.

\- \*\*headers\*\* \_Resolvable\&lt;Record\&lt;string, string | undefined\&gt;\&gt;\_

&nbsp; Headers to include in the requests. Can be provided in multiple formats:

&nbsp; - A record of header key-value pairs: `Record<string, string | undefined>`

&nbsp; - A function that returns headers: `() => Record<string, string | undefined>`

&nbsp; - An async function that returns headers: `async () => Record<string, string | undefined>`

&nbsp; - A promise that resolves to headers: `Promise<Record<string, string | undefined>>`

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\### Language Models

You can create models that call the \[Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages\_post) using the provider instance.

The first argument is the model id, e.g. `claude-3-haiku-20240307`.

Some models have multi-modal capabilities.

```ts
const model = anthropic("claude-3-haiku-20240307");
```

You can use Anthropic language models to generate text with the `generateText` function:

```ts

import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: vertexAnthropic('claude-3-haiku-20240307'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Anthropic language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

<Note>

&nbsp; The Anthropic API returns streaming tool calls all at once after a delay. This

&nbsp; causes the `streamObject` function to generate the object fully after a delay

&nbsp; instead of streaming it incrementally.

</Note>

The following optional provider options are available for Anthropic models:

\- `sendReasoning` \_boolean\_

&nbsp; Optional. Include reasoning content in requests sent to the model. Defaults to `true`.

&nbsp; If you are experiencing issues with the model handling requests involving

&nbsp; reasoning content, you can set this to `false` to omit them from the request.

\- `thinking` \_object\_

&nbsp; Optional. See \[Reasoning section](#reasoning) for more details.

\### Reasoning

Anthropic has reasoning support for the `claude-3-7-sonnet@20250219` model.

You can enable it using the `thinking` provider option

and specifying a thinking budget in tokens.

```ts

import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';

import { generateText } from 'ai';



const { text, reasoningText, reasoning } = await generateText({

&nbsp; model: vertexAnthropic('claude-3-7-sonnet@20250219'),

&nbsp; prompt: 'How many people will live in the world in 2040?',

&nbsp; providerOptions: {

&nbsp;   anthropic: {

&nbsp;     thinking: { type: 'enabled', budgetTokens: 12000 },

&nbsp;   },

&nbsp; },

});



console.log(reasoningText); // reasoning text

console.log(reasoning); // reasoning details including redacted reasoning

console.log(text); // text response

```

See \[AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details

on how to integrate reasoning into your chatbot.

\#### Cache Control

<Note>

&nbsp; Anthropic cache control is in a Pre-Generally Available (GA) state on Google

&nbsp; Vertex. For more see \[Google Vertex Anthropic cache control

&nbsp; documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude-prompt-caching).

</Note>

In the messages and message parts, you can use the `providerOptions` property to set cache control breakpoints.

You need to set the `anthropic` property in the `providerOptions` object to `{ cacheControl: { type: 'ephemeral' } }` to set a cache control breakpoint.

The cache creation input tokens are then returned in the `providerMetadata` object

for `generateText` and `generateObject`, again under the `anthropic` property.

When you use `streamText` or `streamObject`, the response contains a promise

that resolves to the metadata. Alternatively you can receive it in the

`onFinish` callback.

```ts highlight="8,18-20,29-30"

import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';

import { generateText } from 'ai';



const errorMessage = '... long error message ...';



const result = await generateText({

&nbsp; model: vertexAnthropic('claude-3-5-sonnet-20240620'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       { type: 'text', text: 'You are a JavaScript expert.' },

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: `Error message: ${errorMessage}`,

&nbsp;         providerOptions: {

&nbsp;           anthropic: { cacheControl: { type: 'ephemeral' } },

&nbsp;         },

&nbsp;       },

&nbsp;       { type: 'text', text: 'Explain the error message.' },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});



console.log(result.text);

console.log(result.providerMetadata?.anthropic);

// e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }

```

You can also use cache control on system messages by providing multiple system messages at the head of your messages array:

```ts highlight="3,9-11"

const result = await generateText({

&nbsp; model: vertexAnthropic('claude-3-5-sonnet-20240620'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'system',

&nbsp;     content: 'Cached system message part',

&nbsp;     providerOptions: {

&nbsp;       anthropic: { cacheControl: { type: 'ephemeral' } },

&nbsp;     },

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'system',

&nbsp;     content: 'Uncached system message part',

&nbsp;   },

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: 'User prompt',

&nbsp;   },

&nbsp; ],

});

```

For more on prompt caching with Anthropic, see \[Google Vertex AI's Claude prompt caching documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude-prompt-caching) and \[Anthropic's Cache Control documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching).

\### Tools

Google Vertex Anthropic supports a subset of Anthropic's built-in tools. The following tools are available via the `tools` property of the provider instance:

1\. \*\*Bash Tool\*\*: Allows running bash commands.

2\. \*\*Text Editor Tool\*\*: Provides functionality for viewing and editing text files.

3\. \*\*Computer Tool\*\*: Enables control of keyboard and mouse actions on a computer.

4\. \*\*Web Search Tool\*\*: Provides access to real-time web content.

<Note>

&nbsp; Only a subset of Anthropic tools are supported on Google Vertex. Tools like

&nbsp; Code Execution, Memory, and Web Fetch are not available. Use the regular

&nbsp; `@ai-sdk/anthropic` provider if you need access to all Anthropic tools.

</Note>

For more background on Anthropic tools, see \[Anthropic's documentation](https://platform.claude.com/docs/en/agents-and-tools/tool-use/overview).

\#### Bash Tool

The Bash Tool allows running bash commands. Here's how to create and use it:

```ts

const bashTool = vertexAnthropic.tools.bash\_20250124({

&nbsp; execute: async ({ command, restart }) => {

&nbsp;   // Implement your bash command execution logic here

&nbsp;   // Return the result of the command execution

&nbsp; },

});

```

Parameters:

\- `command` (string): The bash command to run. Required unless the tool is being restarted.

\- `restart` (boolean, optional): Specifying true will restart this tool.

\#### Text Editor Tool

The Text Editor Tool provides functionality for viewing and editing text files:

```ts

const textEditorTool = vertexAnthropic.tools.textEditor\_20250124({

&nbsp; execute: async ({

&nbsp;   command,

&nbsp;   path,

&nbsp;   file\_text,

&nbsp;   insert\_line,

&nbsp;   new\_str,

&nbsp;   old\_str,

&nbsp;   view\_range,

&nbsp; }) => {

&nbsp;   // Implement your text editing logic here

&nbsp;   // Return the result of the text editing operation

&nbsp; },

});

```

Parameters:

\- `command` ('view' | 'create' | 'str_replace' | 'insert' | 'undo_edit'): The command to run. Note: `undo\_edit` is not supported in `textEditor\_20250429` and `textEditor\_20250728`.

\- `path` (string): Absolute path to file or directory, e.g. `/repo/file.py` or `/repo`.

\- `file\_text` (string, optional): Required for `create` command, with the content of the file to be created.

\- `insert\_line` (number, optional): Required for `insert` command. The line number after which to insert the new string.

\- `new\_str` (string, optional): New string for `str\_replace` or `insert` commands.

\- `old\_str` (string, optional): Required for `str\_replace` command, containing the string to replace.

\- `view\_range` (number\[], optional): Optional for `view` command to specify line range to show.

\- `max\_characters` (number, optional): Optional maximum number of characters to view in the file (only available in `textEditor\_20250728`).

\#### Computer Tool

The Computer Tool enables control of keyboard and mouse actions on a computer:

```ts

const computerTool = vertexAnthropic.tools.computer\_20241022({

&nbsp; displayWidthPx: 1920,

&nbsp; displayHeightPx: 1080,

&nbsp; displayNumber: 0, // Optional, for X11 environments



&nbsp; execute: async ({ action, coordinate, text }) => {

&nbsp;   // Implement your computer control logic here

&nbsp;   // Return the result of the action



&nbsp;   // Example code:

&nbsp;   switch (action) {

&nbsp;     case 'screenshot': {

&nbsp;       // multipart result:

&nbsp;       return {

&nbsp;         type: 'image',

&nbsp;         data: fs

&nbsp;           .readFileSync('./data/screenshot-editor.png')

&nbsp;           .toString('base64'),

&nbsp;       };

&nbsp;     }

&nbsp;     default: {

&nbsp;       console.log('Action:', action);

&nbsp;       console.log('Coordinate:', coordinate);

&nbsp;       console.log('Text:', text);

&nbsp;       return `executed ${action}`;

&nbsp;     }

&nbsp;   }

&nbsp; },



&nbsp; // map to tool result content for LLM consumption:

&nbsp; toModelOutput({ output }) {

&nbsp;   return typeof output === 'string'

&nbsp;     ? \[{ type: 'text', text: output }]

&nbsp;     : \[{ type: 'image', data: output.data, mediaType: 'image/png' }];

&nbsp; },

});

```

Parameters:

\- `action` ('key' | 'type' | 'mouse_move' | 'left_click' | 'left_click_drag' | 'right_click' | 'middle_click' | 'double_click' | 'screenshot' | 'cursor_position'): The action to perform.

\- `coordinate` (number\[], optional): Required for `mouse\_move` and `left\_click\_drag` actions. Specifies the (x, y) coordinates.

\- `text` (string, optional): Required for `type` and `key` actions.

\#### Web Search Tool

The Web Search Tool provides Claude with direct access to real-time web content:

```ts

const webSearchTool = vertexAnthropic.tools.webSearch\_20250305({

&nbsp; maxUses: 5, // Optional: Maximum number of web searches Claude can perform

&nbsp; allowedDomains: \['example.com'], // Optional: Only search these domains

&nbsp; blockedDomains: \['spam.com'], // Optional: Never search these domains

&nbsp; userLocation: {

&nbsp;   // Optional: Provide location for geographically relevant results

&nbsp;   type: 'approximate',

&nbsp;   city: 'San Francisco',

&nbsp;   region: 'CA',

&nbsp;   country: 'US',

&nbsp;   timezone: 'America/Los\_Angeles',

&nbsp; },

});

```

Parameters:

\- `maxUses` (number, optional): Maximum number of web searches Claude can perform during the conversation.

\- `allowedDomains` (string\[], optional): Optional list of domains that Claude is allowed to search.

\- `blockedDomains` (string\[], optional): Optional list of domains that Claude should avoid when searching.

\- `userLocation` (object, optional): Optional user location information to provide geographically relevant search results.

&nbsp; - `type` ('approximate'): The type of location (must be approximate).

&nbsp; - `city` (string, optional): The city name.

&nbsp; - `region` (string, optional): The region or state.

&nbsp; - `country` (string, optional): The country.

&nbsp; - `timezone` (string, optional): The IANA timezone ID.

These tools can be used in conjunction with supported Claude models to enable more complex interactions and tasks.

\### Model Capabilities

The latest Anthropic model list on Vertex AI is available \[here](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#model-list).

See also \[Anthropic Model Comparison](https://docs.anthropic.com/en/docs/about-claude/models#model-comparison).

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming | Computer Use |

| ------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `claude-3-7-sonnet@20250219` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `claude-3-5-sonnet-v2@20241022` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `claude-3-5-sonnet@20240620` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `claude-3-5-haiku@20241022` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `claude-3-sonnet@20240229` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `claude-3-haiku@20240307` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `claude-3-opus@20240229` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. You can also pass any available provider

&nbsp; model ID as a string if needed.

</Note>

---

title: Rev.ai

description: Learn how to use the Rev.ai provider for the AI SDK.

---

\# Rev.ai Provider

The \[Rev.ai](https://www.rev.ai/) provider contains language model support for the Rev.ai transcription API.

\## Setup

The Rev.ai provider is available in the `@ai-sdk/revai` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/revai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/revai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/revai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/revai" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `revai` from `@ai-sdk/revai`:

```ts
import { revai } from "@ai-sdk/revai";
```

If you need a customized setup, you can import `createRevai` from `@ai-sdk/revai` and create a provider instance with your settings:

```ts

import { createRevai } from '@ai-sdk/revai';



const revai = createRevai({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the Rev.ai provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `REVAI\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Transcription Models

You can create models that call the \[Rev.ai transcription API](https://www.rev.ai/docs/api/transcription)

using the `.transcription()` factory method.

The first argument is the model id e.g. `machine`.

```ts
const model = revai.transcription("machine");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format can sometimes improve transcription performance if known beforehand.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { revai } from '@ai-sdk/revai';

import { readFile } from 'fs/promises';



const result = await transcribe({

&nbsp; model: revai.transcription('machine'),

&nbsp; audio: await readFile('audio.mp3'),

&nbsp; providerOptions: { revai: { language: 'en' } },

});

```

The following provider options are available:

\- \*\*metadata\*\* \_string\_

&nbsp; Optional metadata that was provided during job submission.

\- \*\*notification_config\*\* \_object\_

&nbsp; Optional configuration for a callback url to invoke when processing is complete.

&nbsp; - \*\*url\*\* \_string\_ - Callback url to invoke when processing is complete.

&nbsp; - \*\*auth_headers\*\* \_object\_ - Optional authorization headers, if needed to invoke the callback.

&nbsp; - \*\*Authorization\*\* \_string\_ - Authorization header value.

\- \*\*delete_after_seconds\*\* \_integer\_

&nbsp; Amount of time after job completion when job is auto-deleted.

\- \*\*verbatim\*\* \_boolean\_

&nbsp; Configures the transcriber to transcribe every syllable, including all false starts and disfluencies.

\- \*\*rush\*\* \_boolean\_

&nbsp; \[HIPAA Unsupported] Only available for human transcriber option. When set to true, your job is given higher priority.

\- \*\*skip_diarization\*\* \_boolean\_

&nbsp; Specify if speaker diarization will be skipped by the speech engine.

\- \*\*skip_postprocessing\*\* \_boolean\_

&nbsp; Only available for English and Spanish languages. User-supplied preference on whether to skip post-processing operations.

\- \*\*skip_punctuation\*\* \_boolean\_

&nbsp; Specify if "punct" type elements will be skipped by the speech engine.

\- \*\*remove_disfluencies\*\* \_boolean\_

&nbsp; When set to true, disfluencies (like 'ums' and 'uhs') will not appear in the transcript.

\- \*\*remove_atmospherics\*\* \_boolean\_

&nbsp; When set to true, atmospherics (like `<laugh>`, `<affirmative>`) will not appear in the transcript.

\- \*\*filter_profanity\*\* \_boolean\_

&nbsp; When enabled, profanities will be filtered by replacing characters with asterisks except for the first and last.

\- \*\*speaker_channels_count\*\* \_integer\_

&nbsp; Only available for English, Spanish and French languages. Specify the total number of unique speaker channels in the audio.

\- \*\*speakers_count\*\* \_integer\_

&nbsp; Only available for English, Spanish and French languages. Specify the total number of unique speakers in the audio.

\- \*\*diarization_type\*\* \_string\_

&nbsp; Specify diarization type. Possible values: "standard" (default), "premium".

\- \*\*custom_vocabulary_id\*\* \_string\_

&nbsp; Supply the id of a pre-completed custom vocabulary submitted through the Custom Vocabularies API.

\- \*\*custom_vocabularies\*\* \_Array\_

&nbsp; Specify a collection of custom vocabulary to be used for this job.

\- \*\*strict_custom_vocabulary\*\* \_boolean\_

&nbsp; If true, only exact phrases will be used as custom vocabulary.

\- \*\*summarization_config\*\* \_object\_

&nbsp; Specify summarization options.

&nbsp; - \*\*model\*\* \_string\_ - Model type for summarization. Possible values: "standard" (default), "premium".

&nbsp; - \*\*type\*\* \_string\_ - Summarization formatting type. Possible values: "paragraph" (default), "bullets".

&nbsp; - \*\*prompt\*\* \_string\_ - Custom prompt for flexible summaries (mutually exclusive with type).

\- \*\*translation_config\*\* \_object\_

&nbsp; Specify translation options.

&nbsp; - \*\*target_languages\*\* \_Array\_ - Array of target languages for translation.

&nbsp; - \*\*model\*\* \_string\_ - Model type for translation. Possible values: "standard" (default), "premium".

\- \*\*language\*\* \_string\_

&nbsp; Language is provided as a ISO 639-1 language code. Default is "en".

\- \*\*forced_alignment\*\* \_boolean\_

&nbsp; When enabled, provides improved accuracy for per-word timestamps for a transcript.

&nbsp; Default is `false`.

&nbsp; Currently supported languages:

&nbsp; - English (en, en-us, en-gb)

&nbsp; - French (fr)

&nbsp; - Italian (it)

&nbsp; - German (de)

&nbsp; - Spanish (es)

&nbsp; Note: This option is not available in low-cost environment.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| ---------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `machine` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `low\_cost` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `fusion` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

---

title: Baseten

description: Learn how to use Baseten models with the AI SDK.

---

\# Baseten Provider

\[Baseten](https://baseten.co/) is an inference platform for serving frontier, enterprise-grade opensource AI models via their \[API](https://docs.baseten.co/overview).

\## Setup

The Baseten provider is available via the `@ai-sdk/baseten` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/baseten" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/baseten" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/baseten" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `baseten` from `@ai-sdk/baseten`:

```ts
import { baseten } from "@ai-sdk/baseten";
```

If you need a customized setup, you can import `createBaseten` from `@ai-sdk/baseten`

and create a provider instance with your settings:

```ts

import { createBaseten } from '@ai-sdk/baseten';



const baseten = createBaseten({

&nbsp; apiKey: process.env.BASETEN\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Baseten provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://inference.baseten.co/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `BASETEN\_API\_KEY` environment variable. It is recommended you set the environment variable using `export` so you do not need to include the field everytime.

&nbsp; You can grab your Baseten API Key \[here](https://app.baseten.co/settings/api\_keys)

\- \*\*modelURL\*\* \_string\_

&nbsp; Custom model URL for specific models (chat or embeddings). If not provided,

&nbsp; the default Model APIs will be used.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Model APIs

You can select \[Baseten models](https://www.baseten.co/products/model-apis/) using a provider instance.

The first argument is the model id, e.g. `'moonshotai/Kimi-K2-Instruct-0905'`: The complete supported models under Model APIs can be found \[here](https://docs.baseten.co/development/model-apis/overview#supported-models).

```ts
const model = baseten("moonshotai/Kimi-K2-Instruct-0905");
```

\### Example

You can use Baseten language models to generate text with the `generateText` function:

```ts

import { baseten } from '@ai-sdk/baseten';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: baseten('moonshotai/Kimi-K2-Instruct-0905'),

&nbsp; prompt: 'What is the meaning of life? Answer in one sentence.',

});

```

Baseten language models can also be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

\## Dedicated Models

Baseten supports dedicated model URLs for both chat and embedding models. You have to specify a `modelURL` when creating the provider:

\### OpenAI-Compatible Endpoints (`/sync/v1`)

For models deployed with Baseten's OpenAI-compatible endpoints:

```ts

import { createBaseten } from '@ai-sdk/baseten';



const baseten = createBaseten({

&nbsp; modelURL: 'https://model-{MODEL\_ID}.api.baseten.co/sync/v1',

});

// No modelId is needed because we specified modelURL

const model = baseten();

const { text } = await generateText({

&nbsp; model: model,

&nbsp; prompt: 'Say hello from a Baseten chat model!',

});

```

\### `/predict` Endpoints

`/predict` endpoints are currently NOT supported for chat models. You must use `/sync/v1` endpoints for chat functionality.

\## Embedding Models

You can create models that call the Baseten embeddings API using the `.embeddingModel()` factory method. The Baseten provider uses the high-performance `@basetenlabs/performance-client` for optimal embedding performance.

<Note>

&nbsp; \*\*Important:\*\* Embedding models require a dedicated deployment with a custom

&nbsp; `modelURL`. Unlike chat models, embeddings cannot use Baseten's default Model

&nbsp; APIs and must specify a dedicated model endpoint.

</Note>

```ts

import { createBaseten } from '@ai-sdk/baseten';

import { embed, embedMany } from 'ai';



const baseten = createBaseten({

&nbsp; modelURL: 'https://model-{MODEL\_ID}.api.baseten.co/sync',

});



const embeddingModel = baseten.embeddingModel();



// Single embedding

const { embedding } = await embed({

&nbsp; model: embeddingModel,

&nbsp; value: 'sunny day at the beach',

});



// Batch embeddings

const { embeddings } = await embedMany({

&nbsp; model: embeddingModel,

&nbsp; values: \[

&nbsp;   'sunny day at the beach',

&nbsp;   'rainy afternoon in the city',

&nbsp;   'snowy mountain peak',

&nbsp; ],

});

```

\### Endpoint Support for Embeddings

\*\*Supported:\*\*

\- `/sync` endpoints (Performance Client automatically adds `/v1/embeddings`)

\- `/sync/v1` endpoints (automatically strips `/v1` before passing to Performance Client)

\*\*Not Supported:\*\*

\- `/predict` endpoints (not compatible with Performance Client)

\### Performance Features

The embedding implementation includes:

\- \*\*High-performance client\*\*: Uses `@basetenlabs/performance-client` for optimal performance

\- \*\*Automatic batching\*\*: Efficiently handles multiple texts in a single request

\- \*\*Connection reuse\*\*: Performance Client is created once and reused for all requests

\- \*\*Built-in retries\*\*: Automatic retry logic for failed requests

\## Error Handling

The Baseten provider includes built-in error handling for common API errors:

```ts

import { baseten } from '@ai-sdk/baseten';

import { generateText } from 'ai';



try {

&nbsp; const { text } = await generateText({

&nbsp;   model: baseten('moonshotai/Kimi-K2-Instruct-0905'),

&nbsp;   prompt: 'Hello, world!',

&nbsp; });

} catch (error) {

&nbsp; console.error('Baseten API error:', error.message);

}

```

\### Common Error Scenarios

```ts

// Embeddings require a modelURL

try {

&nbsp; baseten.embeddingModel();

} catch (error) {

&nbsp; // Error: "No model URL provided for embeddings. Please set modelURL option for embeddings."

}



// /predict endpoints are not supported for chat models

try {

&nbsp; const baseten = createBaseten({

&nbsp;   modelURL:

&nbsp;     'https://model-{MODEL\_ID}.api.baseten.co/environments/production/predict',

&nbsp; });

&nbsp; baseten(); // This will throw an error

} catch (error) {

&nbsp; // Error: "Not supported. You must use a /sync/v1 endpoint for chat models."

}



// /sync/v1 endpoints are now supported for embeddings

const baseten = createBaseten({

&nbsp; modelURL:

&nbsp;   'https://model-{MODEL\_ID}.api.baseten.co/environments/production/sync/v1',

});

const embeddingModel = baseten.embeddingModel(); // This works fine!



// /predict endpoints are not supported for embeddings

try {

&nbsp; const baseten = createBaseten({

&nbsp;   modelURL:

&nbsp;     'https://model-{MODEL\_ID}.api.baseten.co/environments/production/predict',

&nbsp; });

&nbsp; baseten.embeddingModel(); // This will throw an error

} catch (error) {

&nbsp; // Error: "Not supported. You must use a /sync or /sync/v1 endpoint for embeddings."

}



// Image models are not supported

try {

&nbsp; baseten.imageModel('test-model');

} catch (error) {

&nbsp; // Error: NoSuchModelError for imageModel

}

```

<Note>

&nbsp; For more information about Baseten models and deployment options, see the

&nbsp; \[Baseten documentation](https://docs.baseten.co/).

</Note>

---

title: Hugging Face

description: Learn how to use Hugging Face Provider.

---

\# Hugging Face Provider

The \[Hugging Face](https://huggingface.co/) provider offers access to thousands of language models through \[Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers/index), including models from Meta, DeepSeek, Qwen, and more.

API keys can be obtained from \[Hugging Face Settings](https://huggingface.co/settings/tokens).

\## Setup

The Hugging Face provider is available via the `@ai-sdk/huggingface` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/huggingface" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/huggingface" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/huggingface" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/huggingface" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `huggingface` from `@ai-sdk/huggingface`:

```ts
import { huggingface } from "@ai-sdk/huggingface";
```

For custom configuration, you can import `createHuggingFace` and create a provider instance with your settings:

```ts

import { createHuggingFace } from '@ai-sdk/huggingface';



const huggingface = createHuggingFace({

&nbsp; apiKey: process.env.HUGGINGFACE\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Hugging Face provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://router.huggingface.co/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `HUGGINGFACE\_API\_KEY` environment variable. You can get your API key

&nbsp; from \[Hugging Face Settings](https://huggingface.co/settings/tokens).

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Language Models

You can create language models using a provider instance:

```ts

import { huggingface } from '@ai-sdk/huggingface';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: huggingface('deepseek-ai/DeepSeek-V3-0324'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

You can also use the `.responses()` or `.languageModel()` factory methods:

```ts
const model = huggingface.responses("deepseek-ai/DeepSeek-V3-0324");

// or

const model = huggingface.languageModel("moonshotai/Kimi-K2-Instruct");
```

Hugging Face language models can be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

You can explore the latest and trending models with their capabilities, context size, throughput and pricing on the \[Hugging Face Inference Models](https://huggingface.co/inference/models) page.

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `meta-llama/Llama-3.1-8B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Llama-3.1-70B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Llama-3.3-70B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `meta-llama/Llama-4-Scout-17B-16E-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-ai/DeepSeek-V3-0324` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-ai/DeepSeek-R1` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-ai/DeepSeek-R1-Distill-Llama-70B` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `Qwen/Qwen3-235B-A22B-Instruct-2507` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `Qwen/Qwen3-Coder-480B-A35B-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `Qwen/Qwen2.5-VL-7B-Instruct` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `google/gemma-3-27b-it` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `moonshotai/Kimi-K2-Instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; The capabilities depend on the specific model you're using. Check the model

&nbsp; documentation on Hugging Face Hub for detailed information about each model's

&nbsp; features.

</Note>

---

title: Mistral AI

description: Learn how to use Mistral.

---

\# Mistral AI Provider

The \[Mistral AI](https://mistral.ai/) provider contains language model support for the Mistral chat API.

\## Setup

The Mistral provider is available in the `@ai-sdk/mistral` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/mistral" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/mistral" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/mistral" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/mistral" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `mistral` from `@ai-sdk/mistral`:

```ts
import { mistral } from "@ai-sdk/mistral";
```

If you need a customized setup, you can import `createMistral` from `@ai-sdk/mistral`

and create a provider instance with your settings:

```ts

import { createMistral } from '@ai-sdk/mistral';



const mistral = createMistral({

&nbsp; // custom settings

});

```

You can use the following optional settings to customize the Mistral provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.mistral.ai/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `MISTRAL\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create models that call the \[Mistral chat API](https://docs.mistral.ai/api/#operation/createChatCompletion) using a provider instance.

The first argument is the model id, e.g. `mistral-large-latest`.

Some Mistral chat models support tool calls.

```ts
const model = mistral("mistral-large-latest");
```

Mistral chat models also support additional model settings that are not part of the \[standard call settings](/docs/ai-sdk-core/settings).

You can pass them as an options argument and utilize `MistralLanguageModelOptions` for typing:

```ts

import { mistral, type MistralLanguageModelOptions } from '@ai-sdk/mistral';

const model = mistral('mistral-large-latest');



await generateText({

&nbsp; model,

&nbsp; providerOptions: {

&nbsp;   mistral: {

&nbsp;     safePrompt: true, // optional safety prompt injection

&nbsp;     parallelToolCalls: false, // disable parallel tool calls (one tool per response)

&nbsp;   } satisfies MistralLanguageModelOptions,

&nbsp; },

});

```

The following optional provider options are available for Mistral models:

\- \*\*safePrompt\*\* \_boolean\_

&nbsp; Whether to inject a safety prompt before all conversations.

&nbsp; Defaults to `false`.

\- \*\*documentImageLimit\*\* \_number\_

&nbsp; Maximum number of images to process in a document.

\- \*\*documentPageLimit\*\* \_number\_

&nbsp; Maximum number of pages to process in a document.

\- \*\*strictJsonSchema\*\* \_boolean\_

&nbsp; Whether to use strict JSON schema validation for structured outputs. Only applies when a schema is provided and only sets the \[`strict` flag](https://docs.mistral.ai/api/#tag/chat/operation/chat\_completion\_v1\_chat\_completions\_post) in addition to using \[Custom Structured Outputs](https://docs.mistral.ai/capabilities/structured-output/custom\_structured\_output/), which is used by default if a schema is provided.

&nbsp; Defaults to `false`.

\- \*\*structuredOutputs\*\* \_boolean\_

&nbsp; Whether to use \[structured outputs](#structured-outputs). When enabled, tool calls and object generation will be strict and follow the provided schema.

&nbsp; Defaults to `true`.

\- \*\*parallelToolCalls\*\* \_boolean\_

&nbsp; Whether to enable parallel function calling during tool use. When set to false, the model will use at most one tool per response.

&nbsp; Defaults to `true`.

\### Document OCR

Mistral chat models support document OCR for PDF files.

You can optionally set image and page limits using the provider options.

```ts

const result = await generateText({

&nbsp; model: mistral('mistral-small-latest'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is an embedding model according to this document?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: new URL(

&nbsp;           'https://github.com/vercel/ai/blob/main/examples/ai-functions/data/ai.pdf?raw=true',

&nbsp;         ),

&nbsp;         mediaType: 'application/pdf',

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

&nbsp; // optional settings:

&nbsp; providerOptions: {

&nbsp;   mistral: {

&nbsp;     documentImageLimit: 8,

&nbsp;     documentPageLimit: 64,

&nbsp;   },

&nbsp; },

});

```

\### Reasoning Models

Mistral offers reasoning models that provide step-by-step thinking capabilities:

\- \*\*magistral-small-2506\*\*: Smaller reasoning model for efficient step-by-step thinking

\- \*\*magistral-medium-2506\*\*: More powerful reasoning model balancing performance and cost

These models return content that includes `<think>...</think>` tags containing the reasoning process. To properly extract and separate the reasoning from the final answer, use the \[extract reasoning middleware](/docs/reference/ai-sdk-core/extract-reasoning-middleware):

```ts

import { mistral } from '@ai-sdk/mistral';

import {

&nbsp; extractReasoningMiddleware,

&nbsp; generateText,

&nbsp; wrapLanguageModel,

} from 'ai';



const result = await generateText({

&nbsp; model: wrapLanguageModel({

&nbsp;   model: mistral('magistral-small-2506'),

&nbsp;   middleware: extractReasoningMiddleware({

&nbsp;     tagName: 'think',

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'What is 15 \* 24?',

});



console.log('REASONING:', result.reasoningText);

// Output: "Let me calculate this step by step..."



console.log('ANSWER:', result.text);

// Output: "360"

```

The middleware automatically parses the `<think>` tags and provides separate `reasoningText` and `text` properties in the result.

\### Example

You can use Mistral language models to generate text with the `generateText` function:

```ts

import { mistral } from '@ai-sdk/mistral';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: mistral('mistral-large-latest'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Mistral language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core)).

\#### Structured Outputs

Mistral chat models support structured outputs using JSON Schema. You can use `generateObject` or `streamObject`

with Zod, Valibot, or raw JSON Schema. The SDK sends your schema via Mistral's `response\_format: { type: 'json\_schema' }`.

```ts

import { mistral } from '@ai-sdk/mistral';

import { generateObject } from 'ai';

import { z } from 'zod';



const result = await generateObject({

&nbsp; model: mistral('mistral-large-latest'),

&nbsp; schema: z.object({

&nbsp;   recipe: z.object({

&nbsp;     name: z.string(),

&nbsp;     ingredients: z.array(z.string()),

&nbsp;     instructions: z.array(z.string()),

&nbsp;   }),

&nbsp; }),

&nbsp; prompt: 'Generate a simple pasta recipe.',

});



console.log(JSON.stringify(result.object, null, 2));

```

You can enable strict JSON Schema validation using a provider option:

```ts highlight="7-11"

import { mistral } from '@ai-sdk/mistral';

import { generateObject } from 'ai';

import { z } from 'zod';



const result = await generateObject({

&nbsp; model: mistral('mistral-large-latest'),

&nbsp; providerOptions: {

&nbsp;   mistral: {

&nbsp;     strictJsonSchema: true, // reject outputs that don't strictly match the schema

&nbsp;   },

&nbsp; },

&nbsp; schema: z.object({

&nbsp;   title: z.string(),

&nbsp;   items: z.array(z.object({ id: z.string(), qty: z.number().int().min(1) })),

&nbsp; }),

&nbsp; prompt: 'Generate a small shopping list.',

});

```

<Note>

&nbsp; When using structured outputs, the SDK no longer injects an extra "answer with

&nbsp; JSON" instruction. It relies on Mistral's native `json\_schema`/`json\_object`

&nbsp; response formats instead. You can customize the schema name/description via

&nbsp; the standard structured-output APIs.

</Note>

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ----------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `pixtral-large-latest` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistral-large-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistral-medium-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistral-medium-2505` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistral-small-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `magistral-small-2506` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `magistral-medium-2506` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `ministral-3b-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `ministral-8b-latest` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `pixtral-12b-2409` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `open-mistral-7b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `open-mixtral-8x7b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `open-mixtral-8x22b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Mistral

&nbsp; docs](https://docs.mistral.ai/getting-started/models/models\_overview/) for a

&nbsp; full list of available models. The table above lists popular models. You can

&nbsp; also pass any available provider model ID as a string if needed.

</Note>

\## Embedding Models

You can create models that call the \[Mistral embeddings API](https://docs.mistral.ai/api/#operation/createEmbedding)

using the `.embedding()` factory method.

```ts
const model = mistral.embedding("mistral-embed");
```

You can use Mistral embedding models to generate embeddings with the `embed` function:

```ts

import { mistral } from '@ai-sdk/mistral';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: mistral.embedding('mistral-embed'),

&nbsp; value: 'sunny day at the beach',

});

```

\### Model Capabilities

| Model | Default Dimensions |

| --------------- | ------------------ |

| `mistral-embed` | 1024 |

---

title: Together.ai

description: Learn how to use Together.ai's models with the AI SDK.

---

\# Together.ai Provider

The \[Together.ai](https://together.ai) provider contains support for 200+ open-source models through the \[Together.ai API](https://docs.together.ai/reference).

\## Setup

The Together.ai provider is available via the `@ai-sdk/togetherai` module. You can

install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/togetherai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/togetherai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/togetherai" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/togetherai" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `togetherai` from `@ai-sdk/togetherai`:

```ts
import { togetherai } from "@ai-sdk/togetherai";
```

If you need a customized setup, you can import `createTogetherAI` from `@ai-sdk/togetherai`

and create a provider instance with your settings:

```ts

import { createTogetherAI } from '@ai-sdk/togetherai';



const togetherai = createTogetherAI({

&nbsp; apiKey: process.env.TOGETHER\_AI\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Together.ai provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.together.xyz/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `TOGETHER\_AI\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create \[Together.ai models](https://docs.together.ai/docs/serverless-models) using a provider instance. The first argument is the model id, e.g. `google/gemma-2-9b-it`.

```ts
const model = togetherai("google/gemma-2-9b-it");
```

\### Reasoning Models

Together.ai exposes the thinking of `deepseek-ai/DeepSeek-R1` in the generated text using the `<think>` tag.

You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';



const enhancedModel = wrapLanguageModel({

&nbsp; model: togetherai('deepseek-ai/DeepSeek-R1'),

&nbsp; middleware: extractReasoningMiddleware({ tagName: 'think' }),

});

```

You can then use that enhanced model in functions like `generateText` and `streamText`.

\### Example

You can use Together.ai language models to generate text with the `generateText` function:

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Together.ai language models can also be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

The Together.ai provider also supports \[completion models](https://docs.together.ai/docs/serverless-models#language-models) via (following the above example code) `togetherai.completion()` and \[embedding models](https://docs.together.ai/docs/serverless-models#embedding-models) via `togetherai.embedding()`.

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ---------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo` | <Cross size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistralai/Mixtral-8x22B-Instruct-v0.1` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `mistralai/Mistral-7B-Instruct-v0.3` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-ai/DeepSeek-V3` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `google/gemma-2b-it` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `Qwen/Qwen2.5-72B-Instruct-Turbo` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `databricks/dbrx-instruct` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Together.ai

&nbsp; docs](https://docs.together.ai/docs/serverless-models) for a full list of

&nbsp; available models. You can also pass any available provider model ID as a

&nbsp; string if needed.

</Note>

\## Image Models

You can create Together.ai image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { generateImage } from 'ai';



const { images } = await generateImage({

&nbsp; model: togetherai.image('black-forest-labs/FLUX.1-dev'),

&nbsp; prompt: 'A delighted resplendent quetzal mid flight amidst raindrops',

});

```

You can pass optional provider-specific request parameters using the `providerOptions` argument.

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { generateImage } from 'ai';



const { images } = await generateImage({

&nbsp; model: togetherai.image('black-forest-labs/FLUX.1-dev'),

&nbsp; prompt: 'A delighted resplendent quetzal mid flight amidst raindrops',

&nbsp; size: '512x512',

&nbsp; // Optional additional provider-specific request parameters

&nbsp; providerOptions: {

&nbsp;   togetherai: {

&nbsp;     steps: 40,

&nbsp;   },

&nbsp; },

});

```

For a complete list of available provider-specific options, see the \[Together.ai Image Generation API Reference](https://docs.together.ai/reference/post\_images-generations).

\### Image Editing

Together AI supports image editing through FLUX Kontext models. Pass input images via `prompt.images` to transform or edit existing images.

<Note>

&nbsp; Together AI does not support mask-based inpainting. Instead, use descriptive

&nbsp; prompts to specify what you want to change in the image.

</Note>

\#### Basic Image Editing

Transform an existing image using text prompts:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: togetherai.image('black-forest-labs/FLUX.1-kontext-pro'),

&nbsp; prompt: {

&nbsp;   text: 'Turn the cat into a golden retriever dog',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; size: '1024x1024',

&nbsp; providerOptions: {

&nbsp;   togetherai: {

&nbsp;     steps: 28,

&nbsp;   },

&nbsp; },

});

```

\#### Editing with URL Reference

You can also pass image URLs directly:

```ts

const { images } = await generateImage({

&nbsp; model: togetherai.image('black-forest-labs/FLUX.1-kontext-pro'),

&nbsp; prompt: {

&nbsp;   text: 'Make the background a lush rainforest',

&nbsp;   images: \['https://example.com/photo.png'],

&nbsp; },

&nbsp; size: '1024x1024',

&nbsp; providerOptions: {

&nbsp;   togetherai: {

&nbsp;     steps: 28,

&nbsp;   },

&nbsp; },

});

```

<Note>

&nbsp; Input images can be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`,

&nbsp; base64-encoded strings, or URLs. Together AI only supports a single input

&nbsp; image per request.

</Note>

\#### Supported Image Editing Models

| Model | Description |

| -------------------------------------- | ---------------------------------- |

| `black-forest-labs/FLUX.1-kontext-pro` | Production quality, balanced speed |

| `black-forest-labs/FLUX.1-kontext-max` | Maximum image fidelity |

| `black-forest-labs/FLUX.1-kontext-dev` | Development and experimentation |

\### Model Capabilities

Together.ai image models support various image dimensions that vary by model. Common sizes include 512x512, 768x768, and 1024x1024, with some models supporting up to 1792x1792. The default size is 1024x1024.

| Available Models |

| ------------------------------------------ |

| `stabilityai/stable-diffusion-xl-base-1.0` |

| `black-forest-labs/FLUX.1-dev` |

| `black-forest-labs/FLUX.1-dev-lora` |

| `black-forest-labs/FLUX.1-schnell` |

| `black-forest-labs/FLUX.1-canny` |

| `black-forest-labs/FLUX.1-depth` |

| `black-forest-labs/FLUX.1-redux` |

| `black-forest-labs/FLUX.1.1-pro` |

| `black-forest-labs/FLUX.1-pro` |

| `black-forest-labs/FLUX.1-schnell-Free` |

<Note>

&nbsp; Please see the \[Together.ai models

&nbsp; page](https://docs.together.ai/docs/serverless-models#image-models) for a full

&nbsp; list of available image models and their capabilities.

</Note>

\## Embedding Models

You can create Together.ai embedding models using the `.embedding()` factory method.

For more on embedding models with the AI SDK see \[embed()](/docs/reference/ai-sdk-core/embed).

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: togetherai.embedding('togethercomputer/m2-bert-80M-2k-retrieval'),

&nbsp; value: 'sunny day at the beach',

});

```

\### Model Capabilities

| Model | Dimensions | Max Tokens |

| ------------------------------------------------ | ---------- | ---------- |

| `togethercomputer/m2-bert-80M-2k-retrieval` | 768 | 2048 |

| `togethercomputer/m2-bert-80M-8k-retrieval` | 768 | 8192 |

| `togethercomputer/m2-bert-80M-32k-retrieval` | 768 | 32768 |

| `WhereIsAI/UAE-Large-V1` | 1024 | 512 |

| `BAAI/bge-large-en-v1.5` | 1024 | 512 |

| `BAAI/bge-base-en-v1.5` | 768 | 512 |

| `sentence-transformers/msmarco-bert-base-dot-v5` | 768 | 512 |

| `bert-base-uncased` | 768 | 512 |

<Note>

&nbsp; For a complete list of available embedding models, see the \[Together.ai models

&nbsp; page](https://docs.together.ai/docs/serverless-models#embedding-models).

</Note>

\## Reranking Models

You can create Together.ai reranking models using the `.reranking()` factory method.

For more on reranking with the AI SDK see \[rerank()](/docs/reference/ai-sdk-core/rerank).

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { rerank } from 'ai';



const documents = \[

&nbsp; 'sunny day at the beach',

&nbsp; 'rainy afternoon in the city',

&nbsp; 'snowy night in the mountains',

];



const { ranking } = await rerank({

&nbsp; model: togetherai.reranking('Salesforce/Llama-Rank-v1'),

&nbsp; documents,

&nbsp; query: 'talk about rain',

&nbsp; topN: 2,

});



console.log(ranking);

// \[

//   { originalIndex: 1, score: 0.9, document: 'rainy afternoon in the city' },

//   { originalIndex: 0, score: 0.3, document: 'sunny day at the beach' }

// ]

```

Together.ai reranking models support additional provider options for object documents. You can specify which fields to use for ranking:

```ts

import { togetherai } from '@ai-sdk/togetherai';

import { rerank } from 'ai';



const documents = \[

&nbsp; {

&nbsp;   from: 'Paul Doe',

&nbsp;   subject: 'Follow-up',

&nbsp;   text: 'We are happy to give you a discount of 20%.',

&nbsp; },

&nbsp; {

&nbsp;   from: 'John McGill',

&nbsp;   subject: 'Missing Info',

&nbsp;   text: 'Here is the pricing from Oracle: $5000/month',

&nbsp; },

];



const { ranking } = await rerank({

&nbsp; model: togetherai.reranking('Salesforce/Llama-Rank-v1'),

&nbsp; documents,

&nbsp; query: 'Which pricing did we get from Oracle?',

&nbsp; providerOptions: {

&nbsp;   togetherai: {

&nbsp;     rankFields: \['from', 'subject', 'text'], // Specify which fields to rank by

&nbsp;   },

&nbsp; },

});

```

The following provider options are available:

\- \*\*rankFields\*\* \_string\[]\_

&nbsp; Array of field names to use for ranking when documents are JSON objects. If not specified, all fields are used.

\### Model Capabilities

| Model |

| ------------------------------------- |

| `Salesforce/Llama-Rank-v1` |

| `mixedbread-ai/Mxbai-Rerank-Large-V2` |

---

title: Cohere

description: Learn how to use the Cohere provider for the AI SDK.

---

\# Cohere Provider

The \[Cohere](https://cohere.com/) provider contains language and embedding model support for the Cohere chat API.

\## Setup

The Cohere provider is available in the `@ai-sdk/cohere` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/cohere" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/cohere" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/cohere" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/cohere" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `cohere` from `@ai-sdk/cohere`:

```ts
import { cohere } from "@ai-sdk/cohere";
```

If you need a customized setup, you can import `createCohere` from `@ai-sdk/cohere`

and create a provider instance with your settings:

```ts

import { createCohere } from '@ai-sdk/cohere';



const cohere = createCohere({

&nbsp; // custom settings

});

```

You can use the following optional settings to customize the Cohere provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.cohere.com/v2`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `COHERE\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Language Models

You can create models that call the \[Cohere chat API](https://docs.cohere.com/v2/docs/chat-api) using a provider instance.

The first argument is the model id, e.g. `command-r-plus`.

Some Cohere chat models support tool calls.

```ts
const model = cohere("command-r-plus");
```

\### Example

You can use Cohere language models to generate text with the `generateText` function:

```ts

import { cohere } from '@ai-sdk/cohere';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: cohere('command-r-plus'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Cohere language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions

(see \[AI SDK Core](/docs/ai-sdk-core).

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ----------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `command-a-03-2025` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-a-reasoning-08-2025` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-r7b-12-2024` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-r-plus-04-2024` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-r-plus` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-r-08-2024` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-r-03-2024` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command-r` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `command` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `command-nightly` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `command-light` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `command-light-nightly` | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Cohere

&nbsp; docs](https://docs.cohere.com/v2/docs/models#command) for a full list of

&nbsp; available models. You can also pass any available provider model ID as a

&nbsp; string if needed.

</Note>

\#### Reasoning

Cohere has introduced reasoning with the `command-a-reasoning-08-2025` model. You can learn more at https://docs.cohere.com/docs/reasoning.

```ts

import { cohere } from '@ai-sdk/cohere';

import { generateText } from 'ai';



async function main() {

&nbsp; const { text, reasoning } = await generateText({

&nbsp;   model: cohere('command-a-reasoning-08-2025'),

&nbsp;   prompt:

&nbsp;     "Alice has 3 brothers and she also has 2 sisters. How many sisters does Alice's brother have?",

&nbsp;   // optional: reasoning options

&nbsp;   providerOptions: {

&nbsp;     cohere: {

&nbsp;       thinking: {

&nbsp;         type: 'enabled',

&nbsp;         tokenBudget: 100,

&nbsp;       },

&nbsp;     },

&nbsp;   },

&nbsp; });



&nbsp; console.log(reasoning);

&nbsp; console.log(text);

}



main().catch(console.error);

```

\## Embedding Models

You can create models that call the \[Cohere embed API](https://docs.cohere.com/v2/reference/embed)

using the `.embedding()` factory method.

```ts
const model = cohere.embedding("embed-english-v3.0");
```

You can use Cohere embedding models to generate embeddings with the `embed` function:

```ts

import { cohere } from '@ai-sdk/cohere';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: cohere.embedding('embed-english-v3.0'),

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   cohere: {

&nbsp;     inputType: 'search\_document',

&nbsp;   },

&nbsp; },

});

```

Cohere embedding models support additional provider options that can be passed via `providerOptions.cohere`:

```ts

import { cohere } from '@ai-sdk/cohere';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: cohere.embedding('embed-english-v3.0'),

&nbsp; value: 'sunny day at the beach',

&nbsp; providerOptions: {

&nbsp;   cohere: {

&nbsp;     inputType: 'search\_document',

&nbsp;     truncate: 'END',

&nbsp;   },

&nbsp; },

});

```

The following provider options are available:

\- \*\*inputType\*\* \_'search_document' | 'search_query' | 'classification' | 'clustering'\_

&nbsp; Specifies the type of input passed to the model. Default is `search\_query`.

&nbsp; - `search\_document`: Used for embeddings stored in a vector database for search use-cases.

&nbsp; - `search\_query`: Used for embeddings of search queries run against a vector DB to find relevant documents.

&nbsp; - `classification`: Used for embeddings passed through a text classifier.

&nbsp; - `clustering`: Used for embeddings run through a clustering algorithm.

\- \*\*truncate\*\* \_'NONE' | 'START' | 'END'\_

&nbsp; Specifies how the API will handle inputs longer than the maximum token length.

&nbsp; Default is `END`.

&nbsp; - `NONE`: If selected, when the input exceeds the maximum input token length will return an error.

&nbsp; - `START`: Will discard the start of the input until the remaining input is exactly the maximum input token length for the model.

&nbsp; - `END`: Will discard the end of the input until the remaining input is exactly the maximum input token length for the model.

\### Model Capabilities

| Model | Embedding Dimensions |

| ------------------------------- | -------------------- |

| `embed-english-v3.0` | 1024 |

| `embed-multilingual-v3.0` | 1024 |

| `embed-english-light-v3.0` | 384 |

| `embed-multilingual-light-v3.0` | 384 |

| `embed-english-v2.0` | 4096 |

| `embed-english-light-v2.0` | 1024 |

| `embed-multilingual-v2.0` | 768 |

\## Reranking Models

You can create models that call the \[Cohere rerank API](https://docs.cohere.com/v2/reference/rerank)

using the `.reranking()` factory method.

```ts
const model = cohere.reranking("rerank-v3.5");
```

You can use Cohere reranking models to rerank documents with the `rerank` function:

```ts

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const documents = \[

&nbsp; 'sunny day at the beach',

&nbsp; 'rainy afternoon in the city',

&nbsp; 'snowy night in the mountains',

];



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents,

&nbsp; query: 'talk about rain',

&nbsp; topN: 2,

});



console.log(ranking);

// \[

//   { originalIndex: 1, score: 0.9, document: 'rainy afternoon in the city' },

//   { originalIndex: 0, score: 0.3, document: 'sunny day at the beach' }

// ]

```

Cohere reranking models support additional provider options that can be passed via `providerOptions.cohere`:

```ts

import { cohere } from '@ai-sdk/cohere';

import { rerank } from 'ai';



const { ranking } = await rerank({

&nbsp; model: cohere.reranking('rerank-v3.5'),

&nbsp; documents: \['sunny day at the beach', 'rainy afternoon in the city'],

&nbsp; query: 'talk about rain',

&nbsp; providerOptions: {

&nbsp;   cohere: {

&nbsp;     maxTokensPerDoc: 1000,

&nbsp;     priority: 1,

&nbsp;   },

&nbsp; },

});

```

The following provider options are available:

\- \*\*maxTokensPerDoc\*\* \_number\_

&nbsp; Maximum number of tokens per document. Default is `4096`.

\- \*\*priority\*\* \_number\_

&nbsp; Priority of the request. Default is `0`.

\### Model Capabilities

| Model |

| -------------------------- |

| `rerank-v3.5` |

| `rerank-english-v3.0` |

| `rerank-multilingual-v3.0` |

---

title: Fireworks

description: Learn how to use Fireworks models with the AI SDK.

---

\# Fireworks Provider

\[Fireworks](https://fireworks.ai/) is a platform for running and testing LLMs through their \[API](https://readme.fireworks.ai/).

\## Setup

The Fireworks provider is available via the `@ai-sdk/fireworks` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/fireworks" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/fireworks" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/fireworks" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/fireworks" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `fireworks` from `@ai-sdk/fireworks`:

```ts
import { fireworks } from "@ai-sdk/fireworks";
```

If you need a customized setup, you can import `createFireworks` from `@ai-sdk/fireworks`

and create a provider instance with your settings:

```ts

import { createFireworks } from '@ai-sdk/fireworks';



const fireworks = createFireworks({

&nbsp; apiKey: process.env.FIREWORKS\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Fireworks provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.fireworks.ai/inference/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `FIREWORKS\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Language Models

You can create \[Fireworks models](https://fireworks.ai/models) using a provider instance.

The first argument is the model id, e.g. `accounts/fireworks/models/firefunction-v1`:

```ts
const model = fireworks("accounts/fireworks/models/firefunction-v1");
```

\### Reasoning Models

Fireworks exposes the thinking of `deepseek-r1` in the generated text using the `<think>` tag.

You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```ts

import { fireworks } from '@ai-sdk/fireworks';

import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';



const enhancedModel = wrapLanguageModel({

&nbsp; model: fireworks('accounts/fireworks/models/deepseek-r1'),

&nbsp; middleware: extractReasoningMiddleware({ tagName: 'think' }),

});

```

You can then use that enhanced model in functions like `generateText` and `streamText`.

\### Example

You can use Fireworks language models to generate text with the `generateText` function:

```ts

import { fireworks } from '@ai-sdk/fireworks';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: fireworks('accounts/fireworks/models/firefunction-v1'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Fireworks language models can also be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

\### Completion Models

You can create models that call the Fireworks completions API using the `.completion()` factory method:

```ts
const model = fireworks.completion("accounts/fireworks/models/firefunction-v1");
```

\### Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| ---------------------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `accounts/fireworks/models/firefunction-v1` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `accounts/fireworks/models/deepseek-r1` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/deepseek-v3` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/llama-v3p1-405b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `accounts/fireworks/models/llama-v3p1-8b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/llama-v3p2-3b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/llama-v3p3-70b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/mixtral-8x7b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/mixtral-8x7b-instruct-hf` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/mixtral-8x22b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/qwen2p5-coder-32b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/qwen2p5-72b-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/qwen-qwq-32b-preview` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/qwen2-vl-72b-instruct` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/llama-v3p2-11b-vision-instruct` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/qwq-32b` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/yi-large` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

| `accounts/fireworks/models/kimi-k2-instruct` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; The table above lists popular models. Please see the \[Fireworks models

&nbsp; page](https://fireworks.ai/models) for a full list of available models.

</Note>

\## Embedding Models

You can create models that call the Fireworks embeddings API using the `.embedding()` factory method:

```ts
const model = fireworks.embedding("nomic-ai/nomic-embed-text-v1.5");
```

You can use Fireworks embedding models to generate embeddings with the `embed` function:

```ts

import { fireworks } from '@ai-sdk/fireworks';

import { embed } from 'ai';



const { embedding } = await embed({

&nbsp; model: fireworks.embedding('nomic-ai/nomic-embed-text-v1.5'),

&nbsp; value: 'sunny day at the beach',

});

```

\### Model Capabilities

| Model | Dimensions | Max Tokens |

| -------------------------------- | ---------- | ---------- |

| `nomic-ai/nomic-embed-text-v1.5` | 768 | 8192 |

<Note>

&nbsp; For more embedding models, see the \[Fireworks models

&nbsp; page](https://fireworks.ai/models) for a full list of available models.

</Note>

\## Image Models

You can create Fireworks image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

```ts

import { fireworks } from '@ai-sdk/fireworks';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: fireworks.image('accounts/fireworks/models/flux-1-dev-fp8'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; aspectRatio: '16:9',

});

```

<Note>

&nbsp; Model support for `size` and `aspectRatio` parameters varies. See the \[Model

&nbsp; Capabilities](#model-capabilities-1) section below for supported dimensions,

&nbsp; or check the model's documentation on \[Fireworks models

&nbsp; page](https://fireworks.ai/models) for more details.

</Note>

\### Image Editing

Fireworks supports image editing through FLUX Kontext models (`flux-kontext-pro` and `flux-kontext-max`). Pass input images via `prompt.images` to transform or edit existing images.

<Note>

&nbsp; Fireworks Kontext models do not support explicit masks. Editing is

&nbsp; prompt-driven — describe what you want to change in the text prompt.

</Note>

\#### Basic Image Editing

Transform an existing image using text prompts:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: fireworks.image('accounts/fireworks/models/flux-kontext-pro'),

&nbsp; prompt: {

&nbsp;   text: 'Turn the cat into a golden retriever dog',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   fireworks: {

&nbsp;     output\_format: 'jpeg',

&nbsp;   },

&nbsp; },

});

```

\#### Style Transfer

Apply artistic styles to an image:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: fireworks.image('accounts/fireworks/models/flux-kontext-pro'),

&nbsp; prompt: {

&nbsp;   text: 'Transform this into a watercolor painting style',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; aspectRatio: '1:1',

});

```

<Note>

&nbsp; Input images can be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`, or

&nbsp; base64-encoded strings. Fireworks only supports a single input image per

&nbsp; request.

</Note>

\### Model Capabilities

For all models supporting aspect ratios, the following aspect ratios are supported:

`1:1 (default), 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9`

For all models supporting size, the following sizes are supported:

`640 x 1536, 768 x 1344, 832 x 1216, 896 x 1152, 1024x1024 (default), 1152 x 896, 1216 x 832, 1344 x 768, 1536 x 640`

| Model | Dimensions Specification | Image Editing |

| ------------------------------------------------------------ | ------------------------ | ------------------- |

| `accounts/fireworks/models/flux-kontext-pro` | Aspect Ratio | <Check size={18} /> |

| `accounts/fireworks/models/flux-kontext-max` | Aspect Ratio | <Check size={18} /> |

| `accounts/fireworks/models/flux-1-dev-fp8` | Aspect Ratio | <Cross size={18} /> |

| `accounts/fireworks/models/flux-1-schnell-fp8` | Aspect Ratio | <Cross size={18} /> |

| `accounts/fireworks/models/playground-v2-5-1024px-aesthetic` | Size | <Cross size={18} /> |

| `accounts/fireworks/models/japanese-stable-diffusion-xl` | Size | <Cross size={18} /> |

| `accounts/fireworks/models/playground-v2-1024px-aesthetic` | Size | <Cross size={18} /> |

| `accounts/fireworks/models/SSD-1B` | Size | <Cross size={18} /> |

| `accounts/fireworks/models/stable-diffusion-xl-1024-v1-0` | Size | <Cross size={18} /> |

For more details, see the \[Fireworks models page](https://fireworks.ai/models).

\#### Stability AI Models

Fireworks also presents several Stability AI models backed by Stability AI API

keys and endpoint. The AI SDK Fireworks provider does not currently include

support for these models:

| Model ID |

| -------------------------------------- |

| `accounts/stability/models/sd3-turbo` |

| `accounts/stability/models/sd3-medium` |

| `accounts/stability/models/sd3` |

---

title: DeepSeek

description: Learn how to use DeepSeek's models with the AI SDK.

---

\# DeepSeek Provider

The \[DeepSeek](https://www.deepseek.com) provider offers access to powerful language models through the DeepSeek API.

API keys can be obtained from the \[DeepSeek Platform](https://platform.deepseek.com/api\_keys).

\## Setup

The DeepSeek provider is available via the `@ai-sdk/deepseek` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/deepseek" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/deepseek" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/deepseek" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/deepseek" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `deepseek` from `@ai-sdk/deepseek`:

```ts
import { deepseek } from "@ai-sdk/deepseek";
```

For custom configuration, you can import `createDeepSeek` and create a provider instance with your settings:

```ts

import { createDeepSeek } from '@ai-sdk/deepseek';



const deepseek = createDeepSeek({

&nbsp; apiKey: process.env.DEEPSEEK\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the DeepSeek provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls.

&nbsp; The default prefix is `https://api.deepseek.com/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `DEEPSEEK\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Language Models

You can create language models using a provider instance:

```ts

import { deepseek } from '@ai-sdk/deepseek';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: deepseek('deepseek-chat'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

You can also use the `.chat()` or `.languageModel()` factory methods:

```ts
const model = deepseek.chat("deepseek-chat");

// or

const model = deepseek.languageModel("deepseek-chat");
```

DeepSeek language models can be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

\### Reasoning

DeepSeek has reasoning support for the `deepseek-reasoner` model. The reasoning is exposed through streaming:

```ts

import { deepseek } from '@ai-sdk/deepseek';

import { streamText } from 'ai';



const result = streamText({

&nbsp; model: deepseek('deepseek-reasoner'),

&nbsp; prompt: 'How many "r"s are in the word "strawberry"?',

});



for await (const part of result.fullStream) {

&nbsp; if (part.type === 'reasoning') {

&nbsp;   // This is the reasoning text

&nbsp;   console.log('Reasoning:', part.text);

&nbsp; } else if (part.type === 'text') {

&nbsp;   // This is the final answer

&nbsp;   console.log('Answer:', part.text);

&nbsp; }

}

```

See \[AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details

on how to integrate reasoning into your chatbot.

\### Cache Token Usage

DeepSeek provides context caching on disk technology that can significantly reduce token costs for repeated content. You can access the cache hit/miss metrics through the `providerMetadata` property in the response:

```ts

import { deepseek } from '@ai-sdk/deepseek';

import { generateText } from 'ai';



const result = await generateText({

&nbsp; model: deepseek('deepseek-chat'),

&nbsp; prompt: 'Your prompt here',

});



console.log(result.providerMetadata);

// Example output: { deepseek: { promptCacheHitTokens: 1856, promptCacheMissTokens: 5 } }

```

The metrics include:

\- `promptCacheHitTokens`: Number of input tokens that were cached

\- `promptCacheMissTokens`: Number of input tokens that were not cached

<Note>

&nbsp; For more details about DeepSeek's caching system, see the \[DeepSeek caching

&nbsp; documentation](https://api-docs.deepseek.com/guides/kv\_cache#checking-cache-hit-status).

</Note>

\## Model Capabilities

| Model | Text Generation | Object Generation | Image Input | Tool Usage | Tool Streaming |

| ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `deepseek-chat` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `deepseek-reasoner` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; Please see the \[DeepSeek

&nbsp; docs](https://api-docs.deepseek.com/quick\_start/pricing) for a full list of

&nbsp; available models. You can also pass any available provider model ID as a

&nbsp; string if needed.

</Note>

---

title: Cerebras

description: Learn how to use Cerebras's models with the AI SDK.

---

\# Cerebras Provider

The \[Cerebras](https://cerebras.ai) provider offers access to powerful language models through the Cerebras API, including their high-speed inference capabilities powered by Wafer-Scale Engines and CS-3 systems.

API keys can be obtained from the \[Cerebras Platform](https://cloud.cerebras.ai).

\## Setup

The Cerebras provider is available via the `@ai-sdk/cerebras` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/cerebras" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/cerebras" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/cerebras" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/cerebras" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `cerebras` from `@ai-sdk/cerebras`:

```ts
import { cerebras } from "@ai-sdk/cerebras";
```

For custom configuration, you can import `createCerebras` and create a provider instance with your settings:

```ts

import { createCerebras } from '@ai-sdk/cerebras';



const cerebras = createCerebras({

&nbsp; apiKey: process.env.CEREBRAS\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Cerebras provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls.

&nbsp; The default prefix is `https://api.cerebras.ai/v1`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `CEREBRAS\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Language Models

You can create language models using a provider instance:

```ts

import { cerebras } from '@ai-sdk/cerebras';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: cerebras('llama3.1-8b'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

Cerebras language models can be used in the `streamText` function

(see \[AI SDK Core](/docs/ai-sdk-core)).

You can create Cerebras language models using a provider instance. The first argument is the model ID, e.g. `llama-3.3-70b`:

```ts
const model = cerebras("llama-3.3-70b");
```

You can also use the `.languageModel()` and `.chat()` methods:

```ts
const model = cerebras.languageModel("llama-3.3-70b");

const model = cerebras.chat("llama-3.3-70b");
```

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| -------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `llama3.1-8b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `llama-3.3-70b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `gpt-oss-120b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `qwen-3-32b` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `qwen-3-235b-a22b-instruct-2507` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `qwen-3-235b-a22b-thinking-2507` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `zai-glm-4.6` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `zai-glm-4.7` | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

<Note>

&nbsp; Please see the \[Cerebras

&nbsp; docs](https://inference-docs.cerebras.ai/introduction) for more details about

&nbsp; the available models. Note that context windows are temporarily limited to

&nbsp; 8192 tokens in the Free Tier. You can also pass any available provider model

&nbsp; ID as a string if needed.

</Note>

---

title: Replicate

description: Learn how to use Replicate models with the AI SDK.

---

\# Replicate Provider

\[Replicate](https://replicate.com/) is a platform for running open-source AI models.

It is a popular choice for running image generation models.

\## Setup

The Replicate provider is available via the `@ai-sdk/replicate` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/replicate" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/replicate" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/replicate" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/replicate" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `replicate` from `@ai-sdk/replicate`:

```ts
import { replicate } from "@ai-sdk/replicate";
```

If you need a customized setup, you can import `createReplicate` from `@ai-sdk/replicate`

and create a provider instance with your settings:

```ts

import { createReplicate } from '@ai-sdk/replicate';



const replicate = createReplicate({

&nbsp; apiToken: process.env.REPLICATE\_API\_TOKEN ?? '',

});

```

You can use the following optional settings to customize the Replicate provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.replicate.com/v1`.

\- \*\*apiToken\*\* \_string\_

&nbsp; API token that is being sent using the `Authorization` header. It defaults to

&nbsp; the `REPLICATE\_API\_TOKEN` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Image Models

You can create Replicate image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

<Note>

&nbsp; Model support for `size` and other parameters varies by model. Check the

&nbsp; model's documentation on \[Replicate](https://replicate.com/explore) for

&nbsp; supported options and additional parameters that can be passed via

&nbsp; `providerOptions.replicate`.

</Note>

\### Supported Image Models

The following image models are currently supported by the Replicate provider:

\- \[black-forest-labs/flux-1.1-pro-ultra](https://replicate.com/black-forest-labs/flux-1.1-pro-ultra)

\- \[black-forest-labs/flux-1.1-pro](https://replicate.com/black-forest-labs/flux-1.1-pro)

\- \[black-forest-labs/flux-dev](https://replicate.com/black-forest-labs/flux-dev)

\- \[black-forest-labs/flux-pro](https://replicate.com/black-forest-labs/flux-pro)

\- \[black-forest-labs/flux-schnell](https://replicate.com/black-forest-labs/flux-schnell)

\- \[bytedance/sdxl-lightning-4step](https://replicate.com/bytedance/sdxl-lightning-4step)

\- \[fofr/aura-flow](https://replicate.com/fofr/aura-flow)

\- \[fofr/latent-consistency-model](https://replicate.com/fofr/latent-consistency-model)

\- \[fofr/realvisxl-v3-multi-controlnet-lora](https://replicate.com/fofr/realvisxl-v3-multi-controlnet-lora)

\- \[fofr/sdxl-emoji](https://replicate.com/fofr/sdxl-emoji)

\- \[fofr/sdxl-multi-controlnet-lora](https://replicate.com/fofr/sdxl-multi-controlnet-lora)

\- \[ideogram-ai/ideogram-v2-turbo](https://replicate.com/ideogram-ai/ideogram-v2-turbo)

\- \[ideogram-ai/ideogram-v2](https://replicate.com/ideogram-ai/ideogram-v2)

\- \[lucataco/dreamshaper-xl-turbo](https://replicate.com/lucataco/dreamshaper-xl-turbo)

\- \[lucataco/open-dalle-v1.1](https://replicate.com/lucataco/open-dalle-v1.1)

\- \[lucataco/realvisxl-v2.0](https://replicate.com/lucataco/realvisxl-v2.0)

\- \[lucataco/realvisxl2-lcm](https://replicate.com/lucataco/realvisxl2-lcm)

\- \[luma/photon-flash](https://replicate.com/luma/photon-flash)

\- \[luma/photon](https://replicate.com/luma/photon)

\- \[nvidia/sana](https://replicate.com/nvidia/sana)

\- \[playgroundai/playground-v2.5-1024px-aesthetic](https://replicate.com/playgroundai/playground-v2.5-1024px-aesthetic)

\- \[recraft-ai/recraft-v3-svg](https://replicate.com/recraft-ai/recraft-v3-svg)

\- \[recraft-ai/recraft-v3](https://replicate.com/recraft-ai/recraft-v3)

\- \[stability-ai/stable-diffusion-3.5-large-turbo](https://replicate.com/stability-ai/stable-diffusion-3.5-large-turbo)

\- \[stability-ai/stable-diffusion-3.5-large](https://replicate.com/stability-ai/stable-diffusion-3.5-large)

\- \[stability-ai/stable-diffusion-3.5-medium](https://replicate.com/stability-ai/stable-diffusion-3.5-medium)

\- \[tstramer/material-diffusion](https://replicate.com/tstramer/material-diffusion)

You can also use \[versioned models](https://replicate.com/docs/topics/models/versions).

The id for versioned models is the Replicate model id followed by a colon and the version ID (`$modelId:$versionId`), e.g.

`bytedance/sdxl-lightning-4step:5599ed30703defd1d160a25a63321b4dec97101d98b4674bcc56e41f62f35637`.

<Note>

&nbsp; You can also pass any available Replicate model ID as a string if needed.

</Note>

\### Basic Usage

```ts

import { replicate } from '@ai-sdk/replicate';

import { generateImage } from 'ai';

import { writeFile } from 'node:fs/promises';



const { image } = await generateImage({

&nbsp; model: replicate.image('black-forest-labs/flux-schnell'),

&nbsp; prompt: 'The Loch Ness Monster getting a manicure',

&nbsp; aspectRatio: '16:9',

});



await writeFile('image.webp', image.uint8Array);



console.log('Image saved as image.webp');

```

\### Model-specific options

```ts highlight="9-11"

import { replicate } from '@ai-sdk/replicate';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: replicate.image('recraft-ai/recraft-v3'),

&nbsp; prompt: 'The Loch Ness Monster getting a manicure',

&nbsp; size: '1365x1024',

&nbsp; providerOptions: {

&nbsp;   replicate: {

&nbsp;     style: 'realistic\_image',

&nbsp;   },

&nbsp; },

});

```

\### Versioned Models

```ts

import { replicate } from '@ai-sdk/replicate';

import { generateImage } from 'ai';



const { image } = await generateImage({

&nbsp; model: replicate.image(

&nbsp;   'bytedance/sdxl-lightning-4step:5599ed30703defd1d160a25a63321b4dec97101d98b4674bcc56e41f62f35637',

&nbsp; ),

&nbsp; prompt: 'The Loch Ness Monster getting a manicure',

});

```

\### Image Editing

Replicate supports image editing through various models. Pass input images via `prompt.images` to transform or edit existing images.

\#### Basic Image Editing

Transform an existing image using text prompts:

```ts

const imageBuffer = readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: replicate.image('black-forest-labs/flux-kontext-dev'),

&nbsp; prompt: {

&nbsp;   text: 'Turn the cat into a golden retriever dog',

&nbsp;   images: \[imageBuffer],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   replicate: {

&nbsp;     guidance\_scale: 7.5,

&nbsp;     num\_inference\_steps: 30,

&nbsp;   },

&nbsp; },

});

```

\#### Inpainting with Mask

Edit specific parts of an image using a mask. For FLUX Fill models, white areas in the mask indicate where the image should be edited:

```ts

const image = readFileSync('./input-image.png');

const mask = readFileSync('./mask.png'); // White = inpaint, black = keep



const { images } = await generateImage({

&nbsp; model: replicate.image('black-forest-labs/flux-fill-pro'),

&nbsp; prompt: {

&nbsp;   text: 'A sunlit indoor lounge area with a pool containing a flamingo',

&nbsp;   images: \[image],

&nbsp;   mask: mask,

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   replicate: {

&nbsp;     guidance\_scale: 7.5,

&nbsp;     num\_inference\_steps: 30,

&nbsp;   },

&nbsp; },

});

```

<Note>

&nbsp; Input images can be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`, or

&nbsp; base64-encoded strings. Different Replicate models have different parameter

&nbsp; names and capabilities — check the model's documentation on

&nbsp; \[Replicate](https://replicate.com/explore) for details.

</Note>

\### Provider Options

Common provider options for image generation:

\- \*\*maxWaitTimeInSeconds\*\* \_number\_ - Maximum time in seconds to wait for the prediction to complete in sync mode. By default, Replicate uses \[sync mode](https://replicate.com/docs/topics/predictions/create-a-prediction#timeout-duration) with a 60-second timeout. Set to a positive number to use a custom duration (e.g., `120` for 2 minutes). When not specified, uses the default 60-second wait.

\- \*\*guidance_scale\*\* \_number\_ - Guidance scale for classifier-free guidance. Higher values make the output more closely match the prompt.

\- \*\*num_inference_steps\*\* \_number\_ - Number of denoising steps. More steps = higher quality but slower.

\- \*\*negative_prompt\*\* \_string\_ - Negative prompt to guide what to avoid in the generation.

\- \*\*output_format\*\* \_'png' | 'jpg' | 'webp'\_ - Output image format.

\- \*\*output_quality\*\* \_number (1-100)\_ - Output image quality. Only applies to jpg and webp.

\- \*\*strength\*\* \_number (0-1)\_ - Strength of the transformation for img2img. Lower values keep more of the original image.

For more details, see the \[Replicate models page](https://replicate.com/explore).

---

title: Perplexity

description: Learn how to use Perplexity's Sonar API with the AI SDK.

---

\# Perplexity Provider

The \[Perplexity](https://sonar.perplexity.ai) provider offers access to Sonar API - a language model that uniquely combines real-time web search with natural language processing. Each response is grounded in current web data and includes detailed citations, making it ideal for research, fact-checking, and obtaining up-to-date information.

API keys can be obtained from the \[Perplexity Platform](https://docs.perplexity.ai).

\## Setup

The Perplexity provider is available via the `@ai-sdk/perplexity` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/perplexity" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/perplexity" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/perplexity" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/perplexity" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `perplexity` from `@ai-sdk/perplexity`:

```ts
import { perplexity } from "@ai-sdk/perplexity";
```

For custom configuration, you can import `createPerplexity` and create a provider instance with your settings:

```ts

import { createPerplexity } from '@ai-sdk/perplexity';



const perplexity = createPerplexity({

&nbsp; apiKey: process.env.PERPLEXITY\_API\_KEY ?? '',

});

```

You can use the following optional settings to customize the Perplexity provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls.

&nbsp; The default prefix is `https://api.perplexity.ai`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header. It defaults to

&nbsp; the `PERPLEXITY\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

\## Language Models

You can create Perplexity models using a provider instance:

```ts

import { perplexity } from '@ai-sdk/perplexity';

import { generateText } from 'ai';



const { text } = await generateText({

&nbsp; model: perplexity('sonar-pro'),

&nbsp; prompt: 'What are the latest developments in quantum computing?',

});

```

\### Sources

Websites that have been used to generate the response are included in the `sources` property of the result:

```ts

import { perplexity } from '@ai-sdk/perplexity';

import { generateText } from 'ai';



const { text, sources } = await generateText({

&nbsp; model: perplexity('sonar-pro'),

&nbsp; prompt: 'What are the latest developments in quantum computing?',

});



console.log(sources);

```

\### Provider Options \& Metadata

The Perplexity provider includes additional metadata in the response through `providerMetadata`.

Additional configuration options are available through `providerOptions`.

```ts

const result = await generateText({

&nbsp; model: perplexity('sonar-pro'),

&nbsp; prompt: 'What are the latest developments in quantum computing?',

&nbsp; providerOptions: {

&nbsp;   perplexity: {

&nbsp;     return\_images: true, // Enable image responses (Tier-2 Perplexity users only)

&nbsp;   },

&nbsp; },

});



console.log(result.providerMetadata);

// Example output:

// {

//   perplexity: {

//     usage: { citationTokens: 5286, numSearchQueries: 1 },

//     images: \[

//       { imageUrl: "https://example.com/image1.jpg", originUrl: "https://elsewhere.com/page1", height: 1280, width: 720 },

//       { imageUrl: "https://example.com/image2.jpg", originUrl: "https://elsewhere.com/page2", height: 1280, width: 720 }

//     ]

//   },

// }

```

The metadata includes:

\- `usage`: Object containing `citationTokens` and `numSearchQueries` metrics

\- `images`: Array of image URLs when `return\_images` is enabled (Tier-2 users only)

You can enable image responses by setting `return\_images: true` in the provider options. This feature is only available to Perplexity Tier-2 users and above.

\### PDF Support

The Perplexity provider supports reading PDF files.

You can pass PDF files as part of the message content using the `file` type:

```ts

const result = await generateText({

&nbsp; model: perplexity('sonar-pro'),

&nbsp; messages: \[

&nbsp;   {

&nbsp;     role: 'user',

&nbsp;     content: \[

&nbsp;       {

&nbsp;         type: 'text',

&nbsp;         text: 'What is this document about?',

&nbsp;       },

&nbsp;       {

&nbsp;         type: 'file',

&nbsp;         data: fs.readFileSync('./data/ai.pdf'),

&nbsp;         mediaType: 'application/pdf',

&nbsp;         filename: 'ai.pdf', // optional

&nbsp;       },

&nbsp;     ],

&nbsp;   },

&nbsp; ],

});

```

You can also pass the URL of a PDF:

```ts

{

&nbsp; type: 'file',

&nbsp; data: new URL('https://example.com/document.pdf'),

&nbsp; mediaType: 'application/pdf',

&nbsp; filename: 'document.pdf', // optional

}

```

The model will have access to the contents of the PDF file and

respond to questions about it.

<Note>

&nbsp; For more details about Perplexity's capabilities, see the \[Perplexity chat

&nbsp; completion docs](https://docs.perplexity.ai/api-reference/chat-completions).

</Note>

\## Model Capabilities

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming |

| --------------------- | ------------------- | ------------------- | ------------------- | ------------------- |

| `sonar-deep-research` | <Cross size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `sonar-reasoning-pro` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `sonar-reasoning` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `sonar-pro` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

| `sonar` | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>

&nbsp; Please see the \[Perplexity docs](https://docs.perplexity.ai) for detailed API

&nbsp; documentation and the latest updates.

</Note>

---

title: Luma

description: Learn how to use Luma AI models with the AI SDK.

---

\# Luma Provider

\[Luma AI](https://lumalabs.ai/) provides state-of-the-art image generation models through their Dream Machine platform. Their models offer ultra-high quality image generation with superior prompt understanding and unique capabilities like character consistency and multi-image reference support.

\## Setup

The Luma provider is available via the `@ai-sdk/luma` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/luma" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/luma" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/luma" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/luma" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `luma` from `@ai-sdk/luma`:

```ts
import { luma } from "@ai-sdk/luma";
```

If you need a customized setup, you can import `createLuma` and create a provider instance with your settings:

```ts

import { createLuma } from '@ai-sdk/luma';



const luma = createLuma({

&nbsp; apiKey: 'your-api-key', // optional, defaults to LUMA\_API\_KEY environment variable

&nbsp; baseURL: 'custom-url', // optional

&nbsp; headers: {

&nbsp;   /\* custom headers \*/

&nbsp; }, // optional

});

```

You can use the following optional settings to customize the Luma provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Use a different URL prefix for API calls, e.g. to use proxy servers.

&nbsp; The default prefix is `https://api.lumalabs.ai`.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `LUMA\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Image Models

You can create Luma image models using the `.image()` factory method.

For more on image generation with the AI SDK see \[generateImage()](/docs/reference/ai-sdk-core/generate-image).

\### Basic Usage

```ts

import { luma, type LumaImageProviderOptions } from '@ai-sdk/luma';

import { generateImage } from 'ai';

import fs from 'fs';



const { image } = await generateImage({

&nbsp; model: luma.image('photon-1'),

&nbsp; prompt: 'A serene mountain landscape at sunset',

&nbsp; aspectRatio: '16:9',

});



const filename = `image-${Date.now()}.png`;

fs.writeFileSync(filename, image.uint8Array);

console.log(`Image saved to ${filename}`);

```

\### Image Model Settings

You can customize the generation behavior with optional settings:

```ts

const { image } = await generateImage({

&nbsp; model: luma.image('photon-1'),

&nbsp; prompt: 'A serene mountain landscape at sunset',

&nbsp; aspectRatio: '16:9',

&nbsp; maxImagesPerCall: 1, // Maximum number of images to generate per API call

&nbsp; providerOptions: {

&nbsp;   luma: {

&nbsp;     pollIntervalMillis: 5000, // How often to check for completed images (in ms)

&nbsp;     maxPollAttempts: 10, // Maximum number of polling attempts before timeout

&nbsp;   },

&nbsp; } satisfies LumaImageProviderOptions,

});

```

Since Luma processes images through an asynchronous queue system, these settings allow you to tune the polling behavior:

\- \*\*maxImagesPerCall\*\* \_number\_

&nbsp; Override the maximum number of images generated per API call. Defaults to 1.

\- \*\*pollIntervalMillis\*\* \_number\_

&nbsp; Control how frequently the API is checked for completed images while they are

&nbsp; being processed. Defaults to 500ms.

\- \*\*maxPollAttempts\*\* \_number\_

&nbsp; Limit how long to wait for results before timing out, since image generation

&nbsp; is queued asynchronously. Defaults to 120 attempts.

\### Model Capabilities

Luma offers two main models:

| Model | Description |

| ---------------- | ---------------------------------------------------------------- |

| `photon-1` | High-quality image generation with superior prompt understanding |

| `photon-flash-1` | Faster generation optimized for speed while maintaining quality |

Both models support the following aspect ratios:

\- 1:1

\- 3:4

\- 4:3

\- 9:16

\- 16:9 (default)

\- 9:21

\- 21:9

For more details about supported aspect ratios, see the \[Luma Image Generation documentation](https://docs.lumalabs.ai/docs/image-generation).

Key features of Luma models include:

\- Ultra-high quality image generation

\- 10x higher cost efficiency compared to similar models

\- Superior prompt understanding and adherence

\- Unique character consistency capabilities from single reference images

\- Multi-image reference support for precise style matching

\### Image editing

Luma supports different modes of generating images that reference other images.

\#### Modify an image

Images have to be passed as URLs. `weight` can be configured for each image in the `providerOPtions.luma.images` array.

```ts

await generateImage({

&nbsp; model: luma.image('photon-flash-1'),

&nbsp; prompt: {

&nbsp;   text: 'transform the bike to a boat',

&nbsp;   images: \[

&nbsp;     'https://hebbkx1anhila5yf.public.blob.vercel-storage.com/future-me-8hcBWcZOkbE53q3gshhEm16S87qDpF.jpeg',

&nbsp;   ],

&nbsp; },

&nbsp; providerOptions: {

&nbsp;   luma: {

&nbsp;     images: \[{ weight: 1.0 }],

&nbsp;   } satisfies LumaImageProviderOptions,

&nbsp; },

});

```

Learn more at https://docs.lumalabs.ai/docs/image-generation#modify-image.

\#### Referen an image

Use up to 4 reference images to guide your generation. Useful for creating variations or visualizing complex concepts. Adjust the `weight` for each image (0-1) to control the influence of reference images.

```ts

await generateImage({

&nbsp; model: luma.image('photon-flash-1'),

&nbsp; prompt: {

&nbsp;   text: 'A salamander at dusk in a forest pond, in the style of ukiyo-e',

&nbsp;   images: \[

&nbsp;     'https://hebbkx1anhila5yf.public.blob.vercel-storage.com/future-me-8hcBWcZOkbE53q3gshhEm16S87qDpF.jpeg',

&nbsp;   ],

&nbsp; },

&nbsp; aspectRatio: '1:1',

&nbsp; providerOptions: {

&nbsp;   luma: {

&nbsp;     referenceType: 'image',

&nbsp;     images: \[{ weight: 0.8 }],

&nbsp;   } satisfies LumaImageProviderOptions,

&nbsp; },

});

```

Learn more at https://docs.lumalabs.ai/docs/image-generation#image-reference

\#### Style Reference

Apply specific visual styles to your generations using reference images. Control the style influence using the `weight` parameter.

```ts

await generateImage({

&nbsp; model: luma.image('photon-flash-1'),

&nbsp; prompt: 'A blue cream Persian cat launching its website on Vercel',

&nbsp; aspectRatio: '1:1',

&nbsp; providerOptions: {

&nbsp;   luma: {

&nbsp;     referenceType: 'style',

&nbsp;     images: \[{ weight: 0.8 }],

&nbsp;   } satisfies LumaImageProviderOptions,

&nbsp; },

});

```

Learn more at https://docs.lumalabs.ai/docs/image-generation#style-reference

\#### Character Reference

Create consistent and personalized characters using up to 4 reference images of the same subject. More reference images improve character representation.

```ts

await generateImage({

&nbsp; model: luma.image('photon-flash-1'),

&nbsp; prompt: {

&nbsp;   text: 'A woman with a cat riding a broomstick in a forest',

&nbsp;   images: \[

&nbsp;     'https://hebbkx1anhila5yf.public.blob.vercel-storage.com/future-me-8hcBWcZOkbE53q3gshhEm16S87qDpF.jpeg',

&nbsp;   ],

&nbsp; },

&nbsp; aspectRatio: '1:1',

&nbsp; providerOptions: {

&nbsp;   luma: {

&nbsp;     referenceType: 'character',

&nbsp;     images: \[

&nbsp;       {

&nbsp;         id: 'identity0',

&nbsp;       },

&nbsp;     ],

&nbsp;   } satisfies LumaImageProviderOptions,

&nbsp; },

});

```

Learn more at https://docs.lumalabs.ai/docs/image-generation#character-reference

---

title: ElevenLabs

description: Learn how to use the ElevenLabs provider for the AI SDK.

---

\# ElevenLabs Provider

The \[ElevenLabs](https://elevenlabs.io/) provider contains language model support for the ElevenLabs transcription and speech generation APIs.

\## Setup

The ElevenLabs provider is available in the `@ai-sdk/elevenlabs` module. You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/elevenlabs" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/elevenlabs" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/elevenlabs" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/elevenlabs" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

You can import the default provider instance `elevenlabs` from `@ai-sdk/elevenlabs`:

```ts
import { elevenlabs } from "@ai-sdk/elevenlabs";
```

If you need a customized setup, you can import `createElevenLabs` from `@ai-sdk/elevenlabs` and create a provider instance with your settings:

```ts

import { createElevenLabs } from '@ai-sdk/elevenlabs';



const elevenlabs = createElevenLabs({

&nbsp; // custom settings, e.g.

&nbsp; fetch: customFetch,

});

```

You can use the following optional settings to customize the ElevenLabs provider instance:

\- \*\*apiKey\*\* \_string\_

&nbsp; API key that is being sent using the `Authorization` header.

&nbsp; It defaults to the `ELEVENLABS\_API\_KEY` environment variable.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Custom headers to include in the requests.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\## Speech Models

You can create models that call the \[ElevenLabs speech API](https://elevenlabs.io/text-to-speech)

using the `.speech()` factory method.

The first argument is the model id e.g. `eleven\_multilingual\_v2`.

```ts
const model = elevenlabs.speech("eleven\_multilingual\_v2");
```

The `voice` argument can be set to a voice ID from the \[ElevenLabs Voice Library](https://elevenlabs.io/app/voice-library).

You can find voice IDs by selecting a voice in the library and copying its ID.

```ts highlight="6"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { elevenlabs } from '@ai-sdk/elevenlabs';



const result = await generateSpeech({

&nbsp; model: elevenlabs.speech('eleven\_multilingual\_v2'),

&nbsp; text: 'Hello, world!',

&nbsp; voice: '21m00Tcm4TlvDq8ikWAM', // Rachel voice

});

```

You can also pass additional provider-specific options using the `providerOptions` argument:

```ts highlight="7-9"

import { experimental\_generateSpeech as generateSpeech } from 'ai';

import { elevenlabs } from '@ai-sdk/elevenlabs';



const result = await generateSpeech({

&nbsp; model: elevenlabs.speech('eleven\_multilingual\_v2'),

&nbsp; text: 'Hello, world!',

&nbsp; voice: '21m00Tcm4TlvDq8ikWAM',

&nbsp; providerOptions: {

&nbsp;   elevenlabs: {

&nbsp;     voiceSettings: {

&nbsp;       stability: 0.5,

&nbsp;       similarityBoost: 0.75,

&nbsp;     },

&nbsp;   },

&nbsp; },

});

```

\- \*\*language_code\*\* \_string or null\_

&nbsp; Optional. Language code (ISO 639-1) used to enforce a language for the model. Currently, only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, providing a language code will result in an error.

\- \*\*voice_settings\*\* \_object or null\_

&nbsp; Optional. Voice settings that override stored settings for the given voice. These are applied only to the current request.

&nbsp; - \*\*stability\*\* \_double or null\_

&nbsp; Optional. Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range; higher values result in a more monotonous voice.

&nbsp; - \*\*use_speaker_boost\*\* \_boolean or null\_

&nbsp; Optional. Boosts similarity to the original speaker. Increases computational load and latency.

&nbsp; - \*\*similarity_boost\*\* \_double or null\_

&nbsp; Optional. Controls how closely the AI should adhere to the original voice.

&nbsp; - \*\*style\*\* \_double or null\_

&nbsp; Optional. Amplifies the style of the original speaker. May increase latency if set above 0.

\- \*\*pronunciation_dictionary_locators\*\* \_array of objects or null\_

&nbsp; Optional. A list of pronunciation dictionary locators to apply to the text, in order. Up to 3 locators per request.

&nbsp; Each locator object:

&nbsp; - \*\*pronunciation_dictionary_id\*\* \_string\_ (required)

&nbsp; The ID of the pronunciation dictionary.

&nbsp; - \*\*version_id\*\* \_string or null\_ (optional)

&nbsp; The version ID of the dictionary. If not provided, the latest version is used.

\- \*\*seed\*\* \_integer or null\_

&nbsp; Optional. If specified, the system will attempt to sample deterministically. Must be between 0 and 4294967295. Determinism is not guaranteed.

\- \*\*previous_text\*\* \_string or null\_

&nbsp; Optional. The text that came before the current request's text. Can improve continuity when concatenating generations or influence current generation continuity.

\- \*\*next_text\*\* \_string or null\_

&nbsp; Optional. The text that comes after the current request's text. Can improve continuity when concatenating generations or influence current generation continuity.

\- \*\*previous_request_ids\*\* \_array of strings or null\_

&nbsp; Optional. List of request IDs for samples generated before this one. Improves continuity when splitting large tasks. Max 3 IDs. If both `previous\_text` and `previous\_request\_ids` are sent, `previous\_text` is ignored.

\- \*\*next_request_ids\*\* \_array of strings or null\_

&nbsp; Optional. List of request IDs for samples generated after this one. Useful for maintaining continuity when regenerating a sample. Max 3 IDs. If both `next\_text` and `next\_request\_ids` are sent, `next\_text` is ignored.

\- \*\*apply_text_normalization\*\* \_enum\_

&nbsp; Optional. Controls text normalization.

&nbsp; Allowed values: `'auto'` (default), `'on'`, `'off'`.

&nbsp; - `'auto'`: System decides whether to apply normalization (e.g., spelling out numbers).

&nbsp; - `'on'`: Always apply normalization.

&nbsp; - `'off'`: Never apply normalization.

&nbsp; For `eleven\_turbo\_v2\_5` and `eleven\_flash\_v2\_5`, can only be enabled with Enterprise plans.

\- \*\*apply_language_text_normalization\*\* \_boolean\_

&nbsp; Optional. Defaults to `false`. Controls language text normalization, which helps with proper pronunciation in some supported languages (currently only Japanese). May significantly increase latency.

\### Model Capabilities

| Model | Instructions |

| ------------------------ | ------------------- |

| `eleven\_v3` | <Check size={18} /> |

| `eleven\_multilingual\_v2` | <Check size={18} /> |

| `eleven\_flash\_v2\_5` | <Check size={18} /> |

| `eleven\_flash\_v2` | <Check size={18} /> |

| `eleven\_turbo\_v2\_5` | <Check size={18} /> |

| `eleven\_turbo\_v2` | <Check size={18} /> |

| `eleven\_monolingual\_v1` | <Check size={18} /> |

| `eleven\_multilingual\_v1` | <Check size={18} /> |

\## Transcription Models

You can create models that call the \[ElevenLabs transcription API](https://elevenlabs.io/speech-to-text)

using the `.transcription()` factory method.

The first argument is the model id e.g. `scribe\_v1`.

```ts
const model = elevenlabs.transcription("scribe\_v1");
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format can sometimes improve transcription performance if known beforehand.

```ts highlight="6"

import { experimental\_transcribe as transcribe } from 'ai';

import { elevenlabs } from '@ai-sdk/elevenlabs';



const result = await transcribe({

&nbsp; model: elevenlabs.transcription('scribe\_v1'),

&nbsp; audio: new Uint8Array(\[1, 2, 3, 4]),

&nbsp; providerOptions: { elevenlabs: { languageCode: 'en' } },

});

```

The following provider options are available:

\- \*\*languageCode\*\* \_string\_

&nbsp; An ISO-639-1 or ISO-639-3 language code corresponding to the language of the audio file.

&nbsp; Can sometimes improve transcription performance if known beforehand.

&nbsp; Defaults to `null`, in which case the language is predicted automatically.

\- \*\*tagAudioEvents\*\* \_boolean\_

&nbsp; Whether to tag audio events like (laughter), (footsteps), etc. in the transcription.

&nbsp; Defaults to `true`.

\- \*\*numSpeakers\*\* \_integer\_

&nbsp; The maximum amount of speakers talking in the uploaded file.

&nbsp; Can help with predicting who speaks when.

&nbsp; The maximum amount of speakers that can be predicted is 32.

&nbsp; Defaults to `null`, in which case the amount of speakers is set to the maximum value the model supports.

\- \*\*timestampsGranularity\*\* \_enum\_

&nbsp; The granularity of the timestamps in the transcription.

&nbsp; Defaults to `'word'`.

&nbsp; Allowed values: `'none'`, `'word'`, `'character'`.

\- \*\*diarize\*\* \_boolean\_

&nbsp; Whether to annotate which speaker is currently talking in the uploaded file.

&nbsp; Defaults to `true`.

\- \*\*fileFormat\*\* \_enum\_

&nbsp; The format of input audio.

&nbsp; Defaults to `'other'`.

&nbsp; Allowed values: `'pcm\_s16le\_16'`, `'other'`.

&nbsp; For `'pcm\_s16le\_16'`, the input audio must be 16-bit PCM at a 16kHz sample rate, single channel (mono), and little-endian byte order.

&nbsp; Latency will be lower than with passing an encoded waveform.

\### Model Capabilities

| Model | Transcription | Duration | Segments | Language |

| ------------------------ | ------------------- | ------------------- | ------------------- | ------------------- |

| `scribe\_v1` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

| `scribe\_v1\_experimental` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |

---

title: LM Studio

description: Use the LM Studio OpenAI compatible API with the AI SDK.

---

\# LM Studio Provider

\[LM Studio](https://lmstudio.ai/) is a user interface for running local models.

It contains an OpenAI compatible API server that you can use with the AI SDK.

You can start the local server under the \[Local Server tab](https://lmstudio.ai/docs/basics/server) in the LM Studio UI ("Start Server" button).

\## Setup

The LM Studio provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API.

You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

To use LM Studio, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';



const lmstudio = createOpenAICompatible({

&nbsp; name: 'lmstudio',

&nbsp; baseURL: 'http://localhost:1234/v1',

});

```

<Note>

&nbsp; LM Studio uses port `1234` by default, but you can change in the \[app's Local

&nbsp; Server tab](https://lmstudio.ai/docs/basics/server).

</Note>

\## Language Models

You can interact with local LLMs in \[LM Studio](https://lmstudio.ai/docs/basics/server#endpoints-overview) using a provider instance.

The first argument is the model id, e.g. `llama-3.2-1b`.

```ts
const model = lmstudio("llama-3.2-1b");
```

\###### To be able to use a model, you need to \[download it first](https://lmstudio.ai/docs/basics/download-model).

\### Example

You can use LM Studio language models to generate text with the `generateText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateText } from 'ai';



const lmstudio = createOpenAICompatible({

&nbsp; name: 'lmstudio',

&nbsp; baseURL: 'https://localhost:1234/v1',

});



const { text } = await generateText({

&nbsp; model: lmstudio('llama-3.2-1b'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

&nbsp; maxRetries: 1, // immediately error if the server is not running

});

```

LM Studio language models can also be used with `streamText`.

\## Embedding Models

You can create models that call the \[LM Studio embeddings API](https://lmstudio.ai/docs/basics/server#endpoints-overview)

using the `.embeddingModel()` factory method.

```ts
const model = lmstudio.embeddingModel("text-embedding-nomic-embed-text-v1.5");
```

\### Example - Embedding a Single Value

```tsx

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { embed } from 'ai';



const lmstudio = createOpenAICompatible({

&nbsp; name: 'lmstudio',

&nbsp; baseURL: 'https://localhost:1234/v1',

});



// 'embedding' is a single embedding object (number\[])

const { embedding } = await embed({

&nbsp; model: lmstudio.embeddingModel('text-embedding-nomic-embed-text-v1.5'),

&nbsp; value: 'sunny day at the beach',

});

```

\### Example - Embedding Many Values

When loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG),

it is often useful to embed many values at once (batch embedding).

The AI SDK provides the \[`embedMany`](/docs/reference/ai-sdk-core/embed-many) function for this purpose.

Similar to `embed`, you can use it with embeddings models,

e.g. `lmstudio.embeddingModel('text-embedding-nomic-embed-text-v1.5')` or `lmstudio.embeddingModel('text-embedding-bge-small-en-v1.5')`.

```tsx

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { embedMany } from 'ai';



const lmstudio = createOpenAICompatible({

&nbsp; name: 'lmstudio',

&nbsp; baseURL: 'https://localhost:1234/v1',

});



// 'embeddings' is an array of embedding objects (number\[]\[]).

// It is sorted in the same order as the input values.

const { embeddings } = await embedMany({

&nbsp; model: lmstudio.embeddingModel('text-embedding-nomic-embed-text-v1.5'),

&nbsp; values: \[

&nbsp;   'sunny day at the beach',

&nbsp;   'rainy afternoon in the city',

&nbsp;   'snowy night in the mountains',

&nbsp; ],

});

```

---

title: NVIDIA NIM

description: Use NVIDIA NIM OpenAI compatible API with the AI SDK.

---

\# NVIDIA NIM Provider

\[NVIDIA NIM](https://www.nvidia.com/en-us/ai/) provides optimized inference microservices for deploying foundation models. It offers an OpenAI-compatible API that you can use with the AI SDK.

\## Setup

The NVIDIA NIM provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API.

You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

To use NVIDIA NIM, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';



const nim = createOpenAICompatible({

&nbsp; name: 'nim',

&nbsp; baseURL: 'https://integrate.api.nvidia.com/v1',

&nbsp; headers: {

&nbsp;   Authorization: `Bearer ${process.env.NIM\_API\_KEY}`,

&nbsp; },

});

```

<Note>

&nbsp; You can obtain an API key and free credits by registering at \[NVIDIA

&nbsp; Build](https://build.nvidia.com/explore/discover). New users receive 1,000

&nbsp; inference credits to get started.

</Note>

\## Language Models

You can interact with NIM models using a provider instance. For example, to use \[DeepSeek-R1](https://build.nvidia.com/deepseek-ai/deepseek-r1), a powerful open-source language model:

```ts
const model = nim.chatModel("deepseek-ai/deepseek-r1");
```

\### Example - Generate Text

You can use NIM language models to generate text with the `generateText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateText } from 'ai';



const nim = createOpenAICompatible({

&nbsp; name: 'nim',

&nbsp; baseURL: 'https://integrate.api.nvidia.com/v1',

&nbsp; headers: {

&nbsp;   Authorization: `Bearer ${process.env.NIM\_API\_KEY}`,

&nbsp; },

});



const { text, usage, finishReason } = await generateText({

&nbsp; model: nim.chatModel('deepseek-ai/deepseek-r1'),

&nbsp; prompt: 'Tell me the history of the San Francisco Mission-style burrito.',

});



console.log(text);

console.log('Token usage:', usage);

console.log('Finish reason:', finishReason);

```

\### Example - Stream Text

NIM language models can also generate text in a streaming fashion with the `streamText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { streamText } from 'ai';



const nim = createOpenAICompatible({

&nbsp; name: 'nim',

&nbsp; baseURL: 'https://integrate.api.nvidia.com/v1',

&nbsp; headers: {

&nbsp;   Authorization: `Bearer ${process.env.NIM\_API\_KEY}`,

&nbsp; },

});



const result = streamText({

&nbsp; model: nim.chatModel('deepseek-ai/deepseek-r1'),

&nbsp; prompt: 'Tell me the history of the Northern White Rhino.',

});



for await (const textPart of result.textStream) {

&nbsp; process.stdout.write(textPart);

}



console.log();

console.log('Token usage:', await result.usage);

console.log('Finish reason:', await result.finishReason);

```

NIM language models can also be used with other AI SDK functions like `generateObject` and `streamObject`.

<Note>

&nbsp; Model support for tool calls and structured object generation varies. For

&nbsp; example, the

&nbsp; \[`meta/llama-3.3-70b-instruct`](https://build.nvidia.com/meta/llama-3\_3-70b-instruct)

&nbsp; model supports object generation capabilities. Check each model's

&nbsp; documentation on NVIDIA Build for specific supported features.

</Note>

---

title: Clarifai

description: Use Clarifai OpenAI compatible API with the AI SDK.

---

\# Clarifai Provider

\[Clarifai](https://docs.clarifai.com/getting-started/quickstart) is a platform for building, deploying, and scaling AI-powered applications. It provides a suite of tools and APIs for computer vision, natural language processing, and generative AI. Clarifai offers an OpenAI-compatible API through its full-stack AI development platform, making it easy to integrate powerful AI capabilities using the AI SDK.

\## Setup

The Clarifai provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

To use Clarifai, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';



const clarifai = createOpenAICompatible({

&nbsp; name: 'clarifai',

&nbsp; baseURL: 'https://api.clarifai.com/v2/ext/openai/v1',

&nbsp; apiKey: process.env.CLARIFAI\_PAT,

});

```

<Note>

&nbsp; You can obtain an API key by creating a Personal Access Token (PAT) in your Clarifai \[account settings](https://clarifai.com/settings/security). Make sure to set the `CLARIFAI\_PAT` environment variable with your PAT.

New users can sign up for a free account on \[Clarifai](https://clarifai.com/signup) to get started.

</Note>

\## Language Models

You can interact with various large language models (LLMs) available on Clarifai using the provider instance. For example, to use \[DeepSeek-R1](https://clarifai.com/deepseek-ai/deepseek-chat/models/DeepSeek-R1-0528-Qwen3-8B), a powerful open-source language model:

```ts

const model = clarifai.chatModel(

&nbsp; 'https://clarifai.com/deepseek-ai/deepseek-chat/models/DeepSeek-R1-0528-Qwen3-8B',

);

```

\### Example - Generate Text

You can use Clarifai language models to generate text with the `generateText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateText } from 'ai';



const clarifai = createOpenAICompatible({

&nbsp; name: 'clarifai',

&nbsp; baseURL: 'https://api.clarifai.com/v2/ext/openai/v1',

&nbsp; apiKey: process.env.CLARIFAI\_PAT,

});



const model = clarifai.chatModel(

&nbsp; 'https://clarifai.com/deepseek-ai/deepseek-chat/models/DeepSeek-R1-0528-Qwen3-8B',

);



const { text, usage, finishReason } = await generateText({

&nbsp; model,

&nbsp; prompt: 'What is photosynthesis?',

});



console.log(text);

console.log('Token usage:', usage);

console.log('Finish reason:', finishReason);

```

\### Example - Streaming Text

You can also stream text responses from Clarifai models using the `streamText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { streamText } from 'ai';



const clarifai = createOpenAICompatible({

&nbsp; name: 'clarifai',

&nbsp; baseURL: 'https://api.clarifai.com/v2/ext/openai/v1',

&nbsp; apiKey: process.env.CLARIFAI\_PAT,

});



const model = clarifai.chatModel(

&nbsp; 'https://clarifai.com/deepseek-ai/deepseek-chat/models/DeepSeek-R1-0528-Qwen3-8B',

);



const result = streamText({

&nbsp; model,

&nbsp; prompt: 'What is photosynthesis?',

});



for await (const message of result.textStream) {

&nbsp; console.log(message);

}

```

For full list of available models, you can refer to the \[Clarifai Model Gallery](https://clarifai.com/explore).

---

title: Heroku

description: Use a Heroku OpenAI compatible API with the AI SDK.

---

\# Heroku Provider

\[Heroku](https://heroku.com/) is a cloud platform that allows you to deploy and run applications, including AI models with OpenAI API compatibility.

You can deploy models that are OpenAI API compatible and use them with the AI SDK.

\## Setup

The Heroku provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API.

You can install it with

<Tabs items={\['pnpm', 'npm', 'yarn']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

</Tabs>

\### Heroku Setup

1\. Create a test app in Heroku:

```bash

heroku create

```

2\. Inference using claude-3-5-haiku:

```bash

heroku ai:models:create -a $APP\_NAME claude-3-5-haiku

```

3\. Export Variables:

```bash

export INFERENCE\_KEY=$(heroku config:get INFERENCE\_KEY -a $APP\_NAME)

export INFERENCE\_MODEL\_ID=$(heroku config:get INFERENCE\_MODEL\_ID -a $APP\_NAME)

export INFERENCE\_URL=$(heroku config:get INFERENCE\_URL -a $APP\_NAME)

```

\## Provider Instance

To use Heroku, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';



const heroku = createOpenAICompatible({

&nbsp; name: 'heroku',

&nbsp; baseURL: process.env.INFERENCE\_URL + '/v1',

&nbsp; apiKey: process.env.INFERENCE\_KEY,

});

```

Be sure to have your `INFERENCE\_KEY`, `INFERENCE\_MODEL\_ID`, and `INFERENCE\_URL` set in your environment variables.

\## Language Models

You can create Heroku models using a provider instance.

The first argument is the served model name, e.g. `claude-3-5-haiku`.

```ts
const model = heroku("claude-3-5-haiku");
```

\### Example

You can use Heroku language models to generate text with the `generateText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateText } from 'ai';



const heroku = createOpenAICompatible({

&nbsp; name: 'heroku',

&nbsp; baseURL: process.env.INFERENCE\_URL + '/v1',

&nbsp; apiKey: process.env.INFERENCE\_KEY,

});



const { text } = await generateText({

&nbsp; model: heroku('claude-3-5-haiku'),

&nbsp; prompt: 'Tell me about yourself in one sentence',

});



console.log(text);

```

Heroku language models are also able to generate text in a streaming fashion with the `streamText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { streamText } from 'ai';



const heroku = createOpenAICompatible({

&nbsp; name: 'heroku',

&nbsp; baseURL: process.env.INFERENCE\_URL + '/v1',

&nbsp; apiKey: process.env.INFERENCE\_KEY,

});



const result = streamText({

&nbsp; model: heroku('claude-3-5-haiku'),

&nbsp; prompt: 'Tell me about yourself in one sentence',

});



for await (const message of result.textStream) {

&nbsp; console.log(message);

}

```

Heroku language models can also be used in the `generateObject`, and `streamObject` functions.

---

title: OpenAI Compatible Providers

description: Use OpenAI compatible providers with the AI SDK.

---

\# OpenAI Compatible Providers

You can use the \[OpenAI Compatible Provider](https://www.npmjs.com/package/@ai-sdk/openai-compatible) package to use language model providers that implement the OpenAI API.

Below we focus on the general setup and provider instance creation. You can also \[write a custom provider package leveraging the OpenAI Compatible package](/providers/openai-compatible-providers/custom-providers).

We provide detailed documentation for the following OpenAI compatible providers:

\- \[LM Studio](/providers/openai-compatible-providers/lmstudio)

\- \[NIM](/providers/openai-compatible-providers/nim)

\- \[Heroku](/providers/openai-compatible-providers/heroku)

\- \[Clarifai](/providers/openai-compatible-providers/clarifai)

The general setup and provider instance creation is the same for all of these providers.

\## Setup

The OpenAI Compatible provider is available via the `@ai-sdk/openai-compatible` module. You can install it with:

<Tabs items={\['pnpm', 'npm', 'yarn', 'bun']}>

&nbsp; <Tab>

&nbsp; <Snippet text="pnpm add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="npm install @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="yarn add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

&nbsp; <Tab>

&nbsp; <Snippet text="bun add @ai-sdk/openai-compatible" dark />

&nbsp; </Tab>

</Tabs>

\## Provider Instance

To use an OpenAI compatible provider, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';



const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

&nbsp; includeUsage: true, // Include usage information in streaming responses

});

```

You can use the following optional settings to customize the provider instance:

\- \*\*baseURL\*\* \_string\_

&nbsp; Set the URL prefix for API calls.

\- \*\*apiKey\*\* \_string\_

&nbsp; API key for authenticating requests. If specified, adds an `Authorization`

&nbsp; header to request headers with the value `Bearer <apiKey>`. This will be added

&nbsp; before any headers potentially specified in the `headers` option.

\- \*\*headers\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Optional custom headers to include in requests. These will be added to request headers

&nbsp; after any headers potentially added by use of the `apiKey` option.

\- \*\*queryParams\*\* \_Record\&lt;string,string\&gt;\_

&nbsp; Optional custom url query parameters to include in request urls.

\- \*\*fetch\*\* \_(input: RequestInfo, init?: RequestInit) => Promise\&lt;Response\&gt;\_

&nbsp; Custom \[fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.

&nbsp; Defaults to the global `fetch` function.

&nbsp; You can use it as a middleware to intercept requests,

&nbsp; or to provide a custom fetch implementation for e.g. testing.

\- \*\*includeUsage\*\* \_boolean\_

&nbsp; Include usage information in streaming responses. When enabled, usage data will be included in the response metadata for streaming requests. Defaults to `undefined` (`false`).

\- \*\*supportsStructuredOutputs\*\* \_boolean\_

&nbsp; Set to true if the provider supports structured outputs. Only relevant for `provider()`, `provider.chatModel()`, and `provider.languageModel()`.

\## Language Models

You can create provider models using a provider instance.

The first argument is the model id, e.g. `model-id`.

```ts
const model = provider("model-id");
```

\### Example

You can use provider language models to generate text with the `generateText` function:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateText } from 'ai';



const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

});



const { text } = await generateText({

&nbsp; model: provider('model-id'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

\### Including model ids for auto-completion

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateText } from 'ai';



type ExampleChatModelIds =

&nbsp; | 'meta-llama/Llama-3-70b-chat-hf'

&nbsp; | 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'

&nbsp; | (string \& {});



type ExampleCompletionModelIds =

&nbsp; | 'codellama/CodeLlama-34b-Instruct-hf'

&nbsp; | 'Qwen/Qwen2.5-Coder-32B-Instruct'

&nbsp; | (string \& {});



type ExampleEmbeddingModelIds =

&nbsp; | 'BAAI/bge-large-en-v1.5'

&nbsp; | 'bert-base-uncased'

&nbsp; | (string \& {});



const model = createOpenAICompatible<

&nbsp; ExampleChatModelIds,

&nbsp; ExampleCompletionModelIds,

&nbsp; ExampleEmbeddingModelIds

>({

&nbsp; name: 'example',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.example.com/v1',

});



// Subsequent calls to e.g. `model.chatModel` will auto-complete the model id

// from the list of `ExampleChatModelIds` while still allowing free-form

// strings as well.



const { text } = await generateText({

&nbsp; model: model.chatModel('meta-llama/Llama-3-70b-chat-hf'),

&nbsp; prompt: 'Write a vegetarian lasagna recipe for 4 people.',

});

```

\### Custom query parameters

Some providers may require custom query parameters. An example is the \[Azure AI

Model Inference

API](https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-chat-completions?view=azureml-api-2)

which requires an `api-version` query parameter.

You can set these via the optional `queryParams` provider setting. These will be

added to all requests made by the provider.

```ts highlight="7-9"

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';



const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

&nbsp; queryParams: {

&nbsp;   'api-version': '1.0.0',

&nbsp; },

});

```

For example, with the above configuration, API requests would include the query parameter in the URL like:

`https://api.provider.com/v1/chat/completions?api-version=1.0.0`.

\## Image Models

You can create image models using the `.imageModel()` factory method:

```ts
const model = provider.imageModel("model-id");
```

\### Basic Image Generation

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateImage } from 'ai';



const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

});



const { images } = await generateImage({

&nbsp; model: provider.imageModel('model-id'),

&nbsp; prompt: 'A futuristic cityscape at sunset',

&nbsp; size: '1024x1024',

});

```

\### Image Editing

The OpenAI Compatible provider supports image editing through the `/images/edits` endpoint. Pass input images via `prompt.images` to transform or edit existing images.

\#### Basic Image Editing

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateImage } from 'ai';

import fs from 'fs';



const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

});



const imageBuffer = fs.readFileSync('./input-image.png');



const { images } = await generateImage({

&nbsp; model: provider.imageModel('model-id'),

&nbsp; prompt: {

&nbsp;   text: 'Turn the cat into a dog but retain the style of the original image',

&nbsp;   images: \[imageBuffer],

&nbsp; },

});

```

\#### Inpainting with Mask

Edit specific parts of an image using a mask:

```ts

import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

import { generateImage } from 'ai';

import fs from 'fs';



const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

});



const image = fs.readFileSync('./input-image.png');

const mask = fs.readFileSync('./mask.png');



const { images } = await generateImage({

&nbsp; model: provider.imageModel('model-id'),

&nbsp; prompt: {

&nbsp;   text: 'A sunlit indoor lounge area with a pool containing a flamingo',

&nbsp;   images: \[image],

&nbsp;   mask,

&nbsp; },

});

```

<Note>

&nbsp; Input images can be provided as `Buffer`, `ArrayBuffer`, `Uint8Array`,

&nbsp; base64-encoded strings, or URLs. The provider will automatically download

&nbsp; URL-based images and convert them to the appropriate format.

</Note>

\## Provider-specific options

The OpenAI Compatible provider supports adding provider-specific options to the request body. These are specified with the `providerOptions` field in the request body.

For example, if you create a provider instance with the name `providerName`, you can add a `customOption` field to the request body like this:

```ts

const provider = createOpenAICompatible({

&nbsp; name: 'providerName',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

});



const { text } = await generateText({

&nbsp; model: provider('model-id'),

&nbsp; prompt: 'Hello',

&nbsp; providerOptions: {

&nbsp;   providerName: { customOption: 'magic-value' },

&nbsp; },

});

```

Note that the `providerOptions` key will be in camelCase. If you set the provider name to `provider-name`, the options still need to be set on `providerOptions.providerName`.

The request body sent to the provider will include the `customOption` field with the value `magic-value`. This gives you an easy way to add provider-specific options to requests without having to modify the provider or AI SDK code.

\## Custom Metadata Extraction

The OpenAI Compatible provider supports extracting provider-specific metadata from API responses through metadata extractors.

These extractors allow you to capture additional information returned by the provider beyond the standard response format.

Metadata extractors receive the raw, unprocessed response data from the provider, giving you complete flexibility

to extract any custom fields or experimental features that the provider may include.

This is particularly useful when:

\- Working with providers that include non-standard response fields

\- Experimenting with beta or preview features

\- Capturing provider-specific metrics or debugging information

\- Supporting rapid provider API evolution without SDK changes

Metadata extractors work with both streaming and non-streaming chat completions and consist of two main components:

1\. A function to extract metadata from complete responses

2\. A streaming extractor that can accumulate metadata across chunks in a streaming response

Here's an example metadata extractor that captures both standard and custom provider data:

```typescript

const myMetadataExtractor: MetadataExtractor = {

&nbsp; // Process complete, non-streaming responses

&nbsp; extractMetadata: ({ parsedBody }) => {

&nbsp;   // You have access to the complete raw response

&nbsp;   // Extract any fields the provider includes

&nbsp;   return {

&nbsp;     myProvider: {

&nbsp;       standardUsage: parsedBody.usage,

&nbsp;       experimentalFeatures: parsedBody.beta\_features,

&nbsp;       customMetrics: {

&nbsp;         processingTime: parsedBody.server\_timing?.total\_ms,

&nbsp;         modelVersion: parsedBody.model\_version,

&nbsp;         // ... any other provider-specific data

&nbsp;       },

&nbsp;     },

&nbsp;   };

&nbsp; },



&nbsp; // Process streaming responses

&nbsp; createStreamExtractor: () => {

&nbsp;   let accumulatedData = {

&nbsp;     timing: \[],

&nbsp;     customFields: {},

&nbsp;   };



&nbsp;   return {

&nbsp;     // Process each chunk's raw data

&nbsp;     processChunk: parsedChunk => {

&nbsp;       if (parsedChunk.server\_timing) {

&nbsp;         accumulatedData.timing.push(parsedChunk.server\_timing);

&nbsp;       }

&nbsp;       if (parsedChunk.custom\_data) {

&nbsp;         Object.assign(accumulatedData.customFields, parsedChunk.custom\_data);

&nbsp;       }

&nbsp;     },

&nbsp;     // Build final metadata from accumulated data

&nbsp;     buildMetadata: () => ({

&nbsp;       myProvider: {

&nbsp;         streamTiming: accumulatedData.timing,

&nbsp;         customData: accumulatedData.customFields,

&nbsp;       },

&nbsp;     }),

&nbsp;   };

&nbsp; },

};

```

You can provide a metadata extractor when creating your provider instance:

```typescript

const provider = createOpenAICompatible({

&nbsp; name: 'my-provider',

&nbsp; apiKey: process.env.PROVIDER\_API\_KEY,

&nbsp; baseURL: 'https://api.provider.com/v1',

&nbsp; metadataExtractor: myMetadataExtractor,

});

```

The extracted metadata will be included in the response under the `providerMetadata` field:

```typescript

const { text, providerMetadata } = await generateText({

&nbsp; model: provider('model-id'),

&nbsp; prompt: 'Hello',

});



console.log(providerMetadata.myProvider.customMetric);

```

This allows you to access provider-specific information while maintaining a consistent interface across different providers.

// CONTRIBUTING GUIDE

// https://github.com/vercel/ai/blob/main/contributing/add-new-tool-to-registry.md

export interface Tool {

&nbsp; slug: string;

&nbsp; name: string;

&nbsp; description: string;

&nbsp; packageName: string;

&nbsp; tags?: string\[];

&nbsp; apiKeyEnvName?: string;

&nbsp; installCommand: {

&nbsp; pnpm: string;

&nbsp; npm: string;

&nbsp; yarn: string;

&nbsp; bun: string;

&nbsp; };

&nbsp; codeExample: string;

&nbsp; docsUrl?: string;

&nbsp; apiKeyUrl?: string;

&nbsp; websiteUrl?: string;

&nbsp; npmUrl?: string;

}

export const tools: Tool\[] = \[

&nbsp; {

&nbsp; slug: 'code-execution',

&nbsp; name: 'Code Execution',

&nbsp; description:

&nbsp; 'Execute Python code in a sandboxed environment using Vercel Sandbox. Run calculations, data processing, and other computational tasks safely in an isolated environment with Python 3.13.',

&nbsp; packageName: 'ai-sdk-tool-code-execution',

&nbsp; tags: \['code-execution', 'sandbox'],

&nbsp; apiKeyEnvName: 'VERCEL_OIDC_TOKEN',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add ai-sdk-tool-code-execution',

&nbsp; npm: 'npm install ai-sdk-tool-code-execution',

&nbsp; yarn: 'yarn add ai-sdk-tool-code-execution',

&nbsp; bun: 'bun add ai-sdk-tool-code-execution',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { executeCode } from 'ai-sdk-tool-code-execution';

const { text } = await generateText({

&nbsp; model: 'openai/gpt-5.1-codex',

&nbsp; prompt: 'What is 5 + 5 minus 84 cubed?',

&nbsp; tools: {

&nbsp; executeCode: executeCode(),

&nbsp; },

&nbsp; stopWhen: stepCountIs(5),

});

console.log(text);`,

&nbsp; docsUrl: 'https://vercel.com/docs/vercel-sandbox',

&nbsp; apiKeyUrl: 'https://vercel.com/docs/vercel-sandbox#authentication',

&nbsp; websiteUrl: 'https://vercel.com/docs/vercel-sandbox',

&nbsp; npmUrl: 'https://www.npmjs.com/package/ai-sdk-tool-code-execution',

&nbsp; },

&nbsp; {

&nbsp; slug: 'exa',

&nbsp; name: 'Exa',

&nbsp; description:

&nbsp; 'Exa is a web search API that adds web search capabilities to your LLMs. Exa can search the web for code docs, current information, news, articles, and a lot more. Exa performs real-time web searches and can get page content from specific URLs. Add Exa web search tool to your LLMs in just a few lines of code.',

&nbsp; packageName: '@exalabs/ai-sdk',

&nbsp; tags: \['search', 'web', 'extraction'],

&nbsp; apiKeyEnvName: 'EXA_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add @exalabs/ai-sdk',

&nbsp; npm: 'npm install @exalabs/ai-sdk',

&nbsp; yarn: 'yarn add @exalabs/ai-sdk',

&nbsp; bun: 'bun add @exalabs/ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { webSearch } from '@exalabs/ai-sdk';

const { text } = await generateText({

&nbsp; model: 'google/gemini-3-pro-preview',

&nbsp; prompt: 'Tell me the latest developments in AI',

&nbsp; tools: {

&nbsp; webSearch: webSearch(),

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.exa.ai/reference/vercel',

&nbsp; apiKeyUrl: 'https://dashboard.exa.ai/api-keys',

&nbsp; websiteUrl: 'https://exa.ai',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@exalabs/ai-sdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'parallel',

&nbsp; name: 'Parallel',

&nbsp; description:

&nbsp; 'Parallel gives AI agents best-in-class tools to search and extract context from the web. Web results returned by Parallel are compressed for optimal token efficiency at inference time.',

&nbsp; packageName: '@parallel-web/ai-sdk-tools',

&nbsp; tags: \['search', 'web', 'extraction'],

&nbsp; apiKeyEnvName: 'PARALLEL_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add @parallel-web/ai-sdk-tools',

&nbsp; npm: 'npm install @parallel-web/ai-sdk-tools',

&nbsp; yarn: 'yarn add @parallel-web/ai-sdk-tools',

&nbsp; bun: 'bun add @parallel-web/ai-sdk-tools',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { searchTool, extractTool } from '@parallel-web/ai-sdk-tools';

const { text } = await generateText({

&nbsp; model: 'google/gemini-3-pro-preview',

&nbsp; prompt: 'When was Vercel Ship AI?',

&nbsp; tools: {

&nbsp; webSearch: searchTool,

&nbsp; webExtract: extractTool,

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; apiKeyUrl: 'https://platform.parallel.ai',

&nbsp; websiteUrl: 'https://parallel.ai',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@parallel-web/ai-sdk-tools',

&nbsp; },

&nbsp; {

&nbsp; slug: 'ctx-zip',

&nbsp; name: 'ctx-zip',

&nbsp; description:

&nbsp; 'Transform MCP tools and AI SDK tools into code, write it to a Vercel sandbox file system and have the agent import the tools, write code, and execute it.',

&nbsp; packageName: 'ctx-zip',

&nbsp; tags: \['code-execution', 'sandbox', 'mcp', 'code-mode'],

&nbsp; apiKeyEnvName: 'VERCEL_OIDC_TOKEN',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add ctx-zip',

&nbsp; npm: 'npm install ctx-zip',

&nbsp; yarn: 'yarn add ctx-zip',

&nbsp; bun: 'bun add ctx-zip',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { createVercelSandboxCodeMode, SANDBOX_SYSTEM_PROMPT } from 'ctx-zip';

const { tools } = await createVercelSandboxCodeMode({

&nbsp; servers: \[

&nbsp; {

&nbsp; name: 'vercel',

&nbsp; url: 'https://mcp.vercel.com',

&nbsp; useSSE: false,

&nbsp; headers: {

&nbsp; Authorization: \\`Bearer \\${process.env.VERCEL\_API\_KEY}\\`,

&nbsp; },

&nbsp; },

&nbsp; ],

&nbsp; standardTools: {

&nbsp; weather: weatherTool,

&nbsp; },

});

const { text } = await generateText({

&nbsp; model: 'openai/gpt-5.2',

&nbsp; tools,

&nbsp; stopWhen: stepCountIs(20),

&nbsp; system: SANDBOX_SYSTEM_PROMPT,

&nbsp; messages: \[

&nbsp; {

&nbsp; role: 'user',

&nbsp; content: 'What tools are available from the Vercel MCP server?',

&nbsp; },

&nbsp; ],

});

console.log(text);

`,

&nbsp; docsUrl: 'https://github.com/karthikscale3/ctx-zip/blob/main/README.md',

&nbsp; apiKeyUrl: 'https://vercel.com/docs/vercel-sandbox#authentication',

&nbsp; websiteUrl: 'https://github.com/karthikscale3/ctx-zip/blob/main/README.md',

&nbsp; npmUrl: 'https://www.npmjs.com/package/ctx-zip',

&nbsp; },

&nbsp; {

&nbsp; slug: 'perplexity-search',

&nbsp; name: 'Perplexity Search',

&nbsp; description:

&nbsp; "Search the web with real-time results and advanced filtering powered by Perplexity's Search API. Provides ranked search results with domain, language, date range, and recency filters. Supports multi-query searches and regional search results.",

&nbsp; packageName: '@perplexity-ai/ai-sdk',

&nbsp; tags: \['search', 'web'],

&nbsp; apiKeyEnvName: 'PERPLEXITY_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add @perplexity-ai/ai-sdk',

&nbsp; npm: 'npm install @perplexity-ai/ai-sdk',

&nbsp; yarn: 'yarn add @perplexity-ai/ai-sdk',

&nbsp; bun: 'bun add @perplexity-ai/ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { perplexitySearch } from '@perplexity-ai/ai-sdk';

const { text } = await generateText({

&nbsp; model: 'openai/gpt-5.2',

&nbsp; prompt: 'What are the latest AI developments? Use search to find current information.',

&nbsp; tools: {

&nbsp; search: perplexitySearch(),

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.perplexity.ai/guides/search-quickstart',

&nbsp; apiKeyUrl: 'https://www.perplexity.ai/account/api/keys',

&nbsp; websiteUrl: 'https://www.perplexity.ai',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@perplexity-ai/ai-sdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'tavily',

&nbsp; name: 'Tavily',

&nbsp; description:

&nbsp; 'Tavily is a web intelligence platform offering real-time web search optimized for AI applications. Tavily provides comprehensive web research capabilities including search, content extraction, website crawling, and site mapping to power AI agents with current information.',

&nbsp; packageName: '@tavily/ai-sdk',

&nbsp; tags: \['search', 'extract', 'crawl'],

&nbsp; apiKeyEnvName: 'TAVILY_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add @tavily/ai-sdk',

&nbsp; npm: 'npm install @tavily/ai-sdk',

&nbsp; yarn: 'yarn add @tavily/ai-sdk',

&nbsp; bun: 'bun add @tavily/ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { tavilySearch } from '@tavily/ai-sdk';

const { text } = await generateText({

&nbsp; model: 'google/gemini-3-pro-preview',

&nbsp; prompt: 'What are the latest developments in agentic search?',

&nbsp; tools: {

&nbsp; webSearch: tavilySearch,

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.tavily.com/documentation/integrations/vercel',

&nbsp; apiKeyUrl: 'https://app.tavily.com/home',

&nbsp; websiteUrl: 'https://tavily.com',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@tavily/ai-sdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'firecrawl',

&nbsp; name: 'Firecrawl',

&nbsp; description:

&nbsp; 'Firecrawl tools for the AI SDK. Web scraping, search, crawling, and data extraction for AI applications. Scrape any website into clean markdown, search the web, crawl entire sites, and extract structured data.',

&nbsp; packageName: 'firecrawl-aisdk',

&nbsp; tags: \['scraping', 'search', 'crawling', 'extraction', 'web'],

&nbsp; apiKeyEnvName: 'FIRECRAWL_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add firecrawl-aisdk',

&nbsp; npm: 'npm install firecrawl-aisdk',

&nbsp; yarn: 'yarn add firecrawl-aisdk',

&nbsp; bun: 'bun add firecrawl-aisdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { scrapeTool } from 'firecrawl-aisdk';

const { text } = await generateText({

&nbsp; model: 'openai/gpt-5-mini',

&nbsp; prompt: 'Scrape https://firecrawl.dev and summarize what it does',

&nbsp; tools: {

&nbsp; scrape: scrapeTool,

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.firecrawl.dev/integrations/ai-sdk',

&nbsp; apiKeyUrl: 'https://firecrawl.dev/app/api-keys',

&nbsp; websiteUrl: 'https://firecrawl.dev',

&nbsp; npmUrl: 'https://www.npmjs.com/package/firecrawl-aisdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'bedrock-agentcore',

&nbsp; name: 'Amazon Bedrock AgentCore',

&nbsp; description:

&nbsp; 'Fully managed Browser and Code Interpreter tools for AI agents. Browser is a fast and secure cloud-based runtime for interacting with web applications, filling forms, navigating websites, and extracting information. Code Interpreter provides an isolated sandbox for executing Python, JavaScript, and TypeScript code to solve complex tasks.',

&nbsp; packageName: 'bedrock-agentcore',

&nbsp; tags: \['code-execution', 'browser-automation', 'sandbox'],

&nbsp; apiKeyEnvName: 'AWS_ROLE_ARN',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add bedrock-agentcore',

&nbsp; npm: 'npm install bedrock-agentcore',

&nbsp; yarn: 'yarn add bedrock-agentcore',

&nbsp; bun: 'bun add bedrock-agentcore',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { bedrock } from '@ai-sdk/amazon-bedrock';

import { awsCredentialsProvider } from '@vercel/oidc-aws-credentials-provider';

import { CodeInterpreterTools } from 'bedrock-agentcore/code-interpreter/vercel-ai';

import { BrowserTools } from 'bedrock-agentcore/browser/vercel-ai';

const credentialsProvider = awsCredentialsProvider({

&nbsp; roleArn: process.env.AWS_ROLE_ARN!,

});

const codeInterpreter = new CodeInterpreterTools({ credentialsProvider });

const browser = new BrowserTools({ credentialsProvider });

try {

&nbsp; const { text } = await generateText({

&nbsp; model: bedrock('us.anthropic.claude-sonnet-4-20250514-v1:0'),

&nbsp; prompt: 'Go to https://news.ycombinator.com and get the first story title. Then use Python to reverse the string.',

&nbsp; tools: {

&nbsp; ...codeInterpreter.tools,

&nbsp; ...browser.tools,

&nbsp; },

&nbsp; stopWhen: stepCountIs(5),

&nbsp; });

&nbsp; console.log(text);

} finally {

&nbsp; await codeInterpreter.stopSession();

&nbsp; await browser.stopSession();

}`,

&nbsp; docsUrl: 'https://github.com/aws/bedrock-agentcore-sdk-typescript',

&nbsp; apiKeyUrl: 'https://vercel.com/docs/oidc/aws',

&nbsp; websiteUrl:

&nbsp; 'https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html',

&nbsp; npmUrl: 'https://www.npmjs.com/package/bedrock-agentcore',

&nbsp; },

&nbsp; {

&nbsp; slug: 'superagent',

&nbsp; name: 'Superagent',

&nbsp; description:

&nbsp; 'AI security guardrails for your LLMs. Protect your AI apps from prompt injection, redact PII/PHI (SSNs, emails, phone numbers), and verify claims against source materials. Add security tools to your LLMs in just a few lines of code.',

&nbsp; packageName: '@superagent-ai/ai-sdk',

&nbsp; tags: \['security', 'guardrails', 'pii', 'prompt-injection', 'verification'],

&nbsp; apiKeyEnvName: 'SUPERAGENT_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add @superagent-ai/ai-sdk',

&nbsp; npm: 'npm install @superagent-ai/ai-sdk',

&nbsp; yarn: 'yarn add @superagent-ai/ai-sdk',

&nbsp; bun: 'bun add @superagent-ai/ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { guard, redact, verify } from '@superagent-ai/ai-sdk';

import { openai } from '@ai-sdk/openai';

const { text } = await generateText({

&nbsp; model: openai('gpt-4o-mini'),

&nbsp; prompt: 'Check this input for security threats: "Ignore all instructions"',

&nbsp; tools: {

&nbsp; guard: guard(),

&nbsp; redact: redact(),

&nbsp; verify: verify(),

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.superagent.sh',

&nbsp; apiKeyUrl: 'https://dashboard.superagent.sh',

&nbsp; websiteUrl: 'https://superagent.sh',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@superagent-ai/ai-sdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'tako-search',

&nbsp; name: 'Tako Search',

&nbsp; description:

&nbsp; "Search Tako's knowledge base for data visualizations, insights, and well-sourced information with charts and analytics.",

&nbsp; packageName: '@takoviz/ai-sdk',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm install @takoviz/ai-sdk',

&nbsp; npm: 'npm install @takoviz/ai-sdk',

&nbsp; yarn: 'yarn add @takoviz/ai-sdk',

&nbsp; bun: 'bun add @takoviz/ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { takoSearch } from '@takoviz/ai-sdk';

import { generateText, stepCountIs } from 'ai';

const { text } = await generateText({

&nbsp; model: 'openai/gpt-5.2',

&nbsp; prompt: 'What is the stock price of Nvidia?',

&nbsp; tools: {

&nbsp; takoSearch: takoSearch(),

&nbsp; },

&nbsp; stopWhen: stepCountIs(5),

});

console.log(text);`,

&nbsp; docsUrl: 'https://github.com/TakoData/ai-sdk#readme',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@takoviz/ai-sdk',

&nbsp; websiteUrl: 'https://tako.com',

&nbsp; apiKeyEnvName: 'TAKO_API_KEY',

&nbsp; apiKeyUrl: 'https://tako.com',

&nbsp; tags: \['search', 'data', 'visualization', 'analytics'],

&nbsp; },

&nbsp; {

&nbsp; slug: 'valyu',

&nbsp; name: 'Valyu',

&nbsp; description:

&nbsp; 'Valyu provides powerful search tools for AI agents. Web search for real-time information, plus specialized domain-specific searchtools: financeSearch (stock prices, earnings, income statements, cash flows, etc), paperSearch (full-text PubMed, arXiv, bioRxiv, medRxiv), bioSearch (clinical trials, FDA drug labels, PubMed, medRxiv, bioRxiv), patentSearch (USPTO patents), secSearch (10-k/10-Q/8-k), economicsSearch (BLS, FRED, World Bank data), and companyResearch (comprehensive company research reports).',

&nbsp; packageName: '@valyu/ai-sdk',

&nbsp; tags: \['search', 'web', 'domain-search'],

&nbsp; apiKeyEnvName: 'VALYU_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm add @valyu/ai-sdk',

&nbsp; npm: 'npm install @valyu/ai-sdk',

&nbsp; yarn: 'yarn add @valyu/ai-sdk',

&nbsp; bun: 'bun add @valyu/ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { webSearch } from '@valyu/ai-sdk';

// Available specialised search tools: financeSearch, paperSearch,

// bioSearch, patentSearch, secSearch, economicsSearch, companyResearch

const { text } = await generateText({

&nbsp; model: 'google/gemini-3-pro-preview',

&nbsp; prompt: 'Latest data center projects for AI inference?',

&nbsp; tools: {

&nbsp; webSearch: webSearch(),

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.valyu.ai/integrations/vercel-ai-sdk',

&nbsp; apiKeyUrl: 'https://platform.valyu.ai',

&nbsp; websiteUrl: 'https://valyu.ai',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@valyu/ai-sdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'airweave',

&nbsp; name: 'Airweave',

&nbsp; description:

&nbsp; 'Airweave is an open-source platform that makes any app searchable for your agent. Sync and search across 35+ data sources (Notion, Slack, Google Drive, databases, and more) with semantic search. Add unified search across all your connected data to your AI applications in just a few lines of code.',

&nbsp; packageName: '@airweave/vercel-ai-sdk',

&nbsp; tags: \['search', 'rag', 'data-sources', 'semantic-search'],

&nbsp; apiKeyEnvName: 'AIRWEAVE_API_KEY',

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm install @airweave/vercel-ai-sdk',

&nbsp; npm: 'npm install @airweave/vercel-ai-sdk',

&nbsp; yarn: 'yarn add @airweave/vercel-ai-sdk',

&nbsp; bun: 'bun add @airweave/vercel-ai-sdk',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { airweaveSearch } from '@airweave/vercel-ai-sdk';

const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4.5',

&nbsp; prompt: 'What were the key decisions from last week?',

&nbsp; tools: {

&nbsp; search: airweaveSearch({

&nbsp; defaultCollection: 'my-knowledge-base',

&nbsp; }),

&nbsp; },

&nbsp; stopWhen: stepCountIs(3),

});

console.log(text);`,

&nbsp; docsUrl: 'https://docs.airweave.ai',

&nbsp; apiKeyUrl: 'https://app.airweave.ai/settings/api-keys',

&nbsp; websiteUrl: 'https://airweave.ai',

&nbsp; npmUrl: 'https://www.npmjs.com/package/@airweave/vercel-ai-sdk',

&nbsp; },

&nbsp; {

&nbsp; slug: 'bash-tool',

&nbsp; name: 'bash-tool',

&nbsp; description:

&nbsp; 'Provides bash, readFile, and writeFile tools for AI agents. Supports @vercel/sandbox for full VM isolation.',

&nbsp; packageName: 'bash-tool',

&nbsp; tags: \['bash', 'file-system', 'sandbox', 'code-execution'],

&nbsp; installCommand: {

&nbsp; pnpm: 'pnpm install bash-tool',

&nbsp; npm: 'npm install bash-tool',

&nbsp; yarn: 'yarn add bash-tool',

&nbsp; bun: 'bun add bash-tool',

&nbsp; },

&nbsp; codeExample: `import { generateText, stepCountIs } from 'ai';

import { createBashTool } from 'bash-tool';

const { tools } = await createBashTool({

&nbsp; files: { 'src/index.ts': "export const hello = 'world';" },

});

const { text } = await generateText({

&nbsp; model: 'anthropic/claude-sonnet-4',

&nbsp; prompt: 'List the files in src/ and show me the contents of index.ts',

&nbsp; tools,

&nbsp; stopWhen: stepCountIs(5),

});

console.log(text);`,

&nbsp; docsUrl: 'https://github.com/vercel/bash-tool',

&nbsp; websiteUrl: 'https://github.com/vercel/bash-tool',

&nbsp; npmUrl: 'https://www.npmjs.com/package/bash-tool',

&nbsp; },

];
